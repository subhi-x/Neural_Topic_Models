RESEARCH Open AccessDeep learning meets ontologies:experiments to anchor the cardiovasculardisease ontology in the biomedicalliteratureMercedes Arguello Casteleiro1, George Demetriou1, Warren Read1, Maria Jesus Fernandez Prieto2, Nava Maroto3,Diego Maseda Fernandez4, Goran Nenadic1,5, Julie Klein6,7, John Keane1,5 and Robert Stevens1*AbstractBackground: Automatic identification of term variants or acceptable alternative free-text terms for gene andprotein names from the millions of biomedical publications is a challenging task. Ontologies, such as theCardiovascular Disease Ontology (CVDO), capture domain knowledge in a computational form and can providecontext for gene/protein names as written in the literature. This study investigates: 1) if word embeddings fromDeep Learning algorithms can provide a list of term variants for a given gene/protein of interest; and 2) if biologicalknowledge from the CVDO can improve such a list without modifying the word embeddings created.Methods: We have manually annotated 105 gene/protein names from 25 PubMed titles/abstracts and mappedthem to 79 unique UniProtKB entries corresponding to gene and protein classes from the CVDO. Using more than14 M PubMed articles (titles and available abstracts), word embeddings were generated with CBOW and Skip-gram.We setup two experiments for a synonym detection task, each with four raters, and 3672 pairs of terms (target termand candidate term) from the word embeddings created. For Experiment I, the target terms for 64 UniProtKBentries were those that appear in the titles/abstracts; Experiment II involves 63 UniProtKB entries and the targetterms are a combination of terms from PubMed titles/abstracts with terms (i.e. increased context) from the CVDOprotein class expressions and labels.Results: In Experiment I, Skip-gram finds term variants (full and/or partial) for 89% of the 64 UniProtKB entries, whileCBOW finds term variants for 67%. In Experiment II (with the aid of the CVDO), Skip-gram finds term variants for95% of the 63 UniProtKB entries, while CBOW finds term variants for 78%. Combining the results of bothexperiments, Skip-gram finds term variants for 97% of the 79 UniProtKB entries, while CBOW finds termvariants for 81%.Conclusions: This study shows performance improvements for both CBOW and Skip-gram on a gene/proteinsynonym detection task by adding knowledge formalised in the CVDO and without modifying the wordembeddings created. Hence, the CVDO supplies context that is effective in inducing term variability for bothCBOW and Skip-gram while reducing ambiguity. Skip-gram outperforms CBOW and finds more pertinent termvariants for gene/protein names annotated from the scientific literature.Keywords: Semantic deep learning, Ontology, Deep learning, CBOW, Skip-gram, Cardiovascular diseaseontology, PubMed* Correspondence: Robert.Stevens@manchester.ac.uk1School of Computer Science, University of Manchester, Manchester, UKFull list of author information is available at the end of the article© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 https://doi.org/10.1186/s13326-018-0181-1BackgroundThe sysVASC project [1] seeks to provide a comprehen-sive systems medicine approach to elucidate pathologicalmechanisms for cardiovascular diseases (CVDs), thenumber one cause of death globally according to theWorld Health Organisation [2]. SysVASC developed theCVD ontology (CVDO) to provide the schema to inte-grate omics data (e.g. genomics, transcriptomics, prote-omics and metabolomics) that, together with the mostrecent scientific papers, are the source of up-to-dateknowledge about the biology of the genes and proteinsunderlying CVD. Extracting knowledge about genes andproteins implicated in CVD for incorporation in theCVDO is an important task in its maintenance. Recog-nising those genes and proteins within the literature is arequired function of this task.Rebholz-Schuhmann et al. [3] distinguish two ap-proaches to identify gene/protein names from literature:1. Lexicon based approaches that are based on largeterminological resources, e.g. resources generatedfrom large databases like the UniProtKnowledgebase (UniProtKB) [4].2. Machine Learning (ML) approaches such asconditional random fields [5] that is used in ABNER[6] and BANNER [7].The first approach has the benefit of normalisation(a.k.a. grounding) [3, 8, 9], i.e. the process of mapping abiological term (e.g. protein name) into a unique entryin a database of biological entities such as UniProtKB.Fundel and Zimmer [10] suggest a limitation that theoverlap of synonyms in different data sources is rathermoderate and thus, terms from other databases, such asthe HUGO Gene Nomenclature Committee database[11] or Entrez Gene [12], are also needed to develop amore complete lexicon for gene and protein names. An-other difficulty is keeping such a lexicon up-to-date, asnew term variants for genes and proteins are producedevery day [8, 9]. Our study takes the second approachusing Deep Learning, an area within ML, to identify suit-able term variants (i.e. short forms such as abbreviationsor acronyms as well as long forms including phrases) forprotein/gene names from the literature.While conventional ML techniques are limited in theirability to process input data in raw natural languageform [13], neural language models from Deep Learningcan associate terms with vectors of real-valued features,and semantically related terms end up close in the vec-tor space [13]. The vector representations learnt for theterms are known as word embeddings (i.e. distributedword representations). As the performance of conven-tional ML techniques are heavily dependent on featureselection [14, 15], a tangible benefit of applying neurallanguage models is that the semantic features of theword embeddings learnt are not explicitly present in theraw natural language input.This study investigates the suitability of the neurallanguage models CBOW (Continuous Bag-of-Words)and Skip-gram of Mikolov et al. [16, 17] to derive alist of acceptable alternative free-text terms (i.e. termvariants) for genes/proteins mentioned in thebiomedical literature. The study focuses on two re-search questions:1. Is it possible to obtain a list of term variants for agene/protein from CBOW and Skip-gram wordembeddings?2. Can an improved list of term variants for a gene/protein be produced from the word embeddings byadding knowledge formalised in the CVDO aboutgenes/proteins (i.e. providing more context toreduce ambiguity)?In this study, a term is a combination of one or morewords/tokens, such as Klf7(?/?) with one token andAnnexin A4 with two tokens. Terms referring to agene and its gene product (typically a protein) are likelyto appear together as well as separately in the literature.CBOW and Skip-gram use content windows as context,i.e. terms appearing together in the textual input.According to Mikolov et al. [17], CBOW predicts thecurrent term based on the context, while Skip-gram pre-dicts surrounding terms given the current term.The CVDO represents information about genes andprotein from the UniProtKB as a subClassOf axioms (i.e.class expressions or descriptions). With the aid of theCVDO ontology, we expect to obtain terms that provide amore pertinent context to terms from the word embed-dings, by, for example a) navigating the class expressionsand retrieving the protein name (e.g. ETS translocationvariant 1) for a gene symbol (e.g. ETV1); or b) retrievingthe full protein name (e.g. Annexin A4) from a partialmatch (e.g. annexin 4) with the protein class name.Knowledge within ontologies has been used in two stud-ies  Pilehvar and Collier [18] and Minarro-Gimenez et al.[19]  to assess the quality of word embeddings inducedfrom the literature. As far as we are aware, the use of on-tologies per se to provide more context (i.e. extra terms)and improve the list of candidate terms from the wordembeddings has not been investigated. This study intendsto exploit the relationship between genes and proteins for-mally represented within the CVDO. A difference betweenour work and Pilehvar and Colliers work [18] is that theword embeddings are not modified, i.e. no post-processing of the term vectors is performed. Hence, theuse of terms that exploits biological knowledge from theCVDO ontology can be seen as an external intervention.Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 2 of 24Related workML methods learn input-output relations from exampleswith the goal of interpreting new inputs; hence, theirperformance is heavily dependent on the choice of datarepresentation (or features) to which they are applied[20]. Various types of models have been proposed to rep-resent words as continuous vectors to estimate continu-ous representation of words and create distributionalsemantic models (DSMs). DSMs derive representationsfor words in such a way that words occurring in similarcontexts have similar representations, and therefore, thecontext needs to be defined.Traditional DSMs include Latent Semantic Analysis(LSA) [21], that generally takes an entire document as acontext (i.e. word-document models), and HyperspaceAnalog to Language (HAL), [22] that takes a slidingword window as a context (i.e. sliding window models).Random Indexing [23] has emerged as a promising alter-native to LSA. LSA, HAL and Random Indexing arespatially motivated DSMs. Examples of probabilisticDSMs are Probabilistic LSA (PLSA) [24] and LatentDirichlet Allocation (LDA) [25]. While spatial DSMscompare terms using distance metrics in high-dimensional space [26], probabilistic DSMs such as LDAor PLSA measure similarity between terms according tothe degree to which they share the same topic distribu-tions [26]. Most DSMs have high computational andstorage costs associated with building the model ormodifying it due to the huge number of dimensions in-volved when a large corpus is modelled [26].This study applies neural language models, i.e. distrib-uted representation of words learnt by neural networks(NNs). Although neural models are not new in DSMs,recent advances in NNs make feasible the derivation ofwords from corpora of billions of words, hence thegrowing interest in Deep Learning and the neural lan-guage models CBOW and Skip-gram [16, 17]. CBOWand Skip-gram have gained popularity to the point of be-ing the baseline for benchmarking word embeddings[27] and as baseline models for performance compari-sons [28]. CBOW and Skip-gram have already beentrained to produce high-quality word embeddings fromEnglish Wikipedia [27, 29].Pyysalo et al. [30] and Minarro-Gimenez et al. [19]were the first to apply neural language models toPubMed corpora. Pyysalo et al. [30] used Skip-gram with22 M PubMed articles as well as more than 672 KPubMed Central Open Access full text articles. Themain aim of Pyysalo et al.s work was to make availableword representations (1- to 5-grams) from the literaturethat could be reused. Minarro-Gimenez et al. [19] usedsmaller datasets from PubMed as well as from othermedical (i.e. Merck Manuals [31], Medscape [32]) andnon-medical sources (i.e. Wikipedia [33]). Many laterstudies have created word embeddings with CBOW andSkip-gram using PubMed corpora.We describe some of these studies taking into accountfour tasks that focus on text words, concepts and theirrelations. At the end of this subsection, we include stud-ies that combine ontologies with word embeddings.Semantic similarity and relatedness taskPedersen et al. [34] align with more recent studies (Hillet al. [35] and Pakhomov et al. [36]) in emphasising thedifference between semantic similarity and semanticallyrelatedness. Pedersen et al. [34] state: semantically simi-lar concepts are deemed to be related on the basis oftheir likeness. Both Pedersen et al. [34] and Hill et al.[35] agree with the view of Resnik [37] that semanticsimilarity represents a special case of semantic related-ness. Pedersen et al. [34] advocate semantic similaritymeasures based on is-a relations, where concepts withina hierarchy are linked directly or indirectly. Prior to Pe-dersen et al. [34], Caviedes and Cimino [38] investigatedconceptual similarity metrics based on the minimumnumber of parent links between concepts. Studies byCaviedes and Cimino [38], Pedersen et al. [34], Hill et al.[35] and Pakhomov et al. [36] made available their data-sets of word-pairs together with human judgments of re-latedness/similarity. Hill et al.s [35] dataset of 999 word-pairs, like the WordSimilarity-353 Test Collection [39](353 word-pairs) and the MEN Test Collection [40] (3 Kword-pairs), are common English words. These datasetscan be regarded as gold standards for the evaluation ofsemantic models.Muneeb [41] et al. applied Skip-gram and CBOW to1.25 M PubMed articles and evaluated the quality of theword embeddings using the Pedersen et al. [34] word-pairs. Muneeb [41] et al. concluded that Skip-gram isbetter suited than CBOW for semantic similarity and re-latedness. Chiu et al. [42] used the Pyysalo et al. [30]datasets and more than 10 M PubMed English abstractsfrom the BioASQ challenge [43] for an intrinsic evalu-ation of the Skip-gram and CBOW word embeddingswith the Pakhomov et al. [36] word-pairs. Chiu et al.[42] conclude that Skip-gram shows overall better re-sults for semantic similarity and relatedness than CBOWwith different pre-processing.Synonymy detection taskHill et al. [35] interpret relatedness as associationand the strongest similarity relation is synonymy. Twowell-known datasets for evaluating synonymy are the 80TOEFL (Test of English as a Foreign Language) syno-nym questions from [21] and the 50 ESL (English as aSecond Language) synonym questions from [44]. Bothstudies [21] and [44] consist of synonym questions with4 options that require knowledge of common EnglishArguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 3 of 24words. It should be noted that the TOEFL synonymquestions dataset is used in the paper that introducesLSA [21].To the best of our knowledge no gold standard of word-pairs together with human judgments for synonymy de-tection exists specific to the biomedical domain.Name entity recognition (NER) and relation extraction tasksThe BioCreative (Critical Assessment of Information Ex-traction systems in Biology) challenge [45] focuses onrecognition of entities in text (i.e. NER) as well as rela-tion extraction. For BioCreative II, Smith et al. [46] men-tion three tasks: gene mention (GM), gene normalisation(GN), and protein-protein interaction (PPI); the first twoare within the scope of NER, whilst the third is a rela-tion extraction task that has NER as a subtask [47].Pyysalo et al. [30] used Skip-gram to create word em-beddings from three datasets: one based on all 22 MPubMed articles; a second based on more than 672 KPubMed Central Open Access full text articles; and athird combining the previous two. Pyysalo et al. [30]clustered the word embeddings created using the well-known K-means clustering algorithm [48] with k = 100.Pyysalo et al. [30] performed a set of NER experimentsto assess the quality of both the word embeddings andthe clusters created. The NER experiments rely on threebiomedical domain corpora: GM using the BioCreativeII dataset; anatomical entity recognition using the Ana-tomical Entity Mention corpus [49]; and disease recogni-tion using the NCBI (National Center for BiotechnologyInformation) Disease corpus [50]. More recently Chiu etal. [42] performed an extrinsic evaluation of word em-beddings created from CBOW and Skip-gram for NERusing two biomedical domain corpora: GM using theBioCreative II dataset and the JNLPBA challenge corpusfrom Kim et al. [51]. The JNLPBA challenge is a NERtask using an extended version of the GENIA corpus(version 3.02) [52]. The GENIA corpus is a manually an-notated corpus of 2 K PubMed/MEDLINE abstracts se-lected from a search using Medical subject headings(MeSH) [53] terms human, blood cells, and tran-scription factors. Chiu et al. [42] conclude that overallSkip-gram shows better results for NER using the data-sets from [46, 51] than CBOW with different pre-processing.Li et al. [54] used Skip-gram with 5.33 M PubMedabstracts obtained from a search with protein as thekeyword. Li et al. [54] like Pyysalo et al. [30] applied theK-means clustering algorithm to cluster word vectors. Adifference to the Pyysalo et al. [30] study is that Li et al.[54] employed the Brown tree-building algorithm [55],which is intended for n-gram language models, after ap-plying K-means clustering. To evaluate the PPI extrac-tion performed, Li et al. [54] relied on five publicallyannotated corpora that has been quantitatively analysedpreviously in a study by Pyysalo et al. [56].Text categorisation (a.k.a. text classification, or topicspotting)Sebastiani [15] states that text categorization is "the ac-tivity of labeling natural language texts with thematiccategories from a predefined set". Therefore, assigningkeywords or key phrases from MeSH to PubMed/MED-LINE titles or titles-plus-abstracts is a type of text cat-egorisation known as MeSH indexing. The 2017 BioASQchallenge comprised three tasks, one is MeSH indexing,i.e. requesting participants to classify new PubMed arti-cles before curators manually assign MeSH terms tothem with some help from the Medical Text Indexer(MTI) [57] from NLM. The MeSHLabeler is an algo-rithm for MeSH indexing (Liu et al... [58]) that outper-forms MTI and won the BioASQ challenge for MeSHindexing in years 2 and 3 of the competition. Both MTIand the MeSHLabeler [58] employ classic bag-of-wordsrepresentations.Peng et al [59] used more than 1 M PubMed citations(some downloaded from NLM and some from theBioASQ Year 3 challenge) and introduced DeepMeSH, aworkflow that exploits CBOW and obtained a slightlybetter performance (2% higher micro F-measure) thanthe MeSHLabeler. It should be noted that MTI, MeSH-Labeler, and DeepMeSH employed implementations ofthe k-nearest neighbour algorithm.Word embeddings and ontologiesThe neural language models CBOW and Skip-gram rep-resent each term as a d-dimensional vector of d realnumbers. Taking the vector for a target term and apply-ing cosine similarity, a list of top ranked terms (highestcosine value) can be obtained from the created wordembeddings. Minarro-Gimenez et al. [19] and Pilehvarand Collier [18] employed the knowledge representedwithin ontologies together with metrics based on cosinesimilarity to evaluate the quality of generated word em-beddings. We overview the studies as follows:1. Minarro-Gimenez et al. [19] focused on fourrelationships (may_treat; may_prevent; has_PE; andhas_MoA) from the National Drug File - ReferenceTerminology (NDF-RT) ontology [60] to assess theword embeddings created based on the hit rate(a.k.a. true positive rate or recall). For example, thenumber of diseases in a may_treat relationshipwith a drug. The hit rate increases if more words forpertinent diseases are within the list of top rankedterms from the word embeddings. Hence, theauthors assessed the word embeddings based on arelation extraction task and benchmark againstArguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 4 of 24knowledge within the NDF-RT ontology. This earlystudy reported a relatively low hit rate; in contrast,later studies (e.g. Levy et al. [29] and Chiu et al.[42]) benefit from the effect of various hyperpara-meter configurations.2. Pilehvar and Collier [18] used the HumanPhenotype Ontology (HPO) [61] to assess wordembeddings created with Skip-gram from 4B tokensfrom PubMed abstracts based on two tasks:synonym (alternative names to a class name) andhypernym (X is-a subclass of Y) identification. Forthe synonym task, the authors benchmark againstknowledge within the HPO for two annotationproperties; oboInOwl:hasRelatedSynonym andoboInOwl:hasExactSynonym. For the HPO in OWL,a class name (rdfs:label) may have synonymsrepresented by these two OWL annotation properties.Based on the position in the list of retrieved terms,Pilehvar and Collier [18] calculated the mean andmedian rank as well as the percentage of phenotypes(i.e. class names in the HPO) for which the rank wasequal to one (i.e. the first term in the list retrieved hasa synonym in the HPO). Pilehvar and Collier [18]reported improvements by post-processing, i.e.recalculating each dimension of the resulting wordvector per phenotype considering a list of weightedwords obtained via Babelfy [62]. The authors state thatfor the phenotype flexion contracture of digit a listof 1.3 K weighted words was obtained via Babelfy.MethodsThis section starts by introducing the three data re-sources used in two experiments. Next, we describe thetwo experiments for a gene/protein synonym detectiontask that use the same vector representations learnt forthe terms (i.e. the word embeddings) with CBOW andSkip-gram. As in the synonym detection task describedby Baroni et al. [63], both experiments consist of a pairof terms (the target and the candidate) where the cosines(the normalized dot product) of each candidate termvector with the target term vector is computed. Finally,we present the human evaluation performed and thethree metrics applied to assess the performance ofCBOW and Skip-gram in the gene/protein synonym de-tection task.Data resourcesCreation of a small-annotated corpus of gene/proteinnames from 25 PubMed articlesThe sysVASC project performed a systematic literaturereview that involved a PubMed query with the text: cor-onary heart disease AND (proteomics OR proteome ORtranscriptomics OR transcriptome OR metabolomics ORmetabolome OR omics) [Julie Klein 2016, personalcommunication, 07 June]. The sysVASC review formedpart of the data collection protocol to obtain patientswith chronic and stable vascular (coronary) disease withexclusion of datasets on acute vascular events or historyof potentially interfering concomitant disease. A collec-tion of 34 omics studies/articles with different biologicalentities of interest (gene, protein, metabolite, miRNA)fulfilled the eligibility criteria. To create a small-annotated corpus relevant for sysVASC and useful forthe synonym detection task, we selected 25 of theseomics studies that focuses mainly on genes/proteins andare available in the MEDLINE/PubMed database [64].We left out articles that focus on metabolites or miRNA.The 25 PubMed articles selected were published be-tween 2004 and 2014.To find the genes/proteins mentioned within the 25PubMed titles/abstracts, we followed Jensen et al. [65]who divided the task into two: first, the recognition ofwords that refer to entities and second, the uniqueidentification of the entities in question. One curatormanually annotated 105 terms related to gene/proteinnames from the 25 PubMed abstracts and titles. Corpusannotation requires at least two annotators and thedevelopment of annotation instructions, and thus, thesmall-annotated corpus cannot be considered a goldstandard corpus as only one curator annotated the gene/protein names and no detailed annotation guidelineswere developed. For unique identification of genes/pro-teins we use UniProtKB identifiers. In the UniProtKBeach protein entry has two identifiers [66]: 1) an acces-sion number (AC) that is assigned to each amino acidsequence upon inclusion into the UniProtKB; and 2) theEntry name (a.k.a. ID), which often contains biologic-ally relevant information. Table 1 contains examples ofthe manual annotation and normalisation processperformed; Table 1 illustrates the lack of standardisationfor protein names in the literature.The next two examples illustrate the subtask of assign-ing UniProtKB identifiers to the genes/proteins anno-tated within the 25 PubMed articles corpus: In the abstract of the PubMed article with ID =15,249,501 the term heat shock protein-27(HSP27) is recognised as a gene/protein name, andsubsequently mapped to UniProtKB AC = P04792. In the abstract of the PubMed article with ID =21,938,407 the term heat shock protein 70 KDa isrecognised as a gene/protein name, andsubsequently mapped to UniProtKB AC = P08107.However, on the 27th May, 2015 this UniProtKBentry became obsolete (see [67]), and it is nowfound with the UniProtKB AC equals P0DMV8 andP0DMV9. Therefore, the term heat shock protein70 KDa is mapped to both UniProtKB ACs, i.e.Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 5 of 24P0DMV8 and P0DMV9. This example can be seenas a case where some level of ambiguity remains, i.e.more than one UniProtKB AC is assigned to thegene/protein term manually annotated.The current study is limited to 25 PubMed titles andabstracts, so we acknowledge that some level of ambigu-ity may remain. We also acknowledge that one straight-forward way to disambiguate is by reading the full paperto find the extra information that may aid in uniquelyidentifying the gene/protein of interest. For example,considering the full text of the article with PubMed ID= 21,938,407, it is clear that the term heat shock protein70 KDa refers to the protein name Heat shock 70 kDaprotein 1A that has the UniProtKB AC = P0DMV8.Thus, the full article helps to clarify the ambiguity.The auxiliary file TermsMapped.xls contains the de-tails of the normalisation performed, i.e. the correlationof the 105 terms annotated to the 79 unique UniProtKBentries, where both the UniProtKB identifiers AC andID are shown.The cardiovascular disease ontology (CVDO)CVDO provides the schema to integrate the omics datafrom multiple biological resources, such as the Uni-ProtKB, the miRBase [68] from EMBL-EBI, the HumanMetabolome Database [69] and the data gathered fromvarious scientific publications (e.g. 34 full-paper omicsstudies from the sysVASC systematic review and theirauxiliary files).At the core of CVDO is the Ontology for BiomedicalInvestigations [70] along with other reference ontologiesproduced by the OBO Consortium, such as the ProteinOntology (PRO) [71], the Sequence Ontology (SO) [72],the three Gene Ontology (GO) sub-ontologies [73], theChemical Entities of Biological Interest Ontology [74],the Cell Ontology [75], the Uber Anatomy Ontology [76],the Phenotypic Quality Ontology [77], and the Relation-ship Ontology [78].For a protein, the CVDO takes as its IRIs the PROIRIs while also keeping the UniProtKB entry identifiers(i.e. the AC and ID) by means of annotation properties.UniProtKB entry updates could mean changes in theamino acid sequence and/or changes in the GO annota-tions. The CVDO represents formally the associationsbetween a protein class and classes from the three GOsub-ontologies. In the CVDO there are 172,121 Uni-ProtKB protein classes related to human, and 86,792UniProtKB protein classes related to mouse. Taking intoaccount the GO annotations for a protein, so far, a totalof only 8,196 UniProtKB protein classes from mouseand human have been identified as of potential interestto sysVASC.The CVDO incorporates information about genes andproteins from the UniProtKB, where no alternativenames for genes and proteins are available in the Uni-ProtKB downloadable files [79]. In terms of knowledgemodelling, the CVDO shares the protein/gene represen-tation used in the Proteasix Ontology (PxO) [80]. TheSubClassOf axioms for the PxO protein class in OWLManchester Syntax [81] are shown in Fig. 1. The axiomprotein SubClassOf (has_gene_template some gene) is aclass expression that conveys an existential restrictionover the object property has_gene_template from thePRO, where the class protein (PR:000000001) is fromthe PRO and the class gene (SO:0000704) is from theSO. Hence, in the CVDO, as in the PxO, the associationbetween a gene and a protein (gene product) is formallyrepresented with the axiom pattern protein SubClassOf(has_gene_template some gene) and this is the keyTable 1 Exemplifying the identification of genes/proteins mentioned within the 25 PubMed titles/abstracts: Terms from PubMedabstract/title from the small-annotated corpus (first column) mapped to UniProtKB ACs (second column) and their correspondingvalues for skos:altLabel annotation properties of the PxO protein classes (third column)Term(s) from PubMed abstract/title UniProtKB AC skos:altLabel for PxO protein classes?(1)-antitrypsinalpha-1-antitrypsinP01009 SERPINA1 (P01009; A1AT_HUMAN) Alpha-1-antitrypsinannexin 4 P09525 ANXA4 (P09525; ANXA4_HUMAN) Annexin A4superoxide dismutase 3 P08294 SOD3 (P08294; SODE_HUMAN) Extracellular superoxide dismutase [Cu-Zn]OLR1 P78380 OLR1 (P78380; OLR1_HUMAN) Oxidized low-density lipoprotein receptor 1glutathione transferase P30711 GSTT1 (P30711; GSTT1_HUMAN) Glutathione S-transferase theta-1FJX1 Q86VR8 FJX1 (Q86VR8; FJX1_HUMAN) Four-jointed box protein 1Class: proteinSubClassOf: 'amino acid chain',has_gene_template some gene'located in' some cellular_component,'participates in' some biological_process,'has function' some molecular_functionFig. 1 The SubClassOf axioms for the PxO protein class in OWLManchester SyntaxArguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 6 of 24knowledge along with the protein and gene names (i.e.lexical content) that we propose to exploit to providemore context for the target terms in Experiment II (seesubsection Setup of Experiment I and Experiment II fora gene/protein synonym detection task for details).For a CVDO protein class, we can use its UniProtKBidentifier (i.e. AC or ID) to build SPARQL [82] SELECTqueries to retrieve: a) the protein class label; and b) thegene class associated to the protein class by exploiting theaxiom pattern protein SubClassOf (has_gene_templatesome gene). The auxiliary file TermsMapped.xls containsthe gene and protein class labels (i.e. rdfs:label) from theCVDO for each of the 79 UniProtKB entries that are partof the small-annotated corpus created.In the PxO, the annotation property skos:altLabel fromthe Simple Knowledge Organization System (SKOS) [83] isassigned to each protein class that represents a UniProtKBentry. The string value for this annotation property alsocontains the identifiers (UniProtKB AC and ID) thatpinpoint the protein uniquely and has typically the formatgene symbol (UniProtKB AC; UniProtKB ID) proteinname. Hence, in the PxO, the association between aprotein and a gene is modelled at the logical level with aSubClassOf axiom as well as information attached to theprotein class (UniProtKB entry) with no effect on thelogical aspects of the class. Table 1 shows how the PxO canprovide more context for the terms annotated, e.g. SER-PINA1 is the gene symbol for the protein name Alpha-1-antitrypsin.The 14 M PubMed datasetWe downloaded the MEDLINE/PubMed baseline filesfor 2015 and the up-to-date files through 8th June 2016.To transform the XML PubMed files (see [84] for detailsof the XML data elements) into a corpus of suitable text-ual input for Skip-gram and CBOW, two pre-processingsteps are carried out. For the first step, we created a pro-cessing pipeline that uses open-source software in Py-thon, such as Beautiful soup [85] and the open-sourceNatural Language Toolkit (NLTK) [86].When pre-processing the textual input for CBOW andSkip-gram, it is common practice to transform the textinto lower-case and to remove systematically all num-bers and punctuation marks. This is, however, unsuitablewhen dealing with protein/gene nomenclature and crit-ical information will be lost if this practice is followed.Tanabe et al. [87] highlight gene and protein namesoften contain hyphens, parentheses, brackets, and othertypes of punctuation. Furthermore, capitalisation andnumerals are essential features in symbols or abbrevia-tions. For instance, for human, non-human primates,chickens, and domestic species, gene symbols containthree to six alphanumeric characters that are all in up-percase (e.g. OLR1), while for mice and rats the firstletter alone is in uppercase (e.g. Olr1). We therefore de-cided to alter the commonly employed pre-processingworkflow. The Python processing examines the PubMedXML files, locates the data elements of interest and ex-tracts information contained within them while preserv-ing uppercase and punctuation marks within a sentenceas well as numbers.For the second step, we employed word2phrase withinthe word2vec software package [88] to get n-grams. Thetitle and abstract (if available) of each PubMed publica-tion are the basis to build the DSMs using Skip-gramand CBOW.Meaningful biomedical terms are typically multi-words; therefore, to obtain better performance titles/ab-stracts need to be transformed into n-grams. To indicatethat more than one word and/or numbers are part of aterm, white space is replaced by _ indicating that themultiple words (and/or numbers) constitute a term.Once pre-processing is complete, we have a biomed-ical unannotated corpus of 14,056,762 PubMed publica-tions (titles and available abstracts) with dates ofpublication between 2000 and 2016 (termed PubMed14 M for short). The complete list of PubMed IDs canbe downloaded from [89].Setup of two experiments for a gene/protein synonymdetection taskThis subsection starts by detailing the creation of theword embeddings with CBOW and Skip-gram using the14 M PubMed dataset. Next, we detail the setup of twoexperiments using a small-annotated corpus of gene/protein names and we also specify the exact contributionof the CVDO in Experiment II.Creation of word embeddings with CBOW and Skip-gramFrom CBOW and Skip-gram we typically obtain: 1) alexicon (i.e. a list of terms) in textual format that isconstructed from the input data; and 2) the vectorrepresentations learnt for the terms, i.e. the wordembeddings.The basic Skip-gram formulation uses the softmaxfunction [17]. The hierarchical softmax is a computa-tionally efficient approximation of the full softmax. If Wis the number of words in the lexicon, hierarchical soft-max only needs to evaluate about log2(W) output nodesto obtain the probability distribution, instead of needingto evaluate W output nodes. This study uses hierarchicalsoftmax.In traditional distributional methods, there are a smallnumber of variables known as the hyperparameters ofthe model. For example, the parameters for the Dirichletpriors in an LDA model are often referred to as hyper-parameters. Levy et al. [29] acknowledges that someArguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 7 of 24hyperparameters are tuneable, while others are alreadytuned to some extent by the algorithms designers.Levy et al. [29] distinguish three types of hyperpara-meters: 1) pre-processing hyperparameters, 2) associ-ation metric hyperparameters, and 3) post-processinghyperparameters. As this study does not modify theresulting term vectors, we present the setup of the pre-processing and association metric hyperparameters im-plemented in word2vec. We refer the reader to Levy etal. [29] and Chiu et al. [42] that study in detail the effectof various hyperparameter configurations.Four pre-processing hyperparameters need to beconsidered: Vector dimension  In word2vec the default value is100. We setup the dimensional representation ofterms to 300. This value is much lower than Levy etal. [29] that uses 500. Context window size  In word2vec the default valueis 5. We setup the window size to 10, similarly toLevy et al. [29]. word2vec implements a weightingscheme where a size-10 window weights its contextsby 10/ 9, 10/ 10, , 2/ 1, 10/ 10. Subsampling  This method dilutes very frequentwords [29]. As recommended by Mikolov et al. [17],and like Levy et al. [29], we use the value 1e-5. Inword2vec subsampling happens before the textualinput is processed and a value zero means that sub-sampling is switched off. Minimum count (min-count)  Terms that occuronly a few times can be discarded and consequentlysome terms will not have vector representations. Inword2vec the default value of min-count is 5, whichis the value taken in this study. Chiu et al. [42] showthat this hyperparameter has a small effect onperformance.The two association metric hyperparameters are: Negative sampling  In word2vec by default negativesampling is zero (i.e. not used). However, Skip-gramwith negative sampling is acknowledged to providestate-of-the-art results on various linguistic tasks[29]. A higher negative sampling means [29]: a)more data and better estimation; and b) negativeexamples are more probable. This study does not usenegative sampling, and therefore, performance gainsfor Skip-gram should be relatively easy to obtain ifnegative sampling is also applied. In other words, itcan be argued that by no using negative sampling weare reducing the performance for Skip-gram. Learning rate  This is a smoothing technique. Inword2vec the default value of alpha is 0.025, whichis used in this study.In this study to create word embeddings with Skip-gram and CBOW, we use a Supermicro with 256GBRAM and two CPUs Intel Xeon E52630 v4 at 2.20GHz.For the 14 M PubMed dataset execution time is lessthan 1 hour for CBOW and more than 10 hours forSkip-gram.Setup of experiment I and experiment II for a gene/proteinsynonym detection taskIn the small-annotated corpus with 105 terms mappedto 79 UniProtKB entries, not all the UniProtKB entrieshave the same number of terms manually annotatedfrom the 25 PubMed titles and abstracts. Consideringthe origin of the target terms and driven by a pragmaticapproach, the 79 UniProtKB AC are divided into twosets that participate in each experiment as follows: Experiment I: the UniProtKB entries that participatein this experiment typically have gene/protein termsmanually annotated from the PubMed titles/abstracts. The target terms for this experiment areonly gene/protein terms manually annotated withvector representations. Experiment II: the UniProtKB entries that participatein this experiment typically have gene/protein termsmanually annotated from the PubMed titles/abstracts for which there is not a vectorrepresentation and/or the CVDO can provide morebiological knowledge (e.g. the gene symbol does notappear among the terms manually annotated for theprotein/gene of interest). The target terms for thisexperiment are a combination of: a) gene/proteinterms manually annotated from PubMed titles and/or abstracts, and b) terms taken from the CVDOprotein and gene class labels. The terms from theCVDO can provide more context to the termsmanually annotated to take full advantage of thebiological knowledge represented within the CVDO.The list of acceptable alternative free-text terms (i.e.candidate terms) for genes/proteins is made of termsfrom the word embeddings with the largest cosine value(the normalized dot product of two vectors) with thetarget term. In this study, we limit the list to the twelvecandidate terms with the highest cosine value (i.e. thetop twelve ranked) and we give more importance to thethree candidate terms with the highest cosine value (i.e.the top three ranked) within the list. We based our deci-sion in cognitive theories such as that of Novak andCañas [90] that states if we give learners 1012 familiarbut unrelated words to memorize in a few seconds, mostwill recall only 59 words. If the words are unfamiliar,such as technical terms introduced for the first time, thelearner may do well to recall correctly two or three ofArguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 8 of 24these. Conversely, if the words are familiar and can berelated to knowledge the learner has in her/his cognitivestructure, e.g. months of the year, 12 or more may be eas-ily recalled.Taking into account the word embeddings obtained,the final setup of both experiments is as follows: Experiment I: this experiment involves 64UniProtKB entries and 85 target terms, wheretypically multiple target terms were tried for thesame UniProtKB entry. For each target term, a listof the top twelve ranked candidate terms (highestcosine similarity) is obtained from the wordembeddings, and thus, this experiment has 1020pairs of terms (the target and the candidate) to beassessed by the four raters with CBOW and Skip-gram. Experiment II: this experiment involves 63UniProtKB entries and 68 target terms, where thecorrespondence between target terms andUniProtKB entries is almost one-to-one. For eachtarget term, a list of the top twelve ranked candidateterms (highest cosine similarity) is obtained fromthe word embeddings, and thus, this experiment has816 pairs of terms (the target and the candidate) tobe assessed by the four raters with CBOW and Skip-gram.A total of 48 UniProtKB entries participate in bothExperiment I and II. In Experiment I there are 16 Uni-ProtKB entries that do not participate in Experiment II,for those that the CVDO cannot provide much moreadded value as they already have the protein name orthe protein name and the gene symbol. In Experiment IIthere are 15 UniProtKB entries that do not participate inExperiment I, those typically correspond to terms anno-tated from PubMed title/abstracts that do not have avector and for which the CVDO may supply target termsfor them by taking terms from the CVDO protein classexpressions and labels.To clarify the similarities and differences between thetwo experiments as well as the exact contribution ofCVDO in Experiment II, we introduce a simple categor-isation that can be applied to: a) the terms from thesmall-annotated corpus, which appear separated by thecharacter | and b) the target terms for the synonym de-tection task, which appear separated by white space. Thesimple categorisation introduced consists of fivecategories:1. Only gene symbol Term is the gene symbol. Forexample: OLR1.2. Gene symbol appears  A combination of termsamong which the gene symbol appears. An examplefrom the small-annotated corpus is C3|complementC3. An example from the target terms for the syno-nym detection task is: oxidized_low-density_lipopro-tein receptor_ 1 OLR1.3. Refer protein name  Terms that refer to the proteinname. An example from the small-annotated corpusis CTRP1|C1q/TNF-related protein 1|adipokineC1q/TNF-related protein (CTRP). An example fromthe target terms for the synonym detection task iscollagen_type_1.4. Only protein name The exact protein name as itappears in the UniProtKB. An example from thetarget terms for the synonym detection task isglutathione_S-transferase theta-1.5. Terms from protein name Terms taken from theprotein name as it appears in the UniProtKB. Anexample from the target terms for the synonymdetection task is c1q tumor_necrosis_factor.Both categories Only protein name and Terms fromprotein name are applied only to the target terms andtake into account the protein name as it appears in theUniProtKB, which is the lexical content from proteinclass labels (i.e. rdfs:label) within the CVDO.Table 2 for Experiment I and Table 3 for ExperimentII apply the simple categorisation proposed to the termsfrom the small-annotated corpus (first column in theTables); and to the target terms for the synonym detec-tion task (second column in the Tables). The third col-umn represents the number of target terms. Forexample, in Table 2 for Experiment I the higher numberTable 2 Setup for Experiment I: The simple categorisationintroduced (see Setup of Experiment I and Experiment II for agene/protein synonym detection task) has been applied to theterms from PubMed abstract/title from the small-annotatedcorpus (first column) as well as to the target terms (secondcolumn). Each row of the third column contains the numberof target terms for the experiment taking into account thecategories that appear in the first and second columnSimple categorisation introducedTerms from PubMed titles/abstracts Target terms nGene symbol appears Gene symbol appears 5Gene symbol appears Only gene symbol 13Gene symbol appears Only protein name 3Gene symbol appears Refer protein name 2Gene symbol appears Terms from protein name 2Only gene symbol Only gene symbol 21Refer protein name Gene symbol appears 1Refer protein name Only protein name 16Refer protein name Refer protein name 18Refer protein name Terms from protein name 4Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 9 of 24of target terms corresponds to the category Only genesymbol with 34 target terms, where 13 of them corres-pond to terms from the small-annotated corpus belong-ing to the category Gene symbol appears.Table 3 for Experiment II has a fourth column to clearlyindicate the origin of the terms added by the CVDO tothe target terms. In Table 3 for Experiment II the highernumber of target terms corresponds to the category Genesymbol appears with 53 target terms, where 27 of themcorrespond to terms from the small-annotated corpus be-longing to the category Refer protein name. For these 27target terms, the CVDO added terms from protein nameand gene symbol, and therefore, exploiting the proteinclass expressions within the CVDO.In the rows of the fourth column of Table 3, thesymbol (R) means that the protein class expressionswithin the CVDO are used to add terms to the targetterms. Hence, 63 of the 68 target terms (i.e. 93%) exploitthe relationship between genes and proteins modelled inthe CVD ontology. Only 5 target terms (i.e. 7%) exploitlexical content from protein class labels.Human evaluation and metrics to assess the performanceof Skip-gram and CBOW in experiment I and IITo assess how many free-text candidate terms withinthe list can be actually considered to be term variants(e.g. synonyms, abbreviations, and variant spellings) werely on four domain experts to rate pairs of terms (thetarget and the candidate) and assess whether the candi-date term is a full-term variant (FTV for short), apartial-term variant (PTV for short), or a non-term vari-ant (NTV for short, meaning none of the previouscategories). The same four raters (A, B, C, and D)assessed the 3672 pairs of terms (target term and candi-date term) in Experiments I and II. Raters A and D aretrained terminologists who work in biomedicine; RatersB and C are bio-curators, who at the time of the studyworked on biochemical knowledge extraction from text-ual resources.We established a strict criterion to mark each pair ofterms (the target and the candidate) from the CBOWand Skip-gram word embeddings. Following Nenadic etal. [91], a candidate term is marked as FTV only whenthe term falls within the following types of term variation:a) orthographic, b) morphological, c) lexical, d) structural,or e) acronyms and abbreviations. Considering the bio-medical value of phraseological expressions (e.g. ankyrin-B_gene or CBS_deficiency), they are marked as PTV ifthey refer to the same protein/gene of interest.In order to calculate precision and recall, which arewell-known metrics for evaluating retrieval (classifica-tion) performance, one set of annotations should be con-sidered as the gold standard [92]. In this study, weadvocate a voting system as we have four annotators/raters and two of them are bio-curators. Hence, we donot follow studies like Thompson et al. [93], which cal-culate precision and recall, and use F score (i.e. a metricthat combines precision and recall) as a way of calculat-ing inter-annotator agreement.When having two raters/coders/annotators, the inter-annotator agreement is typically calculated using CohensKappa measure [94]. For more than two coders, Fleiss[95] proposed a coefficient of agreement that calculatesexpected agreement based on the cumulative distributionTable 3 Setup for Experiment II and contribution of the CVDO: The simple categorisation introduced (see Setup of Experiment I andExperiment II for a gene/protein synonym detection task) has been applied to the terms from PubMed abstract/title from the small-annotated corpus (first column) as well as to the target terms (second column). Each row of the third column contains the numberof target terms for the experiment taking into account the categories that appear in the first and second columnSimple categorisation introducedTerms from PubMed titles/abstracts Target terms n Terms added by CVDO to the target termsGene symbol appears Gene symbol appears 6 Terms from protein name (R)Gene symbol appears Only protein name 1 Protein name (R)Gene symbol appears Refer protein name 1 Terms referring to the protein name (R)Gene symbol appears Terms from protein name 2 Terms from protein name (R)Only gene symbol Gene symbol appears 20 Terms from protein name (R)Only gene symbol Only protein name 4 Protein name (R)Refer protein name Gene symbol appears 27 Terms from protein name and gene symbol (R)Refer protein name Only gene symbol 2 Gene symbol (R)Refer protein name Only protein name 2 Protein nameRefer protein name Refer protein name 1 Terms referring to the protein nameRefer protein name Terms from protein name 2 Terms from protein nameThe fourth column indicates the terms added by the CVDO, when the symbol (R) appears it means that the protein class expressions within the CVDO are usedto add terms to the target termsArguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 10 of 24of judgments by all coders [96]. This measure of inter-annotator agreement is also known as Fleisss multi-? as itcan be interpreted as a generalisation of Scotts ? [97]. Itshould be noted that when all disagreements are consid-ered equal, as in this study, Fleisss multi-? is nearly identi-cal to Krippendorff s ? [98], which is an agreementcoefficient recommended in computational linguistics forcoding tasks without involving nominal and disjoint cat-egories [96]. Hence, we adhere to Artstein and Poesio [96]who state that it is better practice in computationallinguistics to use generalised versions of the coefficients(e.g. Fleisss multi-?) instead of measuring agreement sep-arately for each pair of coders (Cohens Kappa measure),and then report the mean.In this study three metrics are used to assess the per-formance of CBOW and Skip-gram for the synonym de-tection task. The first metric is the area under theReceiver Operating Characteristics (ROC) curve for abinary classifier. FTV and PTV can be merged into onecategory called term variant or TV for short. Hence, themultiple class classification problem can be reduced tothree binary classification problems: 1) FTV and non-FTV; 2) PTV and non-PTV; and 3) TV and non-TV.This study uses ROC curves instead of precision-recallcurves, as ROC curves do not change if the class distri-bution is different [99]. The second metric is the medianof the rank that was used by Pilehvar and Collier [18] ina synonym and hypernym identification tasks with Skip-gram. The third metric is the number of term variants (i.e.FTV and/or PTV) found for each of the 79 UniProtKB en-tries within the small-annotated corpus of gene/proteinnames from 25 PubMed articles.Receiver operating characteristics (ROC) curve and the areaunder the ROC curve (AUC)To compare classifiers, calculating the area under theROC curve, the so-called AUC [100102], is a commonmethod. Fawcett [99] defines the ROC curve as a tech-nique for visualizing, organizing and selecting classifiersbased on their performance. As Bradley [100] stateswhen comparing a number of different classificationschemes it is often desirable to obtain a single figure as ameasure of the classifier's performance. The AUC canbe interpreted as a probability of correct ranking as esti-mated by the Wilcoxon statistic [101]. Furthermore, asHand and Till [102] highlight, the AUC is independent ofcosts, priors, or (consequently) any classification threshold.A ROC curve has two dimensions, where typically TPrate is plotted on the Y axis and FP rate is plotted on theX axis [99]. TP rate stands for true positive rate (a.k.a.hit rate or recall or sensitivity) and is the proportion ofpositives correctly classified as positives; FP rate standsfor false positive rate (a.k.a. false alarm rate) and is theproportion of negatives that are incorrectly classified aspositive. For the perfect classifier TP rate = 1 and FPrate = 0. In the ROC curves, the diagonal line (y = x) isalso plotted which represents random guessing [99] andacts as the baseline for ROC. A random classifier typic-ally slides back and forth on the diagonal [99].As the candidate terms evaluated for the human ratersare ranked (highest cosine value), we have the categoryassigned by the rater to each candidate term (FTV, PTV,or NTV) as well as the position that the candidate termhas in the top twelve ranked list. Firstly, for each experi-ment and rater, we created a table with twelve rows andthree columns: frequency of FTV, frequency of PTV, andfrequency of NTV. For example, the frequency of FTVcolumn accounts for the number of times that a raterassigned FTV for the term in the ith position in the list,with i = [1,, 12]. Secondly, we calculated the cumula-tive frequency, and thus, three more columns wereadded. The cumulative frequency is calculated in de-scending order, where the value of the cumulative fre-quency for the ith position in the list adds to the valuefrom the frequency column in the ith position, the valueof the cumulative frequency for the (i-1)th position inthe list. Thirdly, we calculated the cumulative rate, andtherefore, three more columns were added. For example,the cumulative rate of FTV column is calculated by div-iding the values of the cumulative frequency of FTV col-umn by the total number of FTV assigned by the rater.Hence, the last value in any of the cumulative rate col-umns (12th position) is equal to 1. In the ROC curves,we plot the cumulative rates obtained. Hence, the ROCcurves for FTV, PTV, and TV end at (1, 1).The values for the AUC go from zero to one. Randomguessing will have an AUC = 0. 5 and no realistic classi-fier should have an AUC less than 0. 5 [99]. We plotROC curves for FTV, PTV, and TV and calculate theAUC for each rater and experiment.The median of the rank per human raterBased on the domain expert category assigned (FTV,PTV, or NTV) to each candidate term from the wordembeddings, as well as the position that the candidateterm has in the top twelve ranked list (highest cosinesimilarity), we can calculate the median of the rank forFTV and PTV per rater. A lower median means that theterms marked as terms variants (full or partial) appear atthe beginning of the list.Number of UniProtKB ACs and CVDO classes with a termvariantBased on the 79 unique UniProtKB entries from thesmall-annotated corpus we implement a voting systembased on raters judgement and determine for how manyof the 79 UniProtKB entries mapped to CVDO classes,term variants were found. The voting system takes theArguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 11 of 24domain expert category assigned (FTV, PTV, or NTV)and considers that a candidate term from the top twelveranked list is an FTV if at least one of the four ratersassigned the category FTV once. Likewise, and moregenerally, if at least one of the four raters marks a candi-date term from the top twelve ranked list as FTV orPTV, the voting system concludes a TV has been found.ResultsWe start by illustrating the results obtained in Experi-ment I and II with CBOW and Skip-gram. Next we re-port the human inter-annotator agreement and theresults obtained for the three metrics to assess the per-formance of CBOW and Skip-gram in the gene/proteinsynonym detection task.Exemplifying the results obtained for the gene/proteinsynonym detection task in experiment I and IIEach auxiliary file - CBOW.xls and Skip-gram.xls - con-tains the 1836 pairs of terms (target term and candidateterm) from the word embeddings created, along with thecosine similarity obtained for each pair of terms. Eachfile includes the list of the top twelve ranked candidateterms (highest cosine similarity) per target term, wherethe last four columns have the human judgement (FTV,PTV, or NTV) by the four raters A-D. Each target term:a) relates to a UniProtKB entry that has a UniProtKBidentifier (i.e. the UniProtKB AC column) and also astring value for the annotation property skos:altLabel forthe PxO protein class, b) has a unique identifier in col-umn nQ that also appears in the auxiliary file Terms-Mapped_votingSystem.xls, c) contains at least one termfrom the small-annotated corpus (Term from thePubMed titles/abstracts column), and d) participates inExperiment I (abbreviated as Exp I) or Experiment II(abbreviated as Exp II) as indicated in the Experimentcolumn.We use target terms from the auxiliary files to illus-trate the ranked list of the top twelve candidate terms(highest cosine similarity) for gene/protein names ob-tained from the word embeddings created with CBOWand Skip-gram for Experiments I and II.Table 4 shows the list of the top twelve candidateterms (highest cosine similarity) obtained with CBOWand Skip-gram word embeddings in Experiment I for thetarget term KLF7, which is a gene symbol and appearsas such in the abstract of the PubMed article with ID =23,468,932. For CBOW, all four raters agree that there isnot a full or partial gene/protein term variant (i.e. FTVor PTV) among the list of candidate terms; in otherwords, all the top twelve ranked candidate terms forCBOW were marked as NTV by the four raters. ForSkip-gram, all four raters agree that: a) the candidateterm in the second position in the list is an FTV, and b)the candidate term in the third position in the list is aPTV. Hence, in Experiment I for the target term KLF7,CBOW could not find a TV while Skip-gram found anFTV and also a PTV among the top three ranked candi-date terms in the list. From a biological point of view,the target term KLF7 denotes a human gene, while thecandidate term in the second position in the list Klf7denotes the equivalent gene in mice. The genes KLF7and Klf7 are orthologs according to the NCBI [103]. Thecandidate term in the third position in the list Klf7(?/?)refers to mice which are homozygous for the Klf7 geneknockout. Hence, the pre-processing of the 14 M PubMeddataset that keeps uppercase, punctuation marks, andnumbers, demonstrably preserves valuable biologicalinformation.The term OLR1, which is a gene symbol, appears assuch in the abstract of the PubMed article with ID =22,738,689. Using OLR1 as the target term in Experi-ment I for CBOW and Skip-gram, no candidate termsfrom the word embeddings were suitable as FTV or PVaccording to all four raters.In Experiment II, the term oxidized_low-density_lipo-protein receptor_1 that corresponds to the protein nameis added to the gene symbol OLR1 to create a targetterm. Table 5 shows the top twelve ranked candidateterms obtained by CBOW and Skip-gram word embed-dings in Experiment II using these two terms oxidize-d_low-density_lipoprotein receptor_1 OLR1 as the targetterm. Therefore, the target contains a term that exploitsknowledge within the CVDO and, more concretely, theassociation relationship formally represented betweengenes and proteins. As the CVDO provides more con-text, in Experiment II with both CBOW and Skip-gram,suitable term variants (FTV as well as PTV) were foundfor the protein/gene name.Tables 4 and 5 show higher cosine values for Skip-gram than CBOW. As cosine similarity gives an indica-tion of how strongly semantically related is the pair ofterms (the target and the candidate), it seems naturalthat Skip-grams finds more term variants than CBOW.Table 6 shows the categories FTV, PTV, or NTVassigned by the four human raters (A-D) to the toptwelve ranked candidate terms obtained for Skip-gramin Experiment II using two terms oxidized_low-densi-ty_lipoprotein receptor_1 OLR1 as the target term. Thislist of the top twelve ranked candidate terms appears inthe right-hand side of Table 5. The last three columns ofthe Table exemplify the voting system (abbreviated asVS) applied: full term variant (VS: FTV column), fullterm variant among the top three (VS: FTV for top threecolumn), and full and/or partial term variant (VS: TVcolumn).Two rows appear with a grey background in Table 6.They indicate the process of manually assigned categoriesArguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 12 of 24to be error-prone as Rater C assigned NTV to the candi-date term in the eighth position in the list oxidized_low-density_lipoprotein_(ox-LDL) while marking PTV for thecandidate term in the ninth position, oxidized_low-densi-ty_lipoprotein_(oxLDL). From visual inspection, the onlydifference in these two candidate terms is the appearanceof, or lack of, a -. It should be noted that Raters A, B, andD mark both candidate terms in the list equally, althoughthey differ in the category assigned. The biological back-ground knowledge of Raters B and C (curators) and theirimpact on the manual categorisation process can be de-duced from Table 6. Gene OLR1 has a well-known aliasLOX-1, and thus, Raters B and C marked the candidateterms as FTV if LOX-1 appears alone or PTV if LOX-1appears in combination with other term(s); however,Raters A and D marked all the candidate terms as NTVwhere LOX-1 appears.Human evaluation and metrics to assess the performanceof Skip-gram and CBOW in Experiment I and IIWe start reporting on the inter-annotator agreement coeffi-cients for the four raters. For pairwise inter-annotator agree-ment (the Cohens Kappa measure) per experiment andmodel, we refer the reader to auxiliary file pairwiseIAA.xls.All the inter-annotator agreement coefficients are calculatedwith the implementations from the NLTK [86]:Table 4 Exemplifying results for Experiment I: Top twelve ranked candidate terms (highest cosine similarity) from the wordembeddings created with CBOW and Skip-gram for the target term KLF7 that appears in the abstract of the PubMed article withID = 23,468,932CBOW Skip-gramRank Candidate terms from word embeddings Cosine Candidate terms from word embeddings Cosine1 MoKA 0.376371 Prrx2 0.6019202 pluripotency-associated_genes 0.335113 Klf7 0.5929463 Sp1_regulates 0.334092 Klf7(?/?) 0.5905234 LOC101928923 0.333423 RXRG 0.5898755 p107_dephosphorylation 0.331689 LOC101928923 0.5859796 PU_1 0.329925 SOX-17 0.5852957 histone_demethylase 0.323529 rs820336 0.5850948 gene_promoter 0.321640 GLI-binding_site 0.5810739 homeobox_protein 0.319997 Tead2 0.58001210 histone_arginine 0.315875 hHEX 0.57986811 transfated 0.314202 ACY-957 0.57954212 are_unable_to_repress 0.313112 ETS1 0.577272Table 5 Exemplifying results for Experiment II: Top twelve ranked candidate terms (highest cosine similarity) from the wordembeddings created with CBOW and Skip-gram using two terms as target: OLR1 from the abstract of the PubMed article with ID= 22,738,689; and oxidized_low-density_lipoprotein receptor_ 1 that is the CVDO protein class name (rdfs:label) for the CVDO classgene with name (rdfs:label) OLR1. Hence, the target term exploits the protein class expressions within the CVDOCBOW Skip-gramRank Candidate terms from word embeddings Cosine Candidate terms from word embeddings Cosine1 atherogenesis 0.469405 lectin-like_oxidized_low-density_lipoprotein 0.6886032 atherosclerosis 0.465861 (LOX-1)_is 0.6720423 CD36 0.439280 atherosclerosis_we_investigated 0.6690504 LOX-1 0.424173 receptor-1 0.6648915 atherosclerotic_lesion_formation 0.416537 lectin-like_oxidized_LDL_receptor-1 0.6639886 vascular_inflammation 0.414620 lOX-1_is 0.6601107 inflammatory_genes 0.411186 human_atherosclerotic_lesions 0.6570758 atherosclerotic_lesions 0.405906 oxidized_low-density_lipoprotein_(ox-LDL) 0.6555159 monocyte_chemoattractant_protein-1 0.398739 oxidized_low-density_lipoprotein_(oxLDL) 0.65496510 plaque_destabilization 0.398201 (LOX-1) 0.65209911 oxidized_low-density_lipoprotein_(oxLDL) 0.397967 proatherosclerotic 0.65157112 atherosclerosis_atherosclerosis 0.396677 receptor-1_(LOX-1)_is 0.649000Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 13 of 24 Using data from auxiliary file CBOW.xls, the Fleisssmulti-? for the four raters in Experiment I is0.763205 and for Experiment II is 0.730869. TheKrippendorff s ? for the four raters in Experiment Iis 0.763211 and for Experiment II is 0.730874. Using data from auxiliary file Skip-gram.xls, theFleisss multi-? for the four raters in Experiment I is0.794919 and for Experiment II is 0.673514. TheKrippendorff s ? for the four raters in Experiment Iis 0.794938 and for Experiment II is 0.674181.As expected, the values obtained for the Fleisssmulti-? and the Krippendorff s ? for the four ratersare nearly identical. The inter-annotator agreement islower for Experiment II, which is more challenging interms of biological background knowledge. Camon etal. [104] reports that the chance of curator agreementis 39% to 43% when annotating proteins in the Uni-ProtKB with terms from the GO. Hence, inter-annotator agreement from 0.6734 (lowest value forFleisss multi-?) to 0.7949 (highest value for Fleisssmulti-?) appears reasonable.Receiver operating characteristics (ROC) curve and the areaunder the ROC curve (AUC)Using data from auxiliary files CBOW.xls and Skip-gram.xls, we plotted the ROC curves. For each RaterA-D the ROC curves are shown in Figs. 2, 3, 4 and 5 re-spectively. The ROC curves on the left-hand side plotFTV, PTV, and TV (i.e. the combination of FTV andPTV) for CBOW in Experiment I (abbreviated as Exp I)and Experiment II (abbreviated as Exp II). The ROCcurves on the right-hand side plot FTV, PTV, and TVfor Skip-gram in Experiment I and II.Looking at the AUC values for FTV, PTV, and TV inFigs. 2, 3, 4 and 5, it can be observed that for all fourraters: The AUC values for FTV, PTV, and TV are alwaysgreater than 0. 5 (i.e. better than random guessing) forboth CBOW and Skip-gram in Experiments I and II. The AUC values for TV are always greater inExperiment II than in Experiment I for both CBOWand Skip-gram. The AUC values for TV are always greater for Skip-gram than for CBOW in both Experiment I and II. The AUC values for PTV are always greater inExperiment II than in Experiment I for both CBOWand Skip-gram. The higher AUC values are for FTV with bothCBOW and Skip-gram. The maximum AUC values are for FTV inExperiment II with Skip-gram.The only noticeable discrepancy is that for threeRaters (A, C, and D), CBOW has the higher AUC valuesfor FTV in Experiment II, and for Rater B the higherAUC value for CBOW is for FTV in Experiment I.Table 6 Exemplifying human judgements and voting system for Skip-gram: Categories FTV, PTV, or NTV assigned for the four humanraters (A, B, C, and D) to the top twelve candidate terms for the target term oxidized_low-density_lipoprotein receptor_ 1 OLR1 inExperiment II using Skip-gram. The last three columns show the voting system (VS) applied for FTV (full term variant), FTV amongthe top three, and TV (full and/or partial term variant). The two rows in grey background remark how two almost identical candidateterms from the word embeddings are marked differently by rater C, and thus, the manual annotation by raters is error-proneArguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 14 of 24Considering the ROC curves and the AUC values, weconclude that: a) Skip-gram outperforms CBOW in bothExperiments for the binary classification problem TV andnon-TV, b) both CBOW and Skip-gram perform best forthe binary classification problem FTV and non-FTV, c) thebest performance is for Skip-gram in Experiment II for thebinary classification problem FTV and non-FTV.The median of the rank per human raterUsing data from the auxiliary files CBOW.xls and Skip-gram.xls, we calculated the median of the rank. Table 7shows the median of the rank for Raters A-D. From Table 7: For CBOW and Skip-gram in Experiment II, themean of the median of the rank for an FTV is 3. For CBOW and Skip-gram in Experiment I, themean of the median of the rank for an FTV is 4. For Skip-gram in Experiment I and II, the median ofthe rank for a PTV is 6 for all four raters. For CBOW in Experiment II, the median of the rankfor a PTV is 5 for all four raters. For CBOW in Experiment I, the mean of themedian of the rank for a PTV is 6.The higher the rank (i.e. lowest number) for an FTVthe better, and thus, results obtained for both CBOWand Skip-gram indicate that CVDO can slightly improvethe ranking of an FTV from being among the top fourranked candidate terms in Experiment I (without the aidof the CVDO) to be among the top three ranked candi-date terms in Experiment II (with the aid of the CVDO).Fig. 2 ROC curves for rater A: left-hand side CBOW and right-hand side Skip-gram. Abbreviations: Exp I = Experiment I; Exp II = Experiment IIFig. 3 ROC curves for rater B: left-hand side CBOW and right-hand side Skip-gram. Abbreviations: Exp I = Experiment I; Exp II = Experiment IIArguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 15 of 24Number of UniProtKB entries mapped to CVDO gene andprotein classes with term variantsThe auxiliary file TermsMapped_votingSystem.xlscontains the results of the voting system according tothe simple categorisation introduced (see subsectionSetup of Experiment I and Experiment II for a gene/protein synonym detection task), which has beenapplied to the terms from PubMed abstract/title fromthe small-annotated corpus (Category for terms from thetitle/abstract column) as well as to the target terms (Cat-egory for target terms column). The file has 153 targetterms, each with a unique identifier in column nQ thatalso appears in each auxiliary file under the column nQ.Of these 153 target terms: 85 target terms for 64 Uni-ProtKB entries are mapped to CVDO protein and geneclasses in Experiment I (abbreviated as Exp I), and 68target terms for 63 UniProtKB entries are mapped toCVDO protein and gene classes in Experiment II (abbre-viated as Exp II). The last six columns display the presence(i.e. value equals 1) or absence (i.e. value equals 0) for eachneural language model CBOW and Skip-gram of: fullterm variants (i.e. FTV) among the top twelve ranked can-didate terms for the target term; FTV among the top threeranked candidate terms for the target term; and term vari-ants (i.e. FTV and/or PTV) among the top twelve rankedcandidate terms for the target term.Tables 811 take the data from auxiliary file Terms-Mapped_votingSystem.xls and summarise the resultsobtained.Table 8 shows the overall performance of CBOW andSkip-gram in Experiment I and II according to thevoting system, which can be summarised as follows:Fig. 4 ROC curves for rater C: left-hand side CBOW and right-hand side Skip-gram. Abbreviations: Exp I = Experiment I; Exp II = Experiment IIFig. 5 ROC curves for rater D: left-hand side CBOW and right-hand side Skip-gram. Abbreviations: Exp I = Experiment I; Exp II = Experiment IIArguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 16 of 24 In Experiment I, Skip-gram finds term variantsamong the top twelve ranked candidate terms(Number TV column) for 89% of the 64 uniqueUniProtKB entries mapped to CVDO gene andprotein classes, while CBOW finds term variants for67%. Hence, using as target terms only terms fromPubMed titles/abstracts, the word embeddingsgenerated with the 14 M PubMed dataset can obtaina list of term variants for gene/protein names. In Experiment II (with the aid of the CVDO), Skip-gramfinds term variants among the top twelve ranked candi-date terms (Number TV column) for 95% of the 63unique UniProtKB entries mapped to CVDO gene andprotein classes, while CBOW finds term variants for78%. Hence, both neural language models Skip-gramand CBOW provide more term variants (FTVs and/orPTVs) if the CVDO is used to provide more context forthe target terms, and therefore increasing the chances offinding suitable term variants for a gene/protein name. Combining the results of both experiments, Skip-gramfinds term variants (FTVs and/or PTVs) among the toptwelve ranked candidate terms for 97% of the 79 Uni-ProtKB entries mapped to CVDO gene and proteinclasses, while CBOW finds term variants for 81%. The number of term pairs in Experiment I is 1020while in Experiment II it is 816, however more termvariants are found in Experiment II. Hence,knowledge from the CVDO (i.e. mostly the proteinclass expressions along with lexical content fromprotein class labels) to make the term targets moreefficient as fewer term pairs are needed to producemore term variants.Table 9 shows the performance of CBOW and Skip-gram according to the voting system and considers thenumber of UniProtKB entries that participate in each ex-periment. The third column contains the number of targetterms for the experiment considering the number of Uni-ProtKB entries, where Experiment I has a higher numberof target terms per UniProtKB entry than Experiment II.In Table 9 there are some rows with a grey background;they refer to the 48 UniProtKB entries that participate inboth Experiments. There are 16 UniProtKB entries thatparticipate only in Experiment I and 15 UniProtKB entriesthat participate only in Experiment II. Considering eachnumber of UniProtKB entries in an Experiment, it can beobserved that Skip-gram always outperforms CBOW andfinds more FTVs among the top twelve ranked candidateterms (Number FTV column); FTVs among the top threeranked candidate terms (Number FTV for the top threecolumn); and TVs among the top twelve ranked candidateterms (Number TV column). By considering only the 48UniProtKB entries that participate in both Experiments, itcan be observed that:Table 7 Median of the rank for CBOW and Skip-gram in Experiments I and II for each of the four ratersExperiment Model Rater A Rater B Rater C Rater DMedian FTV Median PTV Median FTV Median PTV Median FTV Median PTV Median FTV Median PTVI CBOW 4 5 3 7 4 6 4 6II CBOW 3 5 4 5 3 5 3 5I Skip-gram 4 6 4 6 4 6 4 6II Skip-gram 3 6 4 6 3 6 3 6Table 8 Overall performance of CBOW and Skip-gram according to the voting system: Number of unique UniProtKB entries andnumber of term pairs for protein/gene names that are involved in Experiment I, II, and combined (i.e. merging Experiment I and II)Voting systemExperiment Model Number of terms pairs Number of UniProtKB entries NumberFTVNumber FTV for top three Number TV(%)I CBOW 1020 64 31 21 43 (67%)II CBOW 816 63 29 21 49 (78%)I and IIcombinedCBOW 1836 79 47 37 64 (81%)I Skip-gram 1020 64 49 37 57 (89%)II Skip-gram 816 63 56 51 60 (95%)I and IIcombinedSkip-gram 1836 79 71 63 77 (97%)According to the voting system, for each model the last three columns show: the number of full term variants among the top twelve ranked candidate terms forthe UniProtKB entries (Number FTV column); the number of full term variants among the top three ranked candidate terms for the UniProtKB entries (Number FTVfor top three); and the number and % of term variants (i.e. FTV and/or PTV) among the top twelve ranked candidate terms for the UniProtKB entries (NumberTV column)Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 17 of 24 CBOW finds TVs (Number TV column) amongthe top twelve ranked candidate terms for 30 ofthe 48 UniProtKB entries in Experiment I (i.e.62%) and for 37 of the 48 UniProtKB entries inExperiment II (i.e. 77%). Skip-gram finds TVs (Number TV column) among thetop twelve ranked candidate terms for 42 of the 48UniProtKB entries in Experiment I (i.e. 87%) and for 45of the 48 UniProtKB entries in Experiment II (i.e. 93%).Tables 10 and 11 display the performance of CBOW andSkip-gram for Experiments I and II respectively accordingto the voting system and considering the categorisation in-troduced (see subsection Setup of Experiment I and Ex-periment II for a gene/protein synonym detection task)that has been applied to the terms from PubMed abstract/title from the small-annotated corpus (first column) as wellas to the target terms (second column). From these two ta-bles, it can be observed: In Table 10, corresponding to Experiment I, the highernumber of target terms corresponds to the categoryOnly gene symbol (two rows with a grey background)with a total of 34 target terms. CBOW finds TVsamong the top twelve ranked candidate terms (nTVcolumn) for 19 of them (i.e. 56%), while Skip-gramfinds TVs among the top twelve ranked candidateterms (nTV column) for 29 of them (i.e. 85%). In Table 11, corresponding to Experiment II, thehigher number of target terms corresponds to theTable 9 Performance of CBOW and Skip-gram - Experiment I and Experiment II: Number of unique UniProtKB entries mapped toCVDO gene and protein classes that participated in Experiment I or IIThe rows with grey background remark the 48 UniProtKB entries that participate in both Experiment I and II. Each row of the third column contains the numberof target terms for the experiment taking into account the number of UniProtKB entries. According to the voting system, for each model and experiment, the lastthree columns show: the number of full term variants among the top twelve ranked candidate terms for the UniProtKB entries (Number FTV column); the numberof full term variants among the top three ranked candidate terms for the UniProtKB entries (Number FTV for top three); and the number of term variants (i.e. FTVand/or PTV) among the top twelve ranked candidate terms for the UniProtKB entries (Number TV column)Table 10 Results for Experiment I according to the voting system and the simple categorisation introduced: Results of the voting systemaccording to the simple categorisation introduced (see Setup of Experiment I and Experiment II for a gene/protein synonymdetection task), which has been applied to the terms from PubMed abstract/title from the small-annotated corpus (first column) aswell as to the target terms (second column)Abbreviations: n = number of target terms; nFTV= number of target terms that have a FTV among the top twelve candidate terms; nFTVr3= number of target terms thathave a FTV among the top three candidate terms; nTV = number of target terms that have a TV (i.e. FTV and/or PTV) among the top twelve candidate termsArguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 18 of 24category Gene symbol appears (three rows with agrey background) with a total of 53 target terms.CBOW finds TVs among the top twelve rankedcandidate terms (nTV column) for 39 of them (i.e.74%), while Skip-gram finds TVs among the toptwelve ranked candidate terms (nTV column) for 50of them (i.e. 94%). Comparing results of the voting system for CBOWand Skip-gram, corresponding to both Experiments I(Table 10) and II (Table 11), Skip-gram always ob-tains an equal or higher number than CBOW for:FTVs variants among the top twelve rankedcandidate terms (nFTV column), FTVs among thetop three ranked candidate terms (nFTVr3 column);and TVs (FTVs and/or PTVs) among the top twelveranked candidate terms (nTV column).Table 8 shows, corresponding to both Experiments Iand II, the number of FTVs among the top twelveranked candidate terms (Number FTV column) forSkip-gram is higher than the number of TVs among thetop twelve ranked candidate terms (Number TV col-umn) for CBOW. To further illustrate this: a) in Experi-ment I, CBOW finds 43 TVs while Skip-gram finds 49FTVs, and b) in Experiment II, CBOW finds 49 TVswhile Skip-gram finds 56 FTVs. Tables 10 and 11 pro-vide more details based on the categorisation intro-duced; it can be observed that for both Experiments Iand II, the number of FTVs among the top twelveranked candidate terms (nFTV column) for Skip-gramis always equal to or greater than the number of TVsamong the top twelve ranked candidate terms (nTV col-umn) for CBOW.We conclude that: a) Skip-gram outperforms CBOWin both Experiments and finds more TVs and FTVs; b)the number of FTVs in both Experiments for Skip-gramis equal to or greater than the number of TVs forCBOW; and c) both Skip-gram and CBOW find moreTVs and FTVs in Experiment II (with the aid of theCVDO) than in Experiment I.DiscussionThe CVDO has a limited lexical content, where eachgene and protein class has only one name (i.e. the valueof the rdfs:label), and thus lacks term variants (e.g. syno-nyms and acronyms) for genes/proteins. Keeping theCVDO up-to-date in this respect is a challenge sharedwith the typical biologist. As Jensen et al. [65] acknow-ledge that for the typical biologist, hands-on literaturemining currently means a keyword search in PubMed.Both biological entity annotations (gene/protein and or-ganism/species) and molecular interaction annotations(protein-protein and genetic interactions) of the free-text scientific literature are needed to support queriesfrom biologists that may use different names to refer tothe same biological entity. However, identification ofbiological entities within the literature has proven diffi-cult due to term variation and term ambiguity [105], be-cause a biological entity can be expressed by variousrealisations. A large-scale database such as PubMedcontains longer forms including phrases (e.g. serumamyloid A-1 protein) as well as shorter forms such asabbreviations or acronyms (e.g. SAA). Finding all termvariants in text is important to improve the results of in-formation retrieval systems such as PubMed that trad-itionally rely on keyword-based approaches. Therefore,Table 11 Results for Experiment II according to the voting system and the simple categorisation introduced: Results of the votingsystem according to the simple categorisation introduced (see Setup of Experiment I and Experiment II for a gene/protein synonymdetection task), which has been applied to the terms from PubMed abstract/title from the small-annotated corpus (first column) aswell as to the target terms (second column)Abbreviations: n ; number of target terms; nFTV ; number of target terms that have a FTV among the top twelve candidate terms; nFTVr3; number of target termsthat have a FTV among the top three candidate terms; nTV; number of target terms that have a TV (i.e. FTV and/or PTV) among the top twelve candidate termsArguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 19 of 24the number of documents retrieved is prone to changewhen using acronyms instead of and/or in combinationwith full terms [106, 107].This study investigates to what extent word embed-dings can contribute to keeping the CVDO up-to-datewith new biomedical publications, and furthermore ifthe CVDO itself can aid such update. Experiment I in-vestigates whether, in taking a gene/protein name fromPubMed titles/articles as a target term, it is possible toobtain a list of term variants from the word embeddingscreated with a 14 M PubMed dataset. The resultsobtained for Experiment I confirm that it is feasible andthat Skip-gram finds 22% more term variants thanCBOW using 85 target terms that correspond to 64UniProtKB entries, which are mapped to CVDO geneand protein classes. Experiment II investigates if thesame word embeddings used in Experiment I can pro-duce a better list of term variants (i.e. more term vari-ants) using as target terms a combination of gene/protein names from PubMed titles/abstracts with terms(i.e. more context) from the CVDO protein class expres-sions and labels. The results obtained for Experiment IIshow an improvement in performance of CBOW by 11%and Skip-gram by 6% using 68 target terms (fewer targetterms than in Experiment I) that corresponds to 63UniProtKB entries, which are mapped to CVDO geneand protein classes. In Experiment II (with the aid of theCVDO), not only is a better list of gene/protein termvariants obtained but also a better ranking, where a full-term variant is likely to appear among the top threeranked candidate terms. Hence, the CVDO supplies con-text that is effective in inducing term variability whilstreducing ambiguity.Studies related to semantic similarity and relatednesstasks employ gold standards specific for the biomedicaldomain that have a relatively small number of termpairs, such as Caviedes and Cimino [38] with 10 term/concept pairs, Pedersen et al. [34] with 30 term/conceptpairs, and Pakhomov et al. [36] with 724 term pairs. Thisstudy considers a total of 3672 term-pairs from the twoexperiments together with human judgments from fourraters. Hence, an outcome of this study is the creation ofa gene/protein names dataset (larger than the MEN TestCollection [40] with 3 K common English word-pairs)that can be reused for the evaluation of semantic modelsin a gene/protein synonym detection task. However, theoverall setup of the two experiments is unbalanced as aresult of capturing a realistic scenario where: a) somegene/protein names appearing in PubMed titles/ab-stracts do not have a vector representation; and b) agene and its product (typically a protein) can appear to-gether in the scientific text, and thus, the biologicalknowledge formally represented in the CVDO is alreadypresent.Considering only the 48 UniProtKB entries mapped toCVDO gene and protein classes that participate in bothExperiment I and II, the asymmetry between the two ex-periments can be reduced leading to a smaller gene/pro-tein names dataset with: a) 660 pairs of terms (targetterm and candidate term) taken from the word embed-dings created with CBOW and Skip-gram (i.e. total of1320 term pairs) and assessed by four raters in Experi-ment I; and b) 624 pairs of terms taken from the wordembeddings created with CBOW and Skip-gram (i.e. atotal of 1248 term pairs) and assessed by four raters inExperiment II. Considering only these 2568 term-pairsinstead of the total of 3672 term-pairs from the two ex-periments, the performance obtained for CBOW andSkip-gram is the same as the overall performance re-ported with Skip-gram outperforming CBOW in bothExperiments; and both CBOW and Skip-gram find moreterm variants in Experiment II (with the aid of theCVDO) than in Experiment I.Besides the asymmetry between the two experimentspresented, there are certain areas of improvement possibleregarding the data resources. On one hand, the small-annotated corpus is very narrow in scope with only onecurator performing the gene/protein name annotation for25 PubMed articles (titles and abstracts). On the otherhand, the 14 M PubMed dataset used to generate theword embeddings can be arguably larger or include morerecent PubMed articles as it only contains titles and avail-able abstracts from PubMed articles published between2000 and 2016 (files up to 8th June 2016).As of today, data integration remains a challenge inthe life sciences, and therefore, the main curation effortfor the sysVASC project is in normalisation. Rebholz-Schuhmann et al. [3] emphasises the lack of a completesolution to normalise proteins and genes (e.g. uniqueprotein identifier together with protein properties andalternative names/labels) that facilitates recognisingthem from the scientific text. As part of this study, gene/protein names annotated from PubMed titles and/or ab-stracts are mapped to UniProtKB entries. Other studieshave also carried out normalisation whilst making nodistinction between genes/proteins. For example, Doganet al. [108] annotated genes/proteins of interest andmanually added their corresponding Entrez Gene identi-fiers. There are, however, studies that have a list of mul-tiple types of biomedical entities, such as PubTator[109], and BEST [110]. PubTator considers 5 biomedicalentities and BEST considers 10 biomedical entities. BothPubTator and BEST perform daily updates of PubMedcontent and both have automated identification of bio-medical entities such as genes. Neither PubTator norBEST, however, distinguish between proteins and genes.The results obtained for Experiment II suggest benefitsin using target terms belonging to the category GeneArguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 20 of 24symbol appears introduced  using terms from proteinclass expressions and labels from the CVDO (or thePxO)  with Skip-gram to automatically obtain the topthree ranked candidate terms for a gene/protein of inter-est. Although this study does not present a tool, it sug-gests that the CVDO can provide a better context andimprove the performance of CBOW and Skip-gramwithout modifying the word embeddings (i.e. no post-processing of the term vectors is performed), and thiscould be the foundation for building a tool similar toPubTator or BEST. As the CVDO and the PxO are for-malised in OWL, it seems natural to envision a toolbased on Semantic Web technologies, such as OWL andSPARQL. Therefore taking into account two annotationproperties from SKOS, i.e. skos:altLabel and skos:hidden-Label, we can define the automation for the gene/pro-tein synonym detection task as: for each CVDO protein,find term variants for the string values within skos:altLa-bel and store them in skos:hiddenLabel.Levy et al. [29] remarks that if different models areallowed to tune a similar set of hyperparameters, theirperformance is largely comparable. The neural languagemodels CBOW and Skip-gram have a similar set ofhyperparameters, and thus, their performance has beenalready compared when accomplishing biomedical tasks[41, 42]. Muneeb et al. [41] applied different hyperpara-meter configurations and reported a better performancefor Skip-gram than CBOW in a semantic similarity andrelatedness task for biomedical concepts. Chiu et al. [42]performed a systematic exploration of different hyper-parameter configurations and reported an overall betterperformance for Skip-gram than CBOW in word simi-larity and NER tasks using biomedical corpora. Thisstudy also shows a better performance for Skip-gramthan CBOW in a gene/protein synonym detection taskconsidering two metrics: the AUC for the binary classifi-cation problem TV and non-TV; and the number ofterm variants found for 79 UniProtKB entries. We, how-ever, used the same hyperparameter configuration forCBOW and Skip-gram in a study about patient safety[111] and it was not possible to determine which(CBOW or Skip-gram) had better performance on anNER task. This study does not exploit Skip-gram withnegative sampling, which typically improves its perform-ance [29]. Furthermore, this study does not systematic-ally explore alternative hyperparameter configurationsthat may lead to performance gains.As far as we are aware, the use of ontologies to pro-vide more context (i.e. extra terms) for terms selectedfrom the scientific literature has not previously been in-vestigated. This paper demonstrates that the CVDO, andby extension the PxO, can provide better target termsfor a gene/protein synonym detection task without alter-ing the word embeddings created by Deep Learningalgorithms CBOW and Skip-gram from a 14 M PubMeddataset. At the time of writing BioPortal [112], an openrepository of biomedical ontologies, has 551 ontologies.The PxO is re-used by CVDO and is in BioPortal. Theexperiments reported here can be replicated, and do notdemand post-processing of the word embeddings cre-ated with CBOW or Skip-gram to obtain performancegains. Therefore, other ontologies from BioPortal maybenefit from our proposal to anchor the CVDO in thebiomedical literature.ConclusionThis study shows performance improvements for bothCBOW and Skip-gram on a gene/protein synonym de-tection task by adding knowledge formalised in theCVDO and without modifying the word embeddingscreated. Hence, the CVDO supplies context that is ef-fective in inducing term variability for both CBOW andSkip-gram while reducing ambiguity. Skip-gram outper-forms CBOW and finds more pertinent term variantsfor gene/protein names annotated from the scientificliterature.Additional filesAdditional file 1: TermsMapped.xls, this file contains the mappingperformed for the 105 terms from 25 PubMed titles/abstracts to 79UniProtKB identifiers (ACs and IDs) along with the CVDO gene andprotein classes labels. (XLS 34 kb)Additional file 2: CBOW.xls, this file shows the results for CBOW perexperiment and rater. (XLSX 175 kb)Additional file 3: Skip-gram.xls, this file shows the results for Skip-gramper experiment and rater. (XLS 465 kb)Additional file 4: TermsMapped_votingSystem.xls, this file contains thedetails of the voting system for CBOW and Skip-gram per experiment.(XLS 70 kb)Additional file 5: pairwiseIAA.xls, this file contains the values of theCohens Kappa measure for each pair of raters per experiment andmodel, as well as the average mean. (XLS 8 kb)AcknowledgementsThanks to Tim Furmston for help with software and e-infrastructure, and tothe anonymous reviewers for their useful comments.FundingThis work was supported by a grant from the European Union SeventhFramework Programme (FP7/20072013) for the sysVASC project under grantagreement number 603288.Availability of data and materialsAll data generated or analysed during this study are included in thispublished article and its Additional files 1, 2, 3, 4 and 5.Authors contributionsAll authors contributed to the development of the design of the methodand experiments as well as the writing of the paper. All authors read andapproved the final manuscript.Competing interestThe authors declare that they have no competing interests.Arguello Casteleiro et al. Journal of Biomedical Semantics  (2018) 9:13 Page 21 of 24Ethics approval and consent to participateThe human raters have consented to make their anonymised judgementspublically available.Consent for publicationNot applicable.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1School of Computer Science, University of Manchester, Manchester, UK.2Salford Languages, University of Salford, Salford, UK. 3Departamento deLingüística Aplicada a la Ciencia y a la Tecnología, Universidad Politécnica deMadrid, Madrid, Spain. 4Midcheshire Hospital Foundation Trust NHS, Crewe,England, UK. 5Manchester Institute of Biotechnology, University ofManchester, Manchester, UK. 6Institut National de la Santé et de la RechercheMedicale (INSERM) U1048, Toulouse, France. 7Universite Toulouse III PaulSabatier, route de Narbonne, Toulouse, France.Received: 27 September 2017 Accepted: 6 March 2018Amith and Tao Journal of Biomedical Semantics  (2018) 9:22 https://doi.org/10.1186/s13326-018-0190-0RESEARCH Open AccessRepresenting vaccine misinformationusing ontologiesMuhammad Amith and Cui Tao*AbstractBackground: In this paper, we discuss the design and development of a formal ontology to describe misinformationabout vaccines. Vaccine misinformation is one of the drivers leading to vaccine hesitancy in patients. While there arevarious levels of vaccine hesitancy to combat and specific interventions to address those levels, it is important to havetools that help researchers understand this problem. With an ontology, not only can we collect and analyze variedmisunderstandings about vaccines, but we can also develop tools that can provide informatics solutions.Results: We developed the Vaccine Misinformation Ontology (VAXMO) that extends the Misinformation Ontologyand links to the nanopublication Resource Description Framework (RDF) model for false assertions of vaccines.Preliminary assessment using semiotic evaluation metrics indicated adequate quality for our ontology. We outlinedand demonstrated proposed uses of the ontology to detect and understand anti-vaccine information.Conclusion: We surmised that VAXMO and its proposed use cases can support tools and technology that can pavethe way for vaccine misinformation detection and analysis. Using an ontology, we can formally structure knowledgefor machines and software to better understand the vaccine misinformation domain.Keywords: Vaccine, Misinformation, Ontology, Natural language processing, Semantic web, Semantic similarity,MicroattributionBackgroundSince their introduction, vaccines have been an impor-tant breakthrough that has led to the near-eradication ofmany infectious diseases. Some of these diseases includepolio, typhoid, and smallpox - all which are now uncom-mon. But in themodern era, certain sectors of society haveembraced a post-modernist approach that endorses thatscience and experts are open to questioning ... put[ting]greater emphasis on intuition and social relationships andtends to distrust the scientific method as the best paths tohealing our ills [1]. This, compounded with various otherfactors including misinformation about vaccines, has pre-sented a problem in vaccine uptake into the population.The effects of this are troublesome, considering in one poll20% of those surveyed believed that there is a link betweenautism and vaccine [2], in a Gallup poll, 58% are eitherunsure or actually believe that vaccines cause autism [3],and 11% presume that vaccines are not necessary and 25%*Correspondence: cui.tao@uth.tmc.eduSchool of Biomedical Informatics, The University of Texas Health ScienceCenter, 7000 Fannin Street, Suite 600, Houston, TX, USApresume that autism is a side-effect of vaccines in anothersurvey of parents [4].Vaccine skepticism dates back as far as the 19th century,when the United Kingdom introduced the VaccinationAct of 1853 requiring compulsory inoculation of children.Backlash to the law emerged with the formation of theAnti-Compulsory Vaccination League and ensuing pub-lications to advocate anti-vaccination beliefs and ideas[5, 6]. In the 20th century, the retracted study by AndrewWakefield that claimed a link between vaccine and autismhad an unfortunate impact on vaccine discourse and thedecline of MMR vaccine rates in certain regions of theworld [7, 8]. Even to this day, Andrew Wakefield is stillpropagating the same discredited vaccine claims, and alsohas directed a documentary called Vaxxed:From Cover-Up to Catastrophe that received a special screening at theCannes Film Festival [9]. Other figures, like U.S. PresidentDonald Trump [10], Robert Kennedy, Jr of the Kennedyfamily [11], Dr. Robert Sears [12], Alex Jones [13], BillMaher [14], Jenny McCarthy [15, 16], etc., have continuedto express distorted claims about vaccines.© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Amith and Tao Journal of Biomedical Semantics  (2018) 9:22 Page 2 of 13In the information age, the unregulated nature of theWeb has provided free discourse and information shar-ing to anyone with a computer and Internet access. Tosome researchers, the Web is a Pandoras Box thathas both benefits and costs [17, 18], particularly itsimpact on health-seeking knowledge. In a Pew Researchpoll from 2013 [19], a majority of those surveyed (73%)sought health-related information with a third of those(35%) diagnosing themselves as opposed to seeing a doc-tor. In the same study, of the individuals who soughtvaccine information (17%), 70% made a decision aboutvaccination based on the information they found. Thismay be troubling, as previous studies have highlightedthat anti-vaccination websites appear highly ranked insearch engine hits [17, 20]. Additionally, social media plat-forms have a significant impact on vaccination attitudes[17, 2124]. Overall, the proliferation of vaccine misinfor-mation is accessible to anyone with a mobile device andlimited time to perform extensive research.There are previous studies that have looked at the con-tent of vaccine misinformation and motivation, but nonethat have investigated informatics tools that can assistand automate the analysis of vaccine misinformation tounderstand the drivers behind these false notions. Thetheoretical benefit of such tools can help process massiveamount of content (i.e. social media posts), and also dis-cover new knowledge that may not be apparent throughmanual human analysis. Numerous previous studies canhelp inform the development of tools and technology toaccomplish this objective.We aimed to use semantic web and ontological technol-ogy to represent the domain scope of vaccine misinforma-tion. Also, with ontological representation, we intendedto use this artifact to store various misconceptions aboutvaccines. This would eventually assist in a catalogue mis-information that can be queried and analyzed for futureresearch. While some vaccines are associated with spe-cific misinformation, we focused in this study on thegeneral domain. The Vaccine Misinformation Ontology(VAXMO) is composed of existing ontologies - Misinfor-mation Ontology and nanopublications - and is extendedwith features pertinent to the anti-vaccine domain. Lastly,we introduced possible use cases that will involve the vac-cine misinformation ontology to identify misinformationfor text-mining tasks and other applications.Semantic web and ontologiesThe word ontology has its roots in metaphysical philoso-phy, extending back to Aristotles Categories, as a natureof being. In the early 90s, the definition of ontology wasapplied in the computer science field as a specification ofa conceptualization. [25]. At the turn of the century, SirTim Berners-Lee described his vision for the next genera-tion web called the semantic web in Scientific America,where ontologies would be the foundation for this vision[26]. Simply, an ontology is a machine-readable artifactthat encodes a logical representation of a domain spaceusing vocabularies, and their semantic meanings. It is theoutput of a knowledge engineering process where toolsand methods are used to build the ontology [27]. Over-all, ontologies are used for representing information andknowledge [2830].In general, knowledge in an ontology is representedas triple which is information presented in subject >predicate > object. Essentially, the subject > predicate >object are concepts that are smallest, unambiguous unitof thought ... [that are] uniquely identifiable [31]. Eachtriple can seamlessly link to another triple to form anontological knowledge-base. For this knowledge to bereadable by a machine, we use a computer-based syntaxto encode this knowledge. Once encoded, this artifact canbe shared and distributed for various purposes. More-over, using Web Ontology Language (OWL) or ResourceDescription Framework (RDF), a specific type of webontology language syntax for ontologies, we can definemore complex axioms and assertions to fully describeconcepts which provide machine reasoning capabilities.Nanopublication primerSemantic web technologies, specifically ontologies, havehad continued impact on research and knowledge shar-ing, and standardization in the biomedical domain. Someof what has been described were the benefits of formal-izing information, information integration, informationreuse, and querying and search, etc. We introduce the useof nanopublication, which is an ontology-based micro-publishing format for encoding and distributing singularunits of assertions. Nanopublications have been used pri-marily in the life sciences, pharma sciences, as well asgenomics and proteomic research data [32]. The benefitof nanopublications include [32]: Improve finding of scientific information Connect scientific information from multiple sources Organize provenance information of the researchfinding Verifiable SmallThe model or structure of a nanopublication involvesa scientific assertion, provenance of the assertion, andprovenance information of the nanopublication itself [33].The scientific assertion component is the singular atomicfinding that is represented as subject > predicate >object. An example would be trastuzumab [subject] isindicated for (treats)[predicate] breast cancer[object].The other component is the provenance of the assertion,or the origin or source of something [34], which willAmith and Tao Journal of Biomedical Semantics  (2018) 9:22 Page 3 of 13express metadata information, like DOI, authors, researchinstitution, time and date, experimental method, etc. Thethird part is the provenance information about the nanop-ublication, which generally indicates who created thenanopublication and when it was created (analogous tocitation metadata).Provided (Listing 1) is a basic example of a nanopubli-cation encoding for the research assertion, trastuzumabis indicated for (treats) breast cancer. Specific discussionof the encoding is outside the scope of this proposal,Osborne et al. Journal of Biomedical Semantics  (2018) 9:2 DOI 10.1186/s13326-017-0173-6RESEARCH Open AccessCUILESS2016: a clinical corpus applyingcompositional normalization of text mentionsJohn D. Osborne1, Matthew B. Neu1, Maria I. Danila1, Thamar Solorio2 and Steven J. Bethard3*AbstractBackground: Traditionally text mention normalization corpora have normalized concepts to single ontologyidentifiers (pre-coordinated concepts). Less frequently, normalization corpora have used concepts with multipleidentifiers (post-coordinated concepts) but the additional identifiers have been restricted to a defined set ofrelationships to the core concept. This approach limits the ability of the normalization process to express semanticmeaning. We generated a freely available corpus using post-coordinated concepts without a defined set ofrelationships that we term compositional concepts to evaluate their use in clinical text.Methods: We annotated 5397 disorder mentions from the ShARe corpus to SNOMED CT that were previouslynormalized as CUI-less in the SemEval-2015 Task 14 shared task because they lacked a pre-coordinated mapping.Unlike the previous normalization method, we do not restrict concept mappings to a particular set of the UnifiedMedical Language System (UMLS) semantic types and allow normalization to occur to multiple UMLS ConceptUnique Identifiers (CUIs). We computed annotator agreement and assessed semantic coverage with this method.Results: We generated the largest clinical text normalization corpus to date with mappings to multiple identifiersand made it freely available. All but 8 of the 5397 disorder mentions were normalized using this methodology.Annotator agreement ranged from 52.4% using the strictest metric (exact matching) to 78.2% using a hierarchicalagreement that measures the overlap of shared ancestral nodes.Conclusion: Our results provide evidence that compositional concepts can increase semantic coverage in clinicaltext. To our knowledge we provide the first freely available corpus of compositional concept annotation in clinical text.Keywords: NLP, Information extraction, Concept normalization, Concept recognition, Fine grained named entityrecognitionBackgroundPost-coordinated concepts are concepts represented bycombining multiple concepts from an ontology, in con-trast to pre-coordinated concepts, which are explicitlypredefined and represented in an ontology by a sin-gle identifier. Post-coordinated concepts have been usedby medical ontological systems such as GALEN [1] andSNOMED CT [2] to elucidate a broader range of conceptsthan is possible with pre-coordinated systems [3, 4] usingdescriptive logic. This methodology relies on a restrictedset of pre-defined semantic relationships to avoid or min-*Correspondence: bethard@email.arizona.edu3School of Information, University of Arizona, 85721 Tucson, USAFull list of author information is available at the end of the articleimize semantic ambiguity. This is in contrast to GeneOntology [5], which until the recent introduction of anno-tation extensions [6], assigned multiple annotations to asingle protein without regard to the relationships betweenthe assigned annotations. Not requiring formal semanticrelationships for all multi-concept annotations may intro-duce some semantic ambiguity, but allows higher seman-tic coverage in situations where the source text describesa concept whose logical description cannot be capturedby the set of pre-existing semantic relationships. Indeed,the ideal that an ontology of medicine can express all andonly what is medically sensible has been termed unob-tainable and focusing on all rather than only shouldtake precedence [7].© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Osborne et al. Journal of Biomedical Semantics  (2018) 9:2 Page 2 of 9In clinical interface systems utilizing SNOMED CT,complicated clinical concepts are typically created by clin-icians who select from a set of inter-related atomic con-cepts with pre-defined relations. However the creationof a publicly available clinical text corpus with post-coordinated normalization training data has received lessattention. This is likely due the difficulty and cost of cre-ating and sharing such a corpus. Moreover, earlier work[8] comparing normalization between different SNOMEDCT encoding groups that applied post-coordination tonormalize text mentions in case report forms failed to findany statistically significant semantic agreement.More recently, post-coordination has been applied inbiomedical corpus construction with the creation ofthe NCBI Disease Corpus [9]. During corpus creation,Do?gan first attempted to normalize disease mentions inPubMed abstracts to the MEDIC vocabulary using pre-coordinated concepts, which was successful for 91% ofthe disease mentions. For the remaining 9% of diseasementions, they employed a minimally restricted form ofpost-coordination that we term compositional normal-ization that allowed the use of multiple concepts withoutregard to specific relations or slots. They further cat-egorized these compositional concepts between aggre-gate or composite concepts that consisted of multipleself-contained pre-coordinated concepts in the text men-tion and composed concepts which collectively act todescribe a single concept. The aggregate concepts in thiscontext are simply concepts linked by logical operators(AND/OR) since no provision was made for logical oper-ator usage in the annotation. Examples are shown inTable 1.In the NCBI Disease corpus, only 76 such unique com-positional concepts were normalized (52 aggregate and24 composed) and annotator agreement for these post-coordinated concepts was not reported separately.In contrast to the open-ended nature of Do?ganscompositional concepts, Roberts [10] annotated post-coordinated concepts for only one predefined relation:anatomical location. Roberts work includes both a corpusannotated on medical consumer language and software tonormalize text mentions. However, the corpus containsonly 500 post-coordinated concept instances.SemEval-2015 Task 14 [11] annotated a corpus of clini-cal text with post-coordinated concepts, normalizing eachdisorder mention to a single SNOMED CT concept, andrestricting further post-coordination to 8 predefined rela-tions: body locations, which were normalized to UMLSanatomical concepts, and 7 other small-domain concepttypes. We refer to this corpus as SEMEVAL2015. TheSEMEVAL2015 section of Table 2 shows examples ofeach predefined relation. However, they report anno-tator agreement only for disorder mention normaliza-tion, not the overall normalization annotator agreementfor that mention which would include associated post-coordinated concepts or slots. They were also unableto normalize 30% of the disorder mentions (such men-tions are termed CUI-less) because annotators wereunable to find a single UMLS Concept Unique Iden-tifier (CUI) for the concept. This suggests that thereare limitations in the annotation process, the ontologybeing normalized to (SNOMED CT) or both, which pre-vent the full semantic capture of clinical text. This isknown as the content completeness problem, first coinedby Elkin [12, 13] but recognized earlier by Rogers andRector [14].In the current study we evaluate the extent to whichcompositional annotation, not restricted to a predefinedset of relations, can attenuate the content completenessproblem in clinical text. To address this problem, we gen-erate the largest corpus to date for this compositionalmethod. To our knowledge it is the first such composi-tional corpus in clinical text.MethodCorpus generationWe generated a novel dataset CUILESS2016 derivedfrom the part of ShARe corpus used for the SemEval-2015 Task 14 Shared Task [11], which we term,SEMEVAL2015. Only a subset of SEMEVAL2015 wasutilized, consisting of those disorder mentions that werenot normalized to SNOMED CT, so called CUI-lessdisorders because they lack a Unified Medical LanguageSystem (UMLS) CUI corresponding to a SNOMED CTconcept. Their distribution in the SEMEVAL2015 train-ing and development datasets is shown in Table 3.Table 1 Examples of pre-cordinated and post-coordinated concepts from the NCBI disease corpusType / Subtype Identifiers Text mention example Concept name/sPre-coordinated 1 Bone dysplasia Bone diseases, DevelopmentalCompositional /Aggregate (|)2 Breast or ovarian cancer Breast cancer|Ovarian cancerCompositional /Composed (+)3 Inherited neuromuscular disease Neuromuscular disease + Genetic diseases +InbornPost-coordinated concepts of type (aggregate or composed) have 2 or more identifiersOsborne et al. Journal of Biomedical Semantics  (2018) 9:2 Page 3 of 9Table 2 CUI-less examples from SEMEVAL2015 and CUILESS2016 annotation of ShARe corpusAggregate example Composed exampleSEMEVAL2015 Text mention RRW Surgical defectNegation Yes No*Subject Patient* Patient*Uncertainty No* YesCourse Unmarked* Unmarked*Severity Unmarked* Unmarked*Conditional False* False*Generic False* False*Body location CUI C0225754 (Both lungs) C1521748 (Entire mastoid)Disorder CUI CUI-less CUI-lessCUILESS2016 Disorder CUI C0034642 (Rhales) C0543467 (Operative surgery)C0035508 (Rhonchi) C2004491 (Cicatrix)C0043144 (Wheezing)An * indicates the default value for that slot in SEMEVAL2015. Our CUILESS2016 annotators added identifiers to describe the disorder when the Disorder CUI was markedCUI-less in SEMEVAL2015We re-annotated only the CUI-less disorder CUI; CUI-less body locations or other relations are not re-annotated,as shown in Table 2.Since test data was not readily available, only disor-der mentions from the development and training portionof SEMEVAL2015 were normalized. Approximately 30%(5397) of disorder mentions fit this CUI-less descrip-tion from a set of 298 training notes and a set of 133development notes. The 298 training note set was itselfderived from the notes used in the ShARe/CLEF eHealth2013 Evaluation Lab Task 1 [15]. Statistics for the inputSEMEVAL2015 corpus are provided in Table 4.Annotation methodWe used an open-ended compositional annotationmethodology similar to that of Do?gan [9] to normal-ize all 5397 CUI-less disorder mentions as describedin the Annotation Guidelines (Additional file 1). Exam-ples of our annotations are shown in the CUILESS2016Table 3 SEMEVAL2015 CUI-less distribution by clinical documenttypeData set Document type CUI-less count AverageCUI-lessby NoteDevelopment Discharge summaries 1929 13.9Training Discharge summaries 2796 20.6Training Echocardiogram 331 6.1Training Electrocardiogram 91 1.7Training Radiology 250 4.6Only discharge summaries were available for annotation in the developmentdocument setsection of Table 2. Rules for annotation were similar tothe ShARe/CLEF corpus [15] in that disorders were nor-malized to UMLS CUIs from SNOMED CT using themost specific CUI possible, ignoring negation and tempo-ral modifiers, including acronyms, abbreviations and, tothe fullest extent possible, mentions that are co-referentor anaphoric. There are some critical differences betweenthe ShARe/CLEF annotation and our method that allowus to annotate these additional mentions. They are:1 One or more identifiers were selected to annotate thetext mention if (and only if) no appropriate singleidentifier (pre-coordinated term) is found.2 All of SNOMED CT was available for mentionnormalization.3 The annotators could use existing SEMEVAL2015identifiers to create compositional concepts.For example, if the mention no bowel wall thickeningwas annotated, and no CUI in SNOMED CT existed forbowel wall thickening, but the SEMEVAL2015 annota-tions include a body location CUI for bowel wall andthe disorder was flagged as negated, then the text men-tion was normalized using just the CUI for ThickenedTable 4 SEMEVAL2015 and CUILESS2016 document statisticsSet Word countClinical note countDischarge ECG EKG RadiologyTrain 182K 136 54 54 54Development 153K 133 0 0 0Total 335K 269 54 54 54Osborne et al. Journal of Biomedical Semantics  (2018) 9:2 Page 4 of 9(fndg), since the other two concepts needed for post-coordination are already present in the SEMEVAL2015annotations.Unlike the work of Do?gan [9], we made no distinction asto whether the multiple CUIs used to annotate the spanwere aggregate or composed concepts. Thus, all of theCUIs in our mention were space separated and could rep-resent either aggregation (|) or concatenation (+) per theoperator nomenclature of Do?gan [9].Calculation of annotator agreementAnnotator agreement between the 2 annotators (MID andMN) on the development data set was computed in 2different ways.1 Exact Agreement - Annotators used exactly the sameset of CUIs to annotate the disorder text mention.We report only proportional agreement pa for thistask by which we mean the fraction of text mentionson which the annotators agree. Thus, in Table 5 (inthe Exact agreement row) we count only a singleagreement for both Drug Allergy and Levofloxacin,not 2 agreements. Proportional agreement can bedefined more formally as pa = m/n where m is thenumber of mentions where both annotators agreeand n is the total number of mentions. This shouldapproximate Cohens ? because agreement due tochance is expected to be extremely small. This is dueto the UMLS representation of SNOMED CT havingover 320K distinct CUIs and we allow an unboundednumber of CUIs per mention.2 Hierarchical Agreement - We compute hierarchicalagreement between annotators using the set ofannotated nodes and all their ancestors similar to thehierarchical precision and recall metric used byVerspoor [16]. It is calculated as:1nn?i=1({? Ai} ? {? Bi})/({? Ai} ? {? Bi}) (1)where {? Ai} indicates the set of annotated nodesand their ancestors from annotator A for mention i,{? B} indicates the set of annotated nodes and theirancestors from annotator B for mention i and n isthe total number of mentions annotated. In caseswhere an annotated CUI mapped to multipleSNOMED CT identifiers, SNOMED CT ancestorsfrom all paths were used.Software and dataAnnotations were mapped using BRAT 1.3 softwareas shown in Fig. 1 [17]. Annotators SP, ES, MN andMID normalized the training data to the US Editionof SNOMED CT (2013_03_01) as represented in UMLS2013AB. Development data was normalized to SNOMEDCT (2016_03_01) in UMLS 2016AA by annotators MIDand MN. Disorder CUIs found in the training data thatwere not present in SNOMED CT 2016_03_01 due tovocabulary changes or errors in the original annotationwere normalized to SNOMED CT (2016_09_01) by MIDand JDO.ResultsAs shown in Table 6 we found the majority of dis-order mentions had only a single identifier, whichreflects the expanded range of available concepts andour guidance to use pre-coordinated concepts pref-erentially as outlined in our annotation guidelines.However Table 6 under-represents the true disordermulti-identifier count since disorder CUIs can be post-coordinated with SEMEVAL2015 annotations that rep-resent disorder attributes. Thus no bowel wall thicken-ing would be counted as Single in Table 6 since onlythe identifier for Thickened (fndg) was directly anno-tated; the anatomical CUI and negative polarity werealready present in the linked SEMEVAL2015 attributeannotations.Table 7 shows the overall distribution of disorder-related identifiers both when attributes (non-disorderidentifiers assigned in SEMEVAL2015) are eitherincluded or excluded from consideration. Thus in theTable 5 CUILESS2016 annotator agreement type examplesExactmentionscoreHierarchical men-tion scoreText mention Annotator 1 Concept/s Annotator 2 Concept/s1.0 1.0 Allergies Levofloxacin Drug allergy Drug allergyLevofloxacin Levofloxacin0.0 0.52 Posturing (O/E) - posturing Posturing behaviour0.0 0.64 Rightward shift Midline shift of brain Midline shift of brainTo the right0.0 0.22 Redness Erythema RednessThe computed hierarchical mention score was used instead of annotator judgment in determining an approximate level of agreementOsborne et al. Journal of Biomedical Semantics  (2018) 9:2 Page 5 of 9Fig. 1 Annotation Workflow. BRAT 1.3 [17] used to normalize concepts to UMLS CUIs from SNOMED CTDisorder + Attributes column the text mention nobowel wall thickening was scored as having 3 identifiers,one for the disorder, one for the anatomical location andone for negation. Only when including these attributesare the majority of the concepts in CUILESS2016post-coordinated.Annotator agreement on the development set is shownin Table 8.DiscussionWe have normalized all but 8 of the 5397 original CUI-less concepts in our corpus indicating that a com-positional normalization methodology can alleviate thecontent completeness problem and increase semanticcoverage in clinical text. All examples where our approachfailed to normalize concepts are shown in Table 9. Theseexamples fall into 3 general classes, those where the entityis not really a disease (named entity recognition failure),those where the text is ambiguous, and those where theannotators were unable to find a suitable compositionin SNOMED CT. Only the last of these classes repre-sents a concept that was truly not normalizable underour methodology. The 3 cases that fall into this classrepresent a tiny fraction (0.06%) of the original 5397 men-tions. Leveraging the existing SEMEVAL2015 annotation(which specified 8 different semantic modifiers of dis-orders) and allowing our annotators to normalize usinga general semantic association (without specifying theexact relationship) allowed us to dramatically increasesemantic coverage. Our corpus should be of interest todevelopers of clinical text normalization software inter-ested in annotating a wider range of disorder annotations.We make our corpus freely available.While our methodology is similar to that used by Do?gan[9] for PubMed abstracts, we provide an order of mag-nitude more compositional normalization data. With theexception of some common abbreviations, the majority ofcompositional clinical concepts we created are composedTable 6 Disorder multiple identifier distribution by data setDisorder CUI type Development count Development proportion Training count Training proportionCUI-less 1 0.05 7 0.20Single 1687 87.46 2823 81.40Double 221 11.46 562 16.21Triple 18 0.93 73 2.11Quadruple 2 0.10 3 0.09Total 1929 100 3468 100Differences in disorder mention distribution between the development and training data set are likely due to note composition (see Table 3), a larger (4) set of annotators inthe training data and a lack of a consensus process for the training data since each training document is annotated only by a single annotatorOsborne et al. Journal of Biomedical Semantics  (2018) 9:2 Page 6 of 9Table 7 Overall disorder and attribute multiple identifierdistributionIdentifier type Disorder Disorder + AttributesCount Proportion Count ProportionCUI-less 8 0.1% 3 0.06%Single 4502 83.54% 966 17.90%Double 783 14.53% 2505 46.41%Triple 91 1.7% 1608 29.79%Quadruple 5 0.1% 263 4.87%Pentuple 0 0.0% 50 0.93%Hextuple 0 0.0% 20 0.04%Total 5397 100% 5397 100%The Disorder column shows the count and proportion of disorders annotated withone or more concepts excluding attributes. The Disorder + Attributes columnincludes identifiers from attributes in the count to capture post-coordination withother identifiersconcepts, not aggregate concepts. This is in sharp con-trast to Do?gan [9] where the majority of mentions (114)from PubMed abstracts are aggregates of discrete con-cepts and only 34 mentions (24 unique) require logicaldescription. Moreover, a substantial proportion (at least16%) of the CUI-less clinical concepts required composi-tional normalization to specify the disorder mention. Thisis a higher proportion than is seen previously in PubMedabstracts [9] and consistent with the greater variability ofclinical text.Exact annotator agreementThere is a clear need for multi-identifier annotation in theclinical arena, where multiple identifiers are semanticallycritical for diseases such as cancer [18] and peripheralarterial disease [19]. However, evaluating the annota-tor agreement of post-coordinated concepts is difficultbecause of a lack of a common annotation standard. Pre-vious studies reported proportionate agreement on exactmatches [8, 15, 20], but the definition of an exact matchcan vary.For example Andrews [8], took research questionsfrom case report forms and provided them to 3different coding companies and instructed them toextract (normalize) core SNOMED CT concepts, usingTable 8 Development dataset annotator agreementAgreement type Agreement count Proportionate agreementExact 1011 52.4Hierarchical NA 78.2Total mentions 1929There is no count for hierarchical agreement since each mention is assigned a valuebased on Eq. (1), whereas exact agreement assign every mention as a match (1.0) ornot (0.0)Table 9 Compositional CUI normalization error analysisMention Error ClassAllergies, Calcium Named entity recognition failureAtrial sensed Named entity recognition failureLeft ventricular inflow pattern Named entity recognition failureRCIA Ambiguous textRC one Aneurysm Ambiguous textEchogenic kidney No composition foundMaking grammatical errors No composition foundTortous aorta No composition foundAll 8 mentions where annotators were unable to annotate the disease using thecompositional approacheither pre-coordinated or post-coordinated expressions.Normalization was measured using proportionate agree-ment only at the core concept level, which ignoreddisagreements resulting from additional identifiers frommodifiers. Even with this restriction, agreement betweenall 3 coding companies was calculated to be only 33%,with 44% agreement between the two most similar anno-tation sets. Using Krippendorff s ? as their statistic theyconcluded there was no significant semantic agreement innormalization. In contrast, our proportionate exact agree-ment (our worst performing metric) was 10% higher thantheir best inter-annotator agreement although we weremore stringent in including disagreement to extend tonon-core concepts. This may be due to their data setwhich was focused on rare diseases in case report forms(rather than clinical text), differences in the tool selectionand/or annotator medical knowledge.An alternative measure of annotator normalizationagreement (accuracy) was used in the original annotationof this corpus [15] instead of Cohens ? and Krippendorf s?. Annotator normalization agreement was calculatedbetween annotators and was not separated from theunderlying mention span boundary detection. A relaxedaccuracy calculation where correctness was defined asany overlapping span where the disorder CUIs matchedyielded an accuracy of 0.776, a strict agreement scorebased on exact span matching yielded a much higheragreement of 0.846. However this high accuracy appliesto single CUI disorder agreement. No annotator agree-ment was reported including disagreements with CUIsfrom the body location attribute or other included iden-tifiers. While that reported exact agreement is higherthan ours, we expected our agreement to be substantiallylower since our annotation was for CUI-less disordersthat they did not annotate. The original annotation delib-erately excluded use of the UMLS semantic group findingfor these disorders and reported that this semantic groupwas found to be a noisy, catch-all category, and attemptsOsborne et al. Journal of Biomedical Semantics  (2018) 9:2 Page 7 of 9to consistently annotate against it did not succeed in ourpreliminary studies.Non-exact annotator agreementOur exact agreement calculation cannot determine if apre-coordinated concept and a post-coordinated conceptare logically equivalent. Additionally, exact agreementcannot capture the difference between concepts withcompletely different meanings and hyponyms/hypernymsthat have similar meanings. Our hierarchical agreementmeasure can account for this distinction. Hierarchicalagreement penalizes distant errors and those at thehigher levels of the hierarchy more severely than finermisclassifications, similar to hierarchical precision [16].Unfortunately, the performance of hierarchical agreementis dependent on the structure of the ontology used. It issensitive to the level of branching and assumes a consis-tent correlation between branch length and semantic dis-tance. Thus even semantically similar concepts such as theposturing example seen in Table 5 may not score well, aconsideration given the semantic duplication in SNOMEDCT [21, 22]. We thus asked our annotators to consider thesets of concepts in each disagreement, and judge whetherthey were semantically equivalent, using their knowledgeas medical professionals, rather than the exact structureof the ontology. The two annotators reached consensuseasily on this task; there was only one case where theycould not reach consensus, and for this, a neurologist wasconsulted to resolve the dispute. This process yielded asemantic agreement level of 71.6%, 19% increase overour exact agreement and is consistent with Casper [20]who reported 53% exact agreement and 75% semanticagreement.Compositional annotation rulesOne unresolved consideration with compositional anno-tations is which rules or conditions should govern anno-tation construction. In a previous study [8], the 3 codingcompanies mapping to SNOMED CT presumably (notspecified in paper) used the extremely structured andelaborate SNOMED CT specific post-coordination spec-ification to compose any post-coordinated diseases theyannotated. However Pradhan [15] took a more general(but domain specific) approach specifying only 9 per-missible disorder modifiers. All of these disorder specificdomains (with the exception of body location) had a small(single digit) range of acceptable values. While core dis-order concepts annotated in these publications should becomparable, associated concepts should be expected tobe quite different. The more general annotation approachtaken by Do?gan [9] and this work allowed for any conceptwithin the target ontology or ontologies. This allows formore flexibility at the expense of interpretation. For exam-ple, a body location CUI could refer to the site of diseasefinding, an affected organ, or a procedure site related tothe illness. It is an open-ended question whether it isbetter to define the set of rules and allowable domainsfor post-coordination for each domain or to allow unre-stricted composition. An enumerated set of possible rela-tionships make closed world logic operations possible, butenumerating a complete and useful set of distinct seman-tic relationships that can be described in natural languagetext may not be feasible [7].Practical applicationsA practical application of our work is increasing seman-tic representation in clinical text. The approximately 70%coverage of named entities in SemEval-2015 Task 14 istoo low for many practical purposes. Additionally, whileSEMEVAL2015 corpus has themost exhaustive set of rela-tions or slots for diseases to date, it still does not includeimportant clinical relationships useful for practical appli-cations of NLP. For example, metastasis, infection, surgi-cal procedures or other SNOMED CT specified relationsare relevant for practical clinical use. Additionally, by cre-ating a corpus that includes clinical compositional anno-tation, this corpus opens the door to such annotation bymachines that could potentially reduce the clinical codingburden.LimitationsWe have shown that annotating text from dischargesummaries with compositional concepts from SNOMEDCT is possible with high levels of annotator agreement.While this approach improves semantic coverage andis not bound to specific semantic relationship types, itdoes introduce a measure of semantic ambiguity sincethe relationship between the concepts is unclear. Thus,our annotations are more useful for information extrac-tion than for logical reasoning, especially since we donot annotate logical operators (AND/OR) which wouldbe useful in distinguishing aggregate from compositeconcepts. Future work should be able to make this dis-tinction and also determine if our results are achiev-able for other medical text types (e.g., pathology reports)and other medical ontologies (e.g., the consumer healthvocabulary).We have shown high annotator agreement for anno-tating a single text mention with the identifiers ofmultiple ontological concepts, though we expect thisagreement is lower than agreement on single identifiermentions. Unfortunately, we are unable to directly calcu-late single-identifier agreement because, under our anno-tation scheme, a mention which has been annotated witha single identifier may represent either (1) a true single-identifier disease/disorder where the identifier completelycaptures the meaning, or (2) a disease/disorder where asingle identifier captures only part of the meaning butOsborne et al. Journal of Biomedical Semantics  (2018) 9:2 Page 8 of 9the remaining meaning is captured by linked attributes(e.g., the body location already identified by the SemEval-2015 Task 14 annotations).ConclusionsIn conclusion, we extended the SemEval-2015 Task 14annotations of the ShARe disorder corpus to cover CUI-less concepts and showed that the compositional anno-tation approach first used by Do?gan [9] on PubMedtext can function in clinical text to assign semanticidentifiers to named entities and reduce the contentcompleteness problem [12, 13]. We believe our larger,freely available corpus is an important resource forthe annotation of CUI-less concepts and that infor-mation extraction utilizing compositional normalizationcan lead to a more complete understanding of clinicaltext by complementing annotation approaches using pre-defined relations or slots such as the original ShareClefannotation. While annotation of complex clinical con-cepts using multiple identifiers has been routinely doneby humans in a clinical or research setting, this cor-pus should aid the development of compositional nor-malization by machines to supplement manual codingpractises.Additional fileAdditional file 1: Annotation Guidelines for Annotating CUI-lessConcepts in BRAT. (PDF 1050 kb)AcknowledgementsShyam Patel and Efe Sahine helped annotate the training corpus. Discussionswith Ken Barker were extremely helpful and his support in the creation of theannotation guidelines was much appreciated.FundingResearch reported in this publication was supported by the National Institutesof Health. Support includes grant award number UL1TR001417 from theNational Center for Advancing Translational Sciences and grant award number1R01GM114355 from the National Institute of General Medicine Science,Extended Methods and Software Development for Health NLP. The contentis solely the responsibility of the authors and does not necessarily representthe official views of the National Institutes of Health.Availability of data andmaterialsTo obtain a copy of the dataset it is required that the original ShARe corpus bedownloaded first and a license agreement signed as described on the ShARewebsite (http://alt.qcri.org/semeval2015/task14/index.php?id=data-and-tools). The CUI-less annotations may then be made downloaded from https://physionet.org/works/CuilessClinical/. Annotation guidelines are included inthe Additional Files section. An implementation of the hierarchical annotatoragreement calculation for SNOMED CT can found in theHierarchicalAnnotatorAgreementClient class at https://github.com/ozborn/jbratuimatools. All other intermediate data is available upon request.Authors contributionsJDO conceived the project idea, helped design the experiment, analyzed thedata and wrote the first draft. SB and TS helped design the experiment,analyze the data, and edit the manuscript. MN and MID annotated the corpus,analyzed the data, and edited the manuscript. All authors read and approvedthe final manuscript.Ethics approval and consent to participateNot applicable, this study uses previously published de-identified humansubject data that is classified as non-human subject data because of the lackof identifying information.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1University of Alabama at Birmingham, 7th Ave S, 1720 Birmingham, USA.2Computer Science Department, University of Houston, Düsternbrooker Weg20, 24105 Houston, USA. 3School of Information, University of Arizona, 85721Tucson, USA.Received: 10 April 2017 Accepted: 26 December 2017Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 https://doi.org/10.1186/s13326-018-0192-yRESEARCH Open AccessInvestigating the role of interleukin-1beta and glutamate in inflammatory boweldisease and epilepsy using discovery browsingThomas C. Rindflesch1, Catherine L. Blake2, Michael J. Cairelli3, Marcelo Fiszman4, Caroline J. Zeiss5and Halil Kilicoglu6*AbstractBackground: Structured electronic health records are a rich resource for identifying novel correlations, such asco-morbidities and adverse drug reactions. For drug development and better understanding of biomedicalphenomena, such correlations need to be supported by viable hypotheses about the mechanisms involved, whichcan then form the basis of experimental investigations.Methods: In this study, we demonstrate the use of discovery browsing, a literature-based discovery method, togenerate plausible hypotheses elucidating correlations identified from structured clinical data. The method issupported by Semantic MEDLINE web application, which pinpoints interesting concepts and relevant MEDLINEcitations, which are used to build a coherent hypothesis.Results: Discovery browsing revealed a plausible explanation for the correlation between epilepsy and inflammatorybowel disease that was found in an earlier population study. The generated hypothesis involves interleukin-1 beta(IL-1 beta) and glutamate, and suggests that IL-1 beta influence on glutamate levels is involved in the etiology of bothepilepsy and inflammatory bowel disease.Conclusions: The approach presented in this paper can supplement population-based correlation studies byenabling the scientist to identify literature that may justify the novel patterns identified in such studies and canunderpin basic biomedical research that can lead to improved treatments and better healthcare outcomes.Keywords: Literature-based discovery, Discovery browsing, Epilepsy, Inflammatory bowel disease, Interleukin-1 beta,GlutamateBackgroundInformation needs in clinical setting and basic researchsetting differ significantly. Studies of information needsin clinical setting have focused on the types of questionsasked by clinicians [1] and have informed the specificinformation facets (population, intervention, comparison,and outcome) that need to be identified in order to addressthose clinical questions [2]. In contrast to the clinical set-ting where there are often multiple studies relevant tothe clinical encounter, scientists operate at the discoveryend of the information synthesis spectrum where there*Correspondence: kilicogluh@mail.nih.gov6Lister Hill National Center for Biomedical Communications, U.S. NationalLibrary of Medicine, 8600 Rockville Pike, Bethesda, MD, USAFull list of author information is available at the end of the articleis less information available, and agreement on how toevaluate or combine findings from different studies isstill under development [3]. In such an environment, ascientist begins with what is best described as a hypoth-esis projection, the purely conjectural proliferation of awhole gamut of alternative explanatory hypotheses thatare relatively plausible, a proliferation based on guesswork- though not mere guesswork, but guesswork guided bya scientifically trained intuition. The aim of this enter-prise is to identify those hypotheses that merit detailedscrutiny. [4]Structured data from electronic health records (EHRs)are increasingly mined to identify novel correlations, suchas disease co-occurrences or adverse drug reactions [5].Such studies are sometimes highly localized, relying on© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 2 of 14data collected from a small set of institutions; thus, theycan violate some of the key assumptions made when usingtraditional statistical measures to determine significance,leading to false positive associations [6]. When performedwith population-level data (e.g., Medicare claims data),these data mining studies can provide epidemiologicalevidence for co-morbidities and other biomedical phe-nomena; however, they alone are unable to elucidate themechanisms involved in such phenomena or offer plausi-ble explanations. Such epidemiological evidence must besubjected to further analysis by scientists in order to gen-erate viable hypotheses about the etiology of the observedcorrelations, a critical step for the development of safe andeffective treatments.In this paper, guided by statistical correlations extractedfrom structured EHR data, we show how a literature-based discovery technique called discovery browsing [7, 8]can be used to support scientists as they explore hypoth-esis projections. This study was instigated to generate ahypothesis of mechanism for the results of a recent ret-rospective population study that measured the relation-ship between epilepsy and twelve autoimmune diseases[9]. That study analyzed health insurance claims data for2,518,034 patients, both male and female, 65 years oryounger. They reported that the risk of epilepsy was sig-nificantly heightened among patients with autoimmunedisease. Collectively, individuals with autoimmune dis-ease accounted for 17.5% of patients with epilepsy in thestudy population. This was a significant result that wasnot generally anticipated in the clinical community. Theauthors made several suggestive observations relevant tothis correlation. Glutamate receptors may be involved inthe etiology of epilepsy and other central nervous sys-tem disorders (the glutamate hypothesis) [1012]. Theinflammatory component of autoimmune diseases maybe responsible for the occurrence of epilepsy [13, 14].They did not, however, propose possible mechanismsunderlying their findings and observations. Because thereexists a lack of mechanistic understanding for the rela-tionship between epilepsy and autoimmune disease, weview it as a prime candidate for discovery browsing. Thesuggestive observations mentioned above combined withour prior work that investigated the relationship betweenmajor depression and inflammation via cytokines usingdiscovery browsing [7] indicated to us that focusing onglutamate and inflammation could be a fruitful avenue fora mechanistic understanding.In this study, we focus on epilepsy and one autoimmunedisorder, inflammatory bowel disease (IBD, a broaderterm covering both Crohns disease and ulcerative col-itis as included in Ong et al. [9]). Further, we look atinterleukin-1 beta (IL-1 beta), as one of the principal sub-stances (along with interleukin-6) involved in inflamma-tion. We use discovery browsing to generate a hypothesisabout the mechanisms of both IL-1 beta and glutamate,and suggest that the influence of the former on the lat-ter is involved in the etiology of both IBD and epilepsy,thus proposing a mechanism for the observed connectionbetween these two disorders. This study also investigatesgeneralizability of previous work on discovery browsing.Instead of using discovery browsing to elucidate a generalphenomenon that was observed in many different stud-ies and known anecdotally for years (e.g., obesity paradox[8]), we apply it to a narrower scope, exploring possiblemechanisms for the results of a single study without suchan obvious presence in the clinical community.Related workDiscovery browsingLiterature-based discovery (LBD) [15] is a method ofhypothesis generation, the core premise of which is theso-called ABC paradigm. AB (a relationship between twoterms A and B) and BC (a relationship between B andC) are both known, but an AC relationship has so farnot been proposed. The method can be used for opendiscovery, in which the discovery (or hypothesis) AC isthe result. Alternatively, in closed discovery, AC may beknown (or assumed) and relations AB and BC are soughtto posit B as an explanation for AC (or a mechanistic linkbetween the two concepts). While LBD research has pre-dominantly focused on biomedical literature, it has alsobeen applied to other domains, such as humanities [16],world wide web [17], as well as technology and socialissues [18].Wilkowski et al. [7] introduced discovery browsing as amodification of LBD. They described it as a tool for illumi-nating under-studied and poorly understood phenomenarather than necessarily for making discoveries. Discoverybrowsing also relies on the ABC paradigm and the rela-tionships it exhibits; however, the researcher may assume(or already know) the relationships, but seek to elucidatethe details of these assumptions, hypotheses, or knownrelationships. In the current study, we consider the rela-tionships to be IBD (A)  inflammation (B)  epilepsy (C),in which AB, BC, and AC have all been proposed. Wethen seek to investigate, expand, and elucidate the B rela-tionship between these two diseases for a finer-grainedunderstanding of the mechanisms involved.Wilkowski et al. [7] used discovery browsing to lookat the interaction of melatonin, cytokines, and majordepression. Cairelli et al. [8] formalized the method andexploited it to investigate why obesity is beneficial inintensive care, but detrimental otherwise (i.e. obesityparadox).SemRepSemantic predications extracted fromMEDLINE citations(titles and abstracts) naturally correlate with relations inthe ABC paradigm and underpin this study. A semanticRindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 3 of 14predication is a formal representation of an assertionmade in text. Such structures provide a type of com-putable knowledge representing the information in thetext from which they are extracted. For example, the pred-ication Interleukin-1 beta-CAUSES-Seizures representspart of the meaning of the sentence, In addition, high IL-1beta doses induced seizures only in IL-1beta receptor-expressing mice (mentions relevant to the predication inbold). Note that it does not (necessarily) summarize anentire sentence, and in this case does not contain theinformation limiting the seizure induction to IL-1 betareceptor-expressingmice. A semantic predication consistsof a predicate (CAUSES in this example) and arguments(Interleukin-1 beta and Seizures).We extract predications using the SemRep natural lan-guage processing system [19]. SemRep inspects each sen-tence of input text to identify predications asserted ineach sentence. The system depends on domain knowledgein the Unified Medical Language System (UMLS) devel-oped by the U.S. National Library of Medicine [20, 21]. ASemRep predication has UMLS Metathesaurus conceptsas arguments and a UMLS Semantic Network relation aspredicate.Extracted predications may be filtered by using auto-matic abstraction summarization [22] to focus on specificaspects of biomedicine, such as treatment of diseases orpharmacogenomics. In this study, we used this process tofocus on predications asserting core relations in molecu-lar biology [23]. We used the following meta-predications,where the arguments are represented as general semanticclasses: {Substance} ASSOCIATED_WITH ORPREDISPOSES OR CAUSES{Pathology} {Substance} INTERACTS_WITH OR INHIBITS ORSTIMULATES{Substance} {Substance} AFFECTS OR DISRUPTS ORAUGMENTS {Anatomy OR Process} {Anatomy OR Living Being} LOCATION_OF{Substance} {Anatomy} PART_OF {Anatomy OR Living Being} {Process} PROCESS_OF {Living Being}Semantic MEDLINEThe methodology pursued in this study is implementedwith Semantic MEDLINE [24], a Web application thatintegrates PubMed document retrieval, SemRep naturallanguage processing, automatic abstraction summariza-tion, and visualization into a single Web portal. Sem-Rep predications extracted from all MEDLINE citationsare made available from SemMedDB [25] and are sum-marized according to the meta-predications just noted.Summarized predications are then presented as a con-nected interactive graph of semantic relations (Fig. 1).Subjects and objects are nodes in the graph, while pred-icates are edges. By clicking on an edge, the user can seethe predication represented. The edge has a link to theFig. 1 Illustration of the Semantic MEDLINE web application. The summarized results of a PubMed search are displayed as an interactive graph,where nodes represent subjects and objects of semantic predications and the edges represent the predicates (right). Edges are linked to the originalMEDLINE citation from which the predication is extracted (top-left). Nodes and edges can be filtered using relation and semantic group filters(bottom-left)Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 4 of 14original MEDLINE citation from which the predication isextracted.Semantic MEDLINE supports discovery browsing bypresenting relationships to the user that might not havebeen noticed without it. Miller et al. [26] used Seman-tic MEDLINE to study the effect of the interaction oftestosterone and cortisol on declining sleep quality inaging men. In related work, Cohen et al. [27] discussedEpiphaNet, which displays SemRep predications in graph-ical form for literature-based discovery. Hristovski et al.[28] described several SemanticMEDLINE-based systemsdesigned to facilitate discovery.Other literature-based exploration and discovery sys-tems have also been proposed to formulate and assessscientific hypotheses. For example, Arrowsmith [29] linkstwo sets of articles from biomedical literature using titlewords and phrases and statistical information. In a sim-ilar vein, LitLinker [30] performs open discovery usingUMLS concepts identified by MetaMap [31] as the basis,grouping and pruning them in conjunction with statis-tical correlations. Berlanga et al. [32] focus on semanticintegration and visualization from multiple knowledgeand data sources, using named entity recognition torecognize concepts, exploiting concept taxonomies andco-occurrence across documents to identify interestingassociations and visualize them for exploration purposes.HyQue [33] is concerned with semantic integration forthe purpose of hypothesis evaluation and uses SemanticWeb technologies to standardize representation of input,knowledge sources, data, queries, and outputs. UnlikeSemRep, these systems/tools do not perform explicit rela-tion extraction, mainly relying on concept co-occurrenceor manually curated relationships.Inflammation, epilepsy, and inflammatory bowel diseaseBefore investigating possible mechanistic connectionsbetween IBD and epilepsy, we surveyed MEDLINEregarding the observations from Ong et al. [9] to deter-mine how much research has been published concerningthis relationship. We began by looking at IBD, a dis-order in which the involvement of inflammation is notcontroversial.In order to focus relevant background information,we used Semantic MEDLINE as an adjunct to PubMedto query MEDLINE for the primary proinflammatorycytokines involved in IBD. We issued the Semantic MED-LINE query inflammatory bowel disease on 03/22/2017and restricted results to the most recent 500 citations.4402 predications were extracted and we restricted theseto the core relations in molecular biology. The follow-ing cytokines appear as nodes in the graph: IL-1 alpha,IL-1 beta, IL-6, IL-10, IL-17, IL-18, IL-19, and IL-23. Wethen used PubMed to determine the amount of researchfor each of these substances in association with IBD. Inorder to achieve high recall, we combined the search forIL-1 alpha and IL-1 beta with the query (interleukin-1AND inflammatory bowel disease). All other cytokinesin this list were queried with the form (<IL-X> ANDinflammatory bowel disease). The query results were:IL-1: 340MEDLINE citations; IL-6: 779; IL-10: 841; 1L-17:348; IL-18: 106; IL-19: 10; IL-23: 228.In order to determine which cytokines are most promi-nently involved in both IBD and epilepsy, we thenrepeated this series of queries to PubMed, substitutingepilepsy for inflammatory bowel disease. The resultsfor each query were: IL-1: 201 MEDLINE citations;IL-6: 216; IL-10: 71; IL-17: 19; IL-18: 14; IL-23: 4; and nocitations retrieved for IL-19. Although several cytokinesare prominent in IBD research, only IL-1 (mostly beta)and interleukin-6 have been much studied with respectto epilepsy. The inflammatory aspects of both conditionsare accompanied by alterations in a broad array of medi-ators (see Bevivino and Monteleone [34] and Matin et al.[35] for reviews on IBD and epilepsy, respectively) whoseinteraction in disease etiology is likely to be contextual.While both IL-1 beta and interleukin-6 were promisingcandidates for discovery browsing, we focused on IL-1beta in this paper, partly to keep the scope of this workmanageable. This procedure could be repeated with othersubstances, especially interleukin-6, to potentially revealadditional insights.As further background investigation, we used PubMedto get an overview of research on IL-1 beta and IBD.We looked at a sample from the 340 citations returnedwith the query noted above. IL-1 beta has long been asso-ciated with gastrointestinal disturbances (e.g. [36]) andwith IBD in particular (e.g. [37]). Subsequent research haslooked at various aspects of that association. For exam-ple, Casellas et al. [38] investigated the role of IL-1 betain chronic ulcerative colitis. Heresbach et al. [39] soughtto elucidate genetic susceptibility to IBD, concentratingon IL-1beta and IL-1 receptor antagonist (IL-1ra) genepolymorphisms. Coccia et al. [40] reported on multi-ple mechanisms through which IL-1 beta contributes tointestinal pathology. Li et al. [41] exploited biolumines-cence imaging to determine the location of cells produc-ing IL-1 beta during intestinal inflammation. Das [42]hypothesized that the etiology of IBD is due to inade-quate production of inflammation resolving molecules,such as lipoxins, resolvins, protectins, maresins andnitrolipids.Finally, we queried PubMed to determine whetherthere is any research on IL-1 beta and both IBD andepilepsy. The query (interleukin 1 AND (inflammatorybowel disease OR colitis) AND (seizure OR epilepsy))returned only 1 citation [43], a review which states in theabstract, foreshadowing the conclusions of Ong et al. [9],that There are reports suggesting more predispositionsRindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 5 of 14to seizures during inflammatory conditions like colitis,pneumonia and rheumatoid arthritis.We then moved on to the focus of the paper, whichwas twofold: 1) investigate the research on IL-1 beta andepilepsy, and 2) look at possible mechanistic connectionsbetween IBD and epilepsy involving inflammation (IL-1beta).MethodsAt the core of the discovery browsing methodology pur-sued in this study is cooperative reciprocity between thesystem and the users domain knowledge. This takes theform of the user issuing an initial query to SemanticMEDLINE reflecting an area of interest. All queries wereissued at the end of March, 2017. The graph resultingfrom each query was inspected for concepts (either inthe predications or in the abstracts from which they areextracted) that capture the attention of the researcherand which may incite the development of a potentialhypothesis regarding the study being pursued. At thispoint, PubMed was consulted (using the same query)to determine whether any citation from which Seman-tic MEDLINE did not extract a predication supportedthe viability of the hypothesis being developed. Thisstep is performed in part to address recall problemsof SemRep, which may result in missing informationimportant for hypothesis generation. If the developinghypothesis was supported, it was pursued with anotherquery to Semantic MEDLINE incorporating the conceptof interest, and the process was repeated until we weresatisfied with a coherent argumentation chain. Finally,PubMed was searched to determine whether the hypoth-esis generated is novel. Figure 2 provides an overview ofthe method.For each query to Semantic MEDLINE, we limitedthe abstracts returned to the most recent 500, althoughfewer total abstracts were retrieved for some queries. Thepredications extracted were then summarized using themeta-predications given in previous section. For ease ofinspection, we further limited the graph for each query toa maximum of 50 nodes and 100 edges; nodes are rankedby frequency and the graph is limited to 50 nodes with thehighest frequency.ResultsIL-1 beta and epilepsyThe initial query to Semantic MEDLINE was (inter-leukin 1 AND (epilepsy OR seizure)), which returned2481 predications extracted from 240 citations. In thesummarized graph, several predications were considerednoteworthy as indicating a relationship between IL-1beta and epilepsy. Interleukin-1 beta-AFFECTS-Seizurewas extracted from Vezzani et al. [44], which reportsthat intrahippocampal application of recombinant IL-1rainhibits seizures experimentally induced by bicucullinemethiodide in rodents.This study cites previous work [45],inwhich they found that exogenous application of IL-1 betain the rat hippocampus prolongs kainite-induced seizureby enhancing glutamatergic neurotransmission.The predication Interleukin-1 beta-CAUSES-Seizureswas extracted from two citations. In one, Ravizza andVezzani [46] conducted immunohistochemical analysis oftissue following acute electrical stimulation in the ventralhippocampus of rats. They investigated the role of IL-1beta during resulting epileptic activity, focusing on therole of IL-1 receptor type 1 (IL-1R1) in rat forebrain. Theysuggest that this receptor plays different roles in neuronsand in astrocytes during status epilepticus. Another studyFig. 2 Overview of discovery browsing. An iterative process that incorporates Semantic MEDLINE help identify interesting concepts, which are usedto build an argumentation chainRindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 6 of 14[47] looked at the mechanism of seizure by injecting theright lateral ventricle of rats with both IL-1 beta and gluta-mate. They conclude that there is an interaction betweenIL-1 beta (through the IL-1 receptor) and metabotropic glutamate receptors in the onset of epilepsy. Interleukin-1beta-AUGMENTS-Status Epilepticus was extracted fromPernot et al. [48], in which the relationship betweenneuroinflammation and mesiotemporal lobe epilepsy syn-drome was explored with immunohistochemical analysisof tissue after mesiotemporal lobe epilepsy syndrome wasexperimentally induced in C57BL/6 adult mice by the uni-lateral intrahippocampal injection of kainate. They con-clude that neuroinflammatory pathways are associatedwith epileptogenesis.Opposing results have also been published. One suchstudy, Claycomb et al. [49] (from which the predicationInterleukin-1 beta-ASSOCIATED_WITH-Seizures wasextracted), reports that IL-1 beta is neuroprotective. Thisstudy was conducted on transgenic mice with targeteddisruption in genes for either the ligand IL-1 beta or itssignaling receptor, IL-lR1. Their claim is based on theirfinding that chemoconvulsants administered to IL-1 betaand IL-1R1 -/- mice produced more acute seizures than intheir respective +/+ littermates. It is not clear that theseresults would generalize to animals without such geneticmanipulation. See Table 1 for an overview of our resultson IL-1 beta and epilepsy.Since we saw considerable research implicating IL-1beta in epileptogenesis, we next pursued the potentialinteraction of IL-1 beta and glutamate in the pathogenesisof epilepsy and seizures [45, 47]. We began by looking forresearch that examined glutamate and epilepsy withoutconsidering IL-1 beta, and then looked at the interactionof the two in epilepsy and seizure.Glutamate and epilepsyThe SemanticMEDLINE query (glutamate AND (epilepsyOR seizure)) extracted 5170 predications from the mostrecent 500 citations. After summarization, we examinedseveral predications which appeared to be relevant to glu-tamate in the context of seizure or epilepsy. Juhasz et al. [50]used protonmagnetic resonance spectroscopic imaging totest glutamate concentration levels in epileptic childrenwith Sturge-Weber syndrome (which is strongly associ-ated with epilepsy [51]). They found increased glutamatein the affected hemisphere, which they interpret as sup-port for the role of excess glutamate in these patients(Glutamate-ASSOCIATED_WITH-Seizures). Cavus et al.[52]measured glutamate levels in epileptic and nonepilep-tic cortical sites in 79 patients with refractory epilepsyusing high-performance liquid chromatography. Theyfound elevated extracellular glutamate at epileptogenicas compared to nonepileptogenic sites (Glutamate-ASSOCIATED_WITH-Epilepsy).In considering MEDLINE citations from which Seman-tic MEDLINE did not extract a predication, one notablepaper discusses research on the mechanisms of gluta-mate involvement in epilepsy. Perez et al. [53] assume thatexcessive glutamate underlies refractory temporal lobeepilepsy. They investigated the cause by using immuno-gold electron microscopy to measure glutamate levels intissue extracted from the brains of male Sprague-Dawleyrats infused with methionine sulfoximine, which inducesglutamine synthetase efficiency. They conclude that suchdeficiency leads to increased extracellular glutamate. Thestudies we report on glutamate and epilepsy are summa-rized in Table 2.Based on research indicating glutamate involvement inepilepsy and considering research implicating IL-1 betaTable 1 Summary of articles discussing IL-1 beta and epilepsyStudy Subjects Method Result/ConclusionVezzani et al. [45] Kainite-intoxicated rats Application of IL-1 beta in thehippocampusIL-1 beta prolongs experimentallyinduced seizuresVezzani et al. [44] Bicuculline methiodide-intoxicatedrodentsIntrahippocampal application ofrecombinant IL-1raIL-1ra inhibits experimentallyinduced seizuresRavizza and Vezzani [46] Male Sprague-Dawley rats Immunohistochemical analysisfollowing acute electricalstimulation in the ventralhippocampusIL-1R1 plays different roles inneurons and in astrocytes duringstatus epilepticusWang et al. [47] Rats Injection of right lateral ventriclewith both IL-1 beta and glutamateInteraction between IL-1 beta andmetabotropic glutamate receptorsin the onset of epilepsyPernot et al. [48] C57BL/6 adult mice Immunohistochemical analysis oftissue after mesiotemporal lobeepilepsy syndrome induced byintrahippocampal injection ofkainateNeuroinflammatory pathways areassociated with epileptogenesisClaycomb et al. [49] IL-1 beta and IL1R1 -/- mice Administration ofchemoconvulsantsProduced more acute seizuresRindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 7 of 14Table 2 Summary of articles discussing glutamate and epilepsyStudy Subjects Method Result/ConclusionJuhász et al. [50] Epileptic children with Sturge-WebersyndromeProton magnetic resonance spectro-scopic brain imagingIncreased glutamate concentrationsobservedCavus et al. [52] Epileptic and nonepileptic corti-cal sites in patients with refractoryepilepsyHigh-performance liquid chromatog-raphy based on microdialysis probesElevated extracellular glutamateobserved at epileptogenic sitesPerez et al. [53] Tissue extracted from brains of maleSprague Dawley rats infused withmethionine sulfoximineGlutamate levels measured withimmunogold electron microscopyGlutamine synthetase deficiencyleads to increased extracellularglutamatein epilepsy, we were encouraged to investigate the inter-action of IL-1 beta and glutamate in the context of thisdisorder.IL-1 beta and glutamateWe issued three queries to Semantic MEDLINE to inves-tigate the relationship of IL-1 beta and glutamate in theetiology of epilepsy. One focused on this disorder (andseizure), another specified the brain (but not the disor-der), and a third specified neither disorder nor anatomiclocation.And epilepsyThe query (interleukin-1 AND glutamate AND (seizureOR epilepsy)) retrieved 18 citations and 202 predications,which were not summarized. Two papers identified inthe graph were relevant. Xiaoqin et al. [54] injected thecerebral cortex and hippocampus of rats with IL-1 betaand IL-6. Immunohistochemistry observation revealedthe development of seizures along with increased glu-tamate and decreased GABA (Interleukin-6-CAUSES-Seizures). Donnelly et al. [55] analyzed synaptosomesprepared from the brains of BALB/c female mice, 8-12weeks old, in which epilepsy-like symptoms had beeninduced with glycerol. Synaptosome pellets were thensubjected to a series of in vitro techniques after whichthey observed an increase in IL-1 beta levels and adecrease in glutamate release in hippocampus tissue(Entire hippocampus-LOCATION_OF-Glutamate).In the brainThe Semantic MEDLINE query (interleukin 1 ANDglutamate AND brain) returned 1850 predications from160 citations. After summarization, several predicationswere extracted from citations discussing the interaction ofIL-1 beta and glutamate. Interleukin-1 beta-DISRUPTS-uptake was extracted from an article [56], which reportedthat astrocyte uptake of glutamate is neuroprotective dur-ing brain inflammation. Based on Northern blot analy-sis and other in vitro techniques performed on primalhuman astrocyte cultures subjected to several cytokinesand 3H-glutamate, the authors concluded that proin-flammatory cytokines inhibit astrocyte glutamate uptake.Based on intracerebral microdialysis in unanesthetizedrabbits, Huang et al. [57] reported that organum vasculo-sum laminae terminalis (OVLT) release of glutamate wasinduced by intracerebroventricular injection of IL-1beta(Interleukin-1 beta-STIMULATES-Glutamate).When inspecting citations from which Semantic MED-LINE did not extract a predication, we found an earlierreport which concluded that IL-1 beta enhances glu-tamate. As measured by brain microdialysis in freelymoving male SpragueDawley rats, Mascarucci et al.[58] found that injection of intraperitoneal IL-1 betaincreased glutamate release in the nucleus tractussolitarius.Some studies reported that IL-1 beta inhibits glutamate.Murray et al. [59] prepared hippocampal synaptosomesfrom male Wistar rats, on which in vitro experimentswere conducted. They reported that immunoblottingwith specific antibody revealed that IL-1 beta inhibitedpotassium chloride-stimulated glutamate release in tis-sue from young (4 month) but not older (22 month)rats, and only in the presence of calcium (Interleukin-1beta-STIMULATES-Glutamate (although the predica-tion itself is wrong)). In a study of the influence of IL-1beta on memory consolidation, Gonzalez et al. [60]reported that intrahippocampal injection of IL-1 beta inadult male Wistar rats decreases glutamate release fromdorsal hippocampus synaptosomes after contextual fearconditioning (Interleukin-1 beta-INTERACTS_WITH-CRK protein, human). The studies resulting from thisquery (including brain) and the previous one (notincluding brain) are given in Table 3.Disorder and location not specifiedThe Semantic MEDLINE query (interleukin 1 AND glu-tamate) retrieved 3232 predications from 289 citations.The research we saw in the summarized graph focusedon neuronal involvement. For example, the predicationGlutamate-COEXISTS_WITH-Interleukin-1 beta wasextracted from Casamenti et al. [61], which looked atthe involvement of inflammation with Alzheimers dis-ease. IL-1 beta was injected into the nucleus basalis ofadult male Wistar rats. The authors reported a markedincrease in glutamate (revealed through microdialysis).Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 8 of 14Table 3 Summary of articles discussing IL-1 beta and glutamateStudy Subjects Method Result/ConclusionXiaoqin et al. [54] Cerebral cortex and hippocampusof ratsInjection of IL-1 beta and IL-6;immuno-histochemistryIncreased glutamate and decreasedGABA observedDonnelly et al. [55] Synaptosome pellets preparedfrom brains of 8-12 week-oldBALB/c female mice intoxicatedwith glycerolIn vitro techniques Report increased IL-1 beta levelsand decreased glutamate release inhippocampus tissueHu et al. [56] Human astrocyte cultures sub-jected to several cytokines and3H-glutamateNorthern blot analysis and other invitro techniquesProinflammatory cytokines inhibitastrocyte glutamate uptakeHuang et al. [57] Intracerebroventricular injection ofIL-1beta in adult male New Zealandwhite rabbitsIntracerebral microdialysis Glutamate induced by IL-1 betaMascarucci et al. [58] Intraperitoneal injection of IL-1 betain freely moving male SpragueDawley ratsBrain microdialysis Increased glutamate released in thenucleus tractus solitariusMurray et al. [59] Synaptosomes prepared from maleWistar ratsImmunoblotting with specific anti-bodyIL-1 beta inhibits potassiumchloride-stimulated glutamaterelease in tissue from young (4month), in the presence of calciumGonzalez et al. [60] Adult male Wistar rats Intrahippocampal injection of IL-1beta; preparation of synaptosomes;in vitro technique to assay gluta-mate releaseIL-1 beta decreases glutamaterelease from dorsal hippocampussynaptosomes after contextual fearconditioningThe query which retrieved the first two studies used the term brain, and that which retrieved the other studies used the term epilepsy)Prow and Irani [62] used immunoblotting and immuno-histochemistry, cytokine assays, and histological analysisto examine spinal cord tissue extracted from mice chal-lenged with neuroadapted Sindbis virus. Based on anal-ysis of levels of astroglial glutamate transporter (whichremoves glutamate from the synaptic cleft), IL-1 beta,and glutamate, they claimed that the increase of IL-1beta in response to the virus disrupts glutamate home-ostasis. They concluded (in the abstract) that their dataprovide one of the strongest in vivo links betweeninnate immune responses and the development of exci-totoxicity demonstrated to date. (Interleukin-1 beta-INTERACTS_WITH-Glutamates).In a sample of citations fromwhich SemanticMEDLINEdid not extract a predication, Fogal et al. [63], for example,investigated the etiology of hypoxic-ischemic brain dam-age in IL-1R1 null mutant, mGluR1-/-, and wild-type con-trol mice. From both in vitro and in vivo experiments, theyconcluded that IL-1 beta makes a significant contributionto such neuronal injury and that it increases extracellu-lar glutamate as part of the mechanism. Yan and Weng[64] used both in vitro and in vivo techniques to study themechanisms by which IL-1 beta interacts with glutamatein neuropathic pain experimentally induced in youngadult male Sprague-Dawley rats. They concluded thatIL-1 beta uses presynaptic NMDA receptors to enhanceglutamate release from primary afferents in neuropathicrats. Yan et al. [65] analyzed tissue extracted from youngadultmale Sprague-Dawley rats subjected to partial sciaticnerve ligation. Based on several in vitro techniques todetermine the mechanisms involved, they concluded thatIL-1 beta contributes to neuropathic pain by suppress-ing glial glutamate uptake. The research reported in thissection is summarized in Table 4.After having seen considerable research suggesting thatIL-1 beta may increase glutamate, facilitate its receptors,or inhibit its uptake by glial cells in the context of epilepsyand other neuronal disturbances, we turned to glutamateand IBD.Glutamate and inflammatory bowel diseaseIn order to investigate glutamate and gastrointestinalphenomena, we issued two queries to Semantic MED-LINE, one focused on anatomy and another on disease.The anatomy-focused query (glutamate AND (bowel ORcolon OR intestine OR gastrointestinal OR stomach))retrieved 4718 predications from 500 citations. Manyof these discuss the relevance of normal levels of gluta-mate and its receptors to gastrointestinal processes. Forexample, two recent reviews highlight the prominenceof glutamatergic phenomena underpinning the mech-anisms of gastrointestinal functions. The predicationGut-LOCATION_OF-Glutamate was extracted fromJulio-Pieper et al. [66], which states that glutamate is themain neurotransmitter of the brain-gut axis (realizedin part by the vagus nerve). (Metabotropic) glutamatereceptors occur in the brain as well as throughoutthe gastrointestinal tract, from the mouth to the largeRindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 9 of 14Table 4 Summary of articles discussing IL-1 beta and glutamate (disorder and location not specified in query)Study Subjects Method Result/ConclusionCasamenti et al. [61] Adult male Wistar rats IL-1 beta injected into nucleusbasalis; microdialysisSignificant increase in glutamateProw and Irani [62] Spinal cord tissue extracted frommice challenged with neuroad-apted Sindbis virusImmunoblotting and immunohis-tochemistry, cytokine assays, andhistological analysisIncrease of IL-1 beta in responseto the virus disrupts glutamatehomeostasis (development ofexcitotoxicity)Fogal et al. [63] IL-1RI null mutant, mGluR1-/-, andwild-type control miceBoth in vitro and in vivo experi-mentsIL-1 beta increases extracellularglutamate as part of themechanism of neuronal injuryYan and Weng [64] Young adult male Sprague-DawleyratsIn vitro and in vivo techniques tostudy experimentally induced neu-ropathic painIL-1 beta enhances glutamaterelease from primary afferentsYan et al. [65] Tissue from young adult maleSprague-Dawley rats subjected topartial sciatic nerve ligationSeveral in vitro techniques IL-1 beta contributes toneuropathic pain by suppressingglial glutamate uptakeintestine, and are relevant to digestion as a whole. Thesereceptors are involved in several gastrointestinal reflexes,including swallowing, gastric accommodation, andemesis [67].In one of the citations retrieved with PubMedthat did not produce a SemRep predication, Clarkeet al. [68] investigated the kynurenine pathway oftryptophan degradation in plasma samples from 10male patients with irritable bowel syndrome (IBS)and 26 controls. High performance liquid chromatog-raphy revealed that concentration of the neuropro-tective metabolite kynurenic acid (an antagonist ofthe NMDA glutamate receptor) was decreased in theIBS subjects.The disease-focused query (glutamate AND(inflammatory bowel disease OR colitis)) retrieved995 predications from 89 citations. In the summarizedgraph, Colitis-PROCESS_OF-Rattus norvegicus wasextracted from Varga et al. [69]. In this study, kynurenicacid, an antagonist of NMDA (a glutamate receptor),was administered to male Wistar rats after inducingcolonic inflammation with trinitrobenzene sulfonic acid.Measurements conducted on anesthetized animals aswell as on blood samples and colon biopsies indicated asignificant modulatory effect, including reduced inflam-matory enzyme activities, decreased intestinal motility,and increased tone of the colon.Several of the MEDLINE citations from which Seman-tic MEDLINE did not extract a predication report anassociation between glutamate and intestinal phenom-ena. For example, Carpanese et al. [70] conducteda study based on in vitro cell cultures from adultmale rats. Based on immunocytochemistry, they con-clude that blockade of glutamate receptors (NMDA andAMPA/kainite) may protect enteric neurons subjected toin vitro chemically-induced ischemic injury followed byreperfusion.We then sought additional information on kynurenicacid and its potential role in mitigating gastrointesti-nal disturbances. The PubMed query, ((kynurenic acidOR kynurenine) AND inflammatory bowel disease)returned 7 citations. One of these was Forrest et al. [71], inwhich serum concentrations of purines and kynurenineswere measured in patients with mild IBD. In notingincreased levels of kynurenic acid compared to controls,they concluded that kynurenine modulation of glutamatereceptors is involved in the symptoms of IBD, either as aresponse to an abnormality or as a primary abnormalityitself. The studies we cite on glutamate and IBD are givenin Table 5.Finally, we issued two PubMed queries, one disease-focused and a second anatomy-focused, to look forresearch reporting on the interaction of IL-1 beta andglutamate in IBD. The first, (interleukin-1 AND gluta-mate AND (inflammatory bowel disease or colitis)),returned no citations. The second, (interleukin-1 ANDglutamate AND (bowel OR colon OR intestine OR gas-trointestinal OR stomach)), retrieved three citations, noneof which discuss the interaction of IL-1 beta and gluta-mate in the etiology of gastrointestinal disorders. Saperaset al. [72] discuss a possible effect of IL-1 beta ongastric acid secretion but do not mention the inter-action of interleukin-1 beta and glutamate. Morrow etal. [73] investigated the effect of murine IL-1 beta ongastric contractility, but did not address the relation ofIL-1 beta and glutamate in IBD. Finally, Qu et al. [74]present a review of epigenetic phenomena in gastriccancer. As part of the discussion, IL-1 beta and gluta-mate are mentioned, but their interaction in IBD is notaddressed.Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 10 of 14Table 5 Summary of articles discussing glutamate and inflammatory bowel diseaseStudy Subjects Method Result/ConclusionClarke et al. [68] Male patients with irritable bowelsyndrome and healthy controlsHigh performance liquid chromato-graphy on plasma samplesKynurenic acid was decreased inpatients with diseaseVarga et al. [69] Male Wistar rats after inducingcolonic inflammation with TNBSMeasurements on anesthetized ani-mals and on blood samples andcolon biopsies after administrationof kynurenic acidReduced inflammatory enzymeactivities, decreased intestinalmotility, and increased tone of thecolonCarpanese et al. [70] Enteric neuron cultures from adultmale ratsIn vitro ischemic injury; reperfusion;blockage of glutamate receptors;Immuno-cytochemistry to measurecytotoxicityBlockade of glutamate receptors(NMDA and AMPA/kainite) may beneuroprotectiveForrest et al. [71] Patients with mild IBD Measured serum concentrations ofpurines and kynureninesKynurenine modulation of gluta-mate receptors is involved in thesymptoms of IBDDiscussionThe iterative approach used during discovery browsingmirrors the iteration observed in studies of complex prob-lem solving [75] and is recognized more generally as aninherent part of the information seeking process [7678],characterized as six stages: task initiation, selection,exploration, focus formulation, collection, and presen-tation1. This work shows how predicates provided bySemRep and the interactive Semantic MEDLINE inter-face supports a user as they iterate between selection,exploration, and focus formulation steps.Based on the results of this study, the statistical corre-lation found in clinical data [9] can be explained by anincrease in glutamate due to IL-1 beta which is involvedin the etiology of both IBD and epilepsy. To recapitulatethe research that supports that claim, we first looked atIL-1 beta involvement with IBD and epilepsy individu-ally. It is widely accepted that IL-1 beta is etiologicallyassociated with IBD. Regarding epilepsy, there is con-siderable research suggesting that IL-1 beta is cruciallyinvolved in the mechanism of that disorder, and, further,that excess glutamate, being excitotoxic, also contributesto the etiology of epilepsy and seizure.We next looked at research investigating the interac-tion of IL-1 beta and glutamate in epilepsy. Several studiesbased on both animal and human in vitro and in vivostudies suggest that IL-1 beta induces glutamate activ-ity increasing glutamate, facilitating its receptors, andinhibiting its uptake by glial cells, although some stud-ies report that IL-1 beta inhibits glutamate under theirresearch conditions.Glutamate plays an important role in several normalgastrointestinal functions, and some research suggeststhat excessive levels of glutamate contribute to distur-bances. The strongest evidence for this is that the NMDAantagonist kynurenic acid has demonstrated therapeuticvalue in IBD models. Although it is widely accepted thatIL-1 beta plays a crucial role in IBD, we did not find anystudies investigating the interaction of IL-1 beta and glu-tamate in IBD. In the context of the rest of our findings,this would seem to be a potentially valuable direction topursue. Figure 3 shows an overview of the hypothesis andsupporting research.Finally, we queried PubMed in an effort to deter-mine whether the hypothesis that elevated glutamatelevels due to IL-1 beta are part of the mechanismof both IBD and epilepsy is novel. Despite this sup-porting evidence, the PubMed query, (interleukin1 AND glutamate AND (inflammatory boweldisease OR colitis) AND (seizure OR epilepsy))returned no citations, indicating the novelty of thishypothesis.There are some limitations to our study. First, discov-ery browsing is a means of generating hypotheses, notof determining evidence. Therefore, hypotheses derivedmust be subjected to experimental investigation to deter-mine their value and significance. Secondly, SemRepis not perfectly accurate; its precision is estimated tobe about 75% (lower for predications involving cellu-lar/molecular interactions) and its recall is lower (esti-mated to be approximately 50% [25]). Note, however, thatautomatic summarization filters out some of the precisionerrors, and also that a precision error can still be use-ful in pinpointing MEDLINE citations that merit closerscrutiny (as shown in one of the examples above). Onthe other hand, by issuing queries directly to PubMed,in addition to Semantic MEDLINE, we aimed to mit-igate the effect of recall errors. Lastly, our methodol-ogy relies solely on semantic predications and manualinspection of MEDLINE citations that they are extractedfrom. There is a wealth of taxonomic and relationalknowledge which can be mined directly from UMLS andbiomedical ontologies (e.g., Gene Ontology) and incorpo-rated into discovery browsing to pinpoint other interest-ing associations. We plan to explore this integration infuture work.Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 11 of 14Fig. 3 Overview of the hypothesis and supporting research. Increase in glutamate due to IL-1 beta may be involved in the etiology of both IBD andepilepsyConclusionData-mining studies using population-scale data have thepotential to identify novel correlations; however, they gen-erally do not provide plausible explanations for thesecorrelations. Although there is no single experiment thatdemonstrates the direct connection between interleukinand glutamate and inflammatory bowel disease or coli-tis, the discovery browsing approach used in this paperdemonstrates that there is evidence available to sup-port the mechanistic connection between the observationmade in a population study [9] that epilepsy and IBD oftenco-occur and that inflammation is likely involved. Wefollowed cooperative reciprocity in the discovery brows-ing methodology, which involves a complementary inter-action between the user and Semantic MEDLINE andPubMed, with the former suggesting ideas to pursue andthe latter two providing support or disconfirmation. Basedon the results of this method, we proposed the hypothesisthat IL-1 beta influence on glutamate levels is involved inthe etiology of both epilepsy and IBD. This hypothesis canunderpin the development of more effective therapeuticapproaches for both epilepsy and IBD.We conclude by observing that semantics-based discov-ery browsing is complementary to population-based cor-relation studies. The former provides depth (mechanism),while the latter provide breadth. It is possible to startwith either and use the other for support. In this studywe started with a correlation-based study and used dis-covery browsing to elucidate a mechanism. Hypothesessuggested by discovery browsing could also be supportedwith population studies.Endnote1 Prior work that studied biomedical researchers as theysystematically reviewed the literature also found that thisprocess holds, with one additional step (synthesis) thatoccurs between the collection and presentation stages[79, 80].AbbreviationsEHR: Electronic health record IBD: Inflammatory bowel disease IBS: Irritablebowel syndrome IL: Interleukin IL-1R1: Interleukin-1 receptor type 1 LBD:Literature-based discovery OVLT: OrganumRrganum vasculosum laminaeterminalis UMLS: Unified medical language systemAcknowledgementsWe acknowledge Kenneth Mandl and Mei-Sing Ong, whose research providedthe impetus for our investigation.FundingThis research was supported in part by the Intramural Research Program of theNational Institutes of Health, National Library of Medicine.Availability of data andmaterialsSemMedDB data used in this study is publicly available at https://skr3.nlm.nih.gov/SemMedDB/download/download.html.Authors contributionsTCR conceived of the study, performed the searches, and wrote the first draftof the manuscript. CLB, MJC, MF, CJZ, and HK contributed to discussions aboutthe generated hypothesis. HK finalized the manuscript. All authors read,edited, and approved the manuscript.Ethics approval and consent to participateNot applicable.Rindflesch et al. Journal of Biomedical Semantics            (2018) 9:25 Page 12 of 14Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Retired, Washington, DC, USA. 2School of Information Sciences, University ofIllinois at Urbana-Champaign, 501 E Daniel Street, 61820 Champaign, IL, USA.3Kaiser Permanente Southern California, 11975 El Camino Real, 92103, SanDiego, CA, USA. 4Independent researcher, Rio de Janeiro, Brazil. 5Departmentof Comparative Medicine, Yale School of Medicine, 06520 New Haven, CT,USA. 6Lister Hill National Center for Biomedical Communications, U.S. NationalLibrary of Medicine, 8600 Rockville Pike, Bethesda, MD, USA.Received: 20 July 2018 Accepted: 16 November 2018RESEARCH Open AccessDMTO: a realistic ontology for standarddiabetes mellitus treatmentShaker El-Sappagh1*, Daehan Kwak2, Farman Ali3 and Kyung-Sup Kwak3*AbstractBackground: Treatment of type 2 diabetes mellitus (T2DM) is a complex problem. A clinical decision support system(CDSS) based on massive and distributed electronic health record data can facilitate the automation of this process andenhance its accuracy. The most important component of any CDSS is its knowledge base. This knowledge base can beformulated using ontologies. The formal description logic of ontology supports the inference of hidden knowledge.Building a complete, coherent, consistent, interoperable, and sharable ontology is a challenge.Results: This paper introduces the first version of the newly constructed Diabetes Mellitus Treatment Ontology (DMTO)as a basis for shared-semantics, domain-specific, standard, machine-readable, and interoperable knowledge relevant toT2DM treatment. It is a comprehensive ontology and provides the highest coverage and the most complete picture ofcoded knowledge about T2DM patients current conditions, previous profiles, and T2DM-related aspects, includingcomplications, symptoms, lab tests, interactions, treatment plan (TP) frameworks, and glucose-related diseases andmedications. It adheres to the design principles recommended by the Open Biomedical Ontologies Foundry and isbased on ontological realism that follows the principles of the Basic Formal Ontology and the Ontology for GeneralMedical Science. DMTO is implemented under Protégé 5.0 in Web Ontology Language (OWL) 2 format and is publiclyavailable through the National Center for Biomedical Ontologys BioPortal at http://bioportal.bioontology.org/ontologies/DMTO. The current version of DMTO includes more than 10,700 classes, 277 relations, 39,425 annotations, 214 semanticrules, and 62,974 axioms. We provide proof of concept for this approach to modeling TPs.Conclusion: The ontology is able to collect and analyze most features of T2DM as well as customize chronic TPs withthe most appropriate drugs, foods, and physical exercises. DMTO is ready to be used as a knowledge base forsemantically intelligent and distributed CDSS systems.Keywords: Clinical decision support system, Treatment plan, Ontology, Knowledge modeling, Diabetes mellitusBackgroundDiabetes is a complex and potentially debilitating chronicdisease [1]. It affects many individuals, and represents aglobal health burden with a financial impact on nationalhealthcare systems [2]. Diabetes has two main clinical cat-egories: type 1 diabetes mellitus (T1DM) and type 2 dia-betes mellitus (T2DM). T2DM accounts for 9095% ofnew cases. In both conditions, continuous medical care isrequired to minimize the risk of acute and long-termcomplications. T1DM can only be treated with insulin,whereas patients with T2DM have a wide range oftherapeutic options available, including lifestyle changesand administration of multiple oral and/or injectableanti-diabetes drugs, including insulin [3, 4]. This studyconcentrates on the non-insulin medications for T2DM,which is a risk factor for cardiovascular diseases andmicrovascular complications [5].Lifestyle changes, including a healthy diet, weight loss,increased physical activity, self-monitoring of bloodglucose, and diabetes self-management education, canhelp a patients efforts at controlling hyperglycemia.However, they may not be adequate for controlling thedisease in the long term, and most patients will requirepharmacotherapy intervention to achieve and maintainglycemic control [6]. Individualized choices of medica-tions for patients are a challenge, because the number* Correspondence: shaker_elsapagh@yahoo.com; kskwak@inha.ac.kr1Information Systems Department, Faculty of Computers and Informatics,Benha University, Banha Mansura Road, Meit Ghamr - Benha, Banha, AlQalyubia Governorate 3000-104, Egypt3Department of Information and Communication Engineering, InhaUniversity, 100 Inharo, Nam-gu, Incheon 22212, South KoreaFull list of author information is available at the end of the article© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.El-Sappagh et al. Journal of Biomedical Semantics  (2018) 9:8 DOI 10.1186/s13326-018-0176-yof medications used to treat diabetes has dramaticallyincreased in the past few years. T2DM patients are usu-ally treated with multiple drugs, and the choice differsaccording to each patients profile [57].The most recent T2DM clinical practice guidelines(CPGs), including those from the American DiabetesAssociation (ADA) [5], Diabetes Canada (formerly theCanadian Diabetes Association) [8], and the EuropeanAssociation for the Study of Diabetes (EASD), recom-mend patient-centered and individualized diabetes ther-apy goals based on life expectancy, duration of diabetes,presence of comorbidities, potential for hypoglycemiaor other adverse events, and other profile features [9, 10].The tailored therapy decision for a specific patient iscomplex, because these decisions include checking manyinterrelated symptoms, and choosing from various medica-tions and lifestyle plans [11]. T2DM patients usually takemore than one drug, and drug interactions may occur. Therisk of harmful drug interactions that can cause hypergly-cemia, hypoglycemia, nephropathy, retinopathy, gastropar-esis, and sexual dysfunction (among other deleteriouseffects) increases exponentially as the number of medica-tions in a patients regimen increases [12, 13]. Interactionscan occur between different T2DM drugs, between drugsand complications from diabetes, between drugs and foods,or between drugs and exercise [1315]. In addition, theT2DM pathophysiology involves at least seven organs andtissues, including the pancreas, the liver, skeletal muscle,adipose tissue, the brain, the gastrointestinal tract, and thekidneys. Many treatment agents affect the seven organs in-volved in the pathogenesis of T2DM. Each agent has amechanism of action on these organs, and each has ad-verse effects and contraindications. Not every patient withT2DM will respond the same way to a given treatment.The reason might be that physicians do not take all of thepatients characteristics under consideration, includingJudkins et al. Journal of Biomedical Semantics  (2018) 9:15 https://doi.org/10.1186/s13326-018-0183-zRESEARCH Open AccessExtending the DIDEO ontology to includeentities from the natural product druginteraction domain of discourseJohn Judkins1, Jessica Tay-Sontheimer2, Richard D. Boyce3 and Mathias Brochhausen1*AbstractBackground: Prompted by the frequency of concomitant use of prescription drugs with natural products, and thelack of knowledge regarding the impact of pharmacokinetic-based natural product-drug interactions (PK-NPDIs), theUnited States National Center for Complementary and Integrative Health has established a center of excellence forPK-NPDI. The Center is creating a public database to help researchers (primarly pharmacologists and medicinalchemists) to share and access data, results, and methods from PK-NPDI studies. In order to represent the semantics ofthe data and foster interoperability, we are extending the Drug-Drug Interaction and Evidence Ontology (DIDEO) toinclude definitions for terms used by the data repository. This is feasible due to a number of similarities betweenpharmacokinetic drug-drug interactions and PK-NPDIs.Methods: To achieve this, we set up an iterative domain analysis in the following steps. In Step 1 PK-NPDI domainexperts produce a list of terms and definitions based on data from PK-NPDI studies, in Step 2 an ontology expertcreates ontologically appropriate classes and definitions from the list along with class axioms, in Step 3 there is aniterative editing process during which the domain experts and the ontology experts review, assess, and amend classlabels and definitions and in Step 4 the ontology expert implements the new classes in the DIDEO developmentbranch. This workflow often results in different labels and definitions for the new classes in DIDEO than the domainexperts initially provided; the latter are preserved in DIDEO as separate annotations.Results: Step 1 resulted in a list of 344 terms. During Step 2 we found that 9 of these terms already existed in DIDEO,and 6 existed in other OBO Foundry ontologies. These 6 were imported into DIDEO; additional terms from multipleOBO Foundry ontologies were also imported, either to serve as superclasses for new terms in the initial list or to buildaxioms for these terms. At the time of writing, 7 terms have definitions ready for review (Step 2), 64 are ready forimplementation (Step 3) and 112 have been pushed to DIDEO (Step 4). Step 2 also suggested that 26 terms of theoriginal list were redundant and did not need implementation; the domain experts agreed to remove them. Step 4resulted in many terms being added to DIDEO that help to provide an additional layer of granularity in describingexperimental conditions and results, e.g. transfected cultured cells used in metabolism studies and chemical reactionsused in measuring enzyme activity. These terms also were integrated into the NaPDI repository.Conclusion: We found DIDEO to provide a sound foundation for semantic representation of PK-NPDI terms, and wehave shown the novelty of the project in that DIDEO is the only ontology in which NPDI terms are formally defined.Keywords: Biomedical ontologies, OWL, Pharmaceuticals, Pharmacokinetics, Drug-drug interactions, Naturalproduct-drug interactions*Correspondence: mbrochhausen@uams.edu1Department of Biomedical Informatics, University of Arkansas for MedicalSciences, Little Rock, AR, USAFull list of author information is available at the end of the article© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Judkins et al. Journal of Biomedical Semantics  (2018) 9:15 Page 2 of 9BackgroundConcomitant use of prescription drugs and natural prod-ucts, including vitamin, mineral, or herbal supplements,is a frequent occurrence. The high prevalence of naturalproducts (NP) usage raises concerns about the poten-tial impact on drug effectiveness and toxicity from nat-ural product drug interactions. Pharmacokinetic-basednatural product-drug interactions (PK-NPDIs) are of par-ticular concern because their potential impact on drugeffectiveness or toxicity is often unknown.To provide evidence-based information regarding pur-ported PK-NPDIs, a new Center of Excellence for PK-NPDI Research was established by the United StatesNational Center for Complementary and IntegrativeHealth (grant number U54 AT008909). The Center iscreating a publicly accessible database where researcherscan access scientific results, raw data and recommendedapproaches to optimally assess the clinical significanceof PK-NPDIs. One of the requirements of the reposi-tory is that it represent data in a semantically rich andinteroperable way.There have been previous efforts to provide ontolog-ical representation of this domain. We reviewed exist-ing ontologies to consider re-use. Searching the NCBOBioportal [1] retrieves the Natural Product Ontology(NATPRO) [2], which we consider a potentially relevantresource for our project. According to an article by theperson whose name appears as contact on the Bioportalsite for NATPRO [3], it seems that the use case underwhich NATPRO was developed was mining informationabout natural products to bring new ideas to drug devel-opment, which is similar to our own goals. NATPROwas submitted to the NCBO Bioportal in 2012 and, atthe time of writing, there are no reported updates. TheNCBO BioPortal landing page for NATPRO does not pro-vide additional documentation or links to resources (e.g. acode repository). The ontology contains 9465 classes and22012 individuals, is based on the BioTop ontology [4], atop-domain ontology for the life sciences, re-uses classesfrom Chemical Entities of Biomedical Interest (ChEBI) [5]Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 https://doi.org/10.1186/s13326-018-0177-xRESEARCH Open AccessSupporting shared hypothesis testing inthe biomedical domainAsan Agibetov1,6 , Ernesto Jiménez-Ruiz2, Marta Ondrésik3,4, Alessandro Solimando5, Imon Banerjee1,7,Giovanna Guerrini5*, Chiara E. Catalano1, Joaquim M. Oliveira3,4, Giuseppe Patanè1,Rui L. Reis3,4 and Michela Spagnuolo1AbstractBackground: Pathogenesis of inflammatory diseases can be tracked by studying the causality relationships amongthe factors contributing to its development. We could, for instance, hypothesize on the connections of thepathogenesis outcomes to the observed conditions. And to prove such causal hypotheses we would need to havethe full understanding of the causal relationships, and we would have to provide all the necessary evidences tosupport our claims. In practice, however, we might not possess all the background knowledge on the causalityrelationships, and we might be unable to collect all the evidence to prove our hypotheses.Results: In this work we propose a methodology for the translation of biological knowledge on causalityrelationships of biological processes and their effects on conditions to a computational framework for hypothesistesting. The methodology consists of two main points: hypothesis graph construction from the formalization of thebackground knowledge on causality relationships, and confidence measurement in a causality hypothesis as anormalized weighted path computation in the hypothesis graph. In this framework, we can simulate collection ofevidences and assess confidence in a causality hypothesis by measuring it proportionally to the amount of availableknowledge and collected evidences.Conclusions: We evaluate our methodology on a hypothesis graph that represents both contributing factors whichmay cause cartilage degradation and the factors which might be caused by the cartilage degradation duringosteoarthritis. Hypothesis graph construction has proven to be robust to the addition of potentially contradictoryinformation on the simultaneously positive and negative effects. The obtained confidence measures for the specificcausality hypotheses have been validated by our domain experts, and, correspond closely to their subjectiveassessments of confidences in investigated hypotheses. Overall, our methodology for a shared hypothesis testingframework exhibits important properties that researchers will find useful in literature review for their experimentalstudies, planning and prioritizing evidence collection acquisition procedures, and testing their hypotheses withdifferent depths of knowledge on causal dependencies of biological processes and their effects on the observedconditions.Keywords: Biomedical ontology, Ontology mapings, Network analysis, Hypothesis testing, Incomplete knowledgeBackgroundDiseases and pathologies may be evidenced across mul-tiple biological scales (e.g., cellular, molecular, organic,behavioural) as a set of factors, linked among each othervia causal relationships, which constitute the multi-scalepathological cascade reactions. To study the underlying*Correspondence: giovanna.guerrini@unige.it5University of Genoa, Genoa, ItalyFull list of author information is available at the end of the articlecausation mechanism of a certain disease, life scienceresearchers rely on various sources, such as (i) currentknowledge (e.g. previously published studies from thefield), (ii) their data deduced from empirical analysisof laboratory experiments (e.g., gene analysis, immuno-assays, cell viability assays, histology) or other tests (i.e.mechanical tests, imaging, gait analysis), as well as on (iii)consultations with other fields (i.e. related research areas,hospitals). To effectively make and test (prove or reject) a© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 2 of 22causality hypothesis life science research studies face twochallenges: i) the information used in research processescomes from various sources and is heterogeneous, whichmakes it hard to organize, analyze, and assess their rele-vance in the overall disease process, ii) researchers fromdifferent fields (i.e. molecular biologist, mechanobiologist,orthopaedists etc.) investigate the same pathological eventfrom different aspects (biological scales), and might notbe aware of the overlaps and the impact of their individ-ual findings in a joint venture of understanding causalitymechanisms of pathologies and diseases.To better convey the idea of causality hypothesis test-ing we will focus on knee articular cartilage degenerationduring the onset of osteoarthritis (OA) to present our use-case scenario. OA is a joint degenerative disease and canbe caused due to several factors, such as genetic predis-position, joint overuse, previous injury to the joint. Theeffect of these factors is hallmarked with a complete jointbreakdown and dysfunction, causing a lot of pain [1, 2].Based on common knowledge, performed experiments,and diagnosis the causality relation of certain factors tothe development of OA might have different degrees ofconfidence. On the one hand, the degeneration of carti-lage, synovial thickening, osteophyte formation and jointspace narrowing, are known to be as the most markedfeatures of OA [36]. On the other hand, for some fac-tors we may have lower degrees of confidence in theircausality relationship to OA. For instance, while beingcommon in patients with OA, the exact causality rela-tion of inflammation to OA is not completely understood[7, 8]. To handle such scenarios of causality hypothe-sis testing, we propose to translate what we observe inthe biology into a computational framework, which sup-ports the researchers in their hypothesis testing. In sucha framework we systematically translate our backgroundknowledge on causality relationships into the represen-tations suitable for the computation, and we quantifyconfidences in our hypothesis with respect to the amountof evidences that we can supply to the framework.Hypothesis testingSchematically, the causality relationships between the fac-tors of diseases can be represented as directed causal-ity networks H0...n, where factors fi are represented asnodes and the causality relationships as arcs (fi, fj). Forinstance, our hypothesis H0 can state that inflammationcontributes to the development of OA, where the inflam-mation is the cause of biological processes which lead tocartilage degradation (factor f2, Fig. 1) and finally mani-fest in joint deformation condition (factor f3, Fig. 1). Toprove such a causality hypothesis we need to evidence theinstances of all the participating factors. For example, thefactors f2, f3 are evidenced as the results of diagnosis ofOA done by radiologists and orthopaedists using imag-ing techniques (i.e. magnetic resonance-MRI, X-ray). Bystudying the literature we can discover that the inflamma-tion can be characterized by the detection of high levelsof pro-inflammatory factors in the synovial cavity, and inparticular tumor necrosis factor alpha (TNF?) (factor f1in Fig. 1), was demonstrated to be present in excess dur-ing OA [9]. A justification or evidence for the factor f1(evidence of f1 in Fig. 1) can be obtained with molecu-lar biological techniques screening the biomarkers of thesynovial fluid. Given our knowledge of the participatingFig. 1 Causality hypothesis of TNF alpha overproduction leading to cartilage degeneration and provoking joint deformationAgibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 3 of 22biological processes (hypothesis H0) and the supportingevidences (evidences for factors f1, f2, f3) we have a cer-tain level of confidence that the synovial inflammation hasbeen the cause of the development of OA. However, isour hypothesisH0 complete enough, and are the evidencesfor factors (f1, f2, f3) enough to support our hypothesis?Have we missed other factors? Have we been completeenough in our characterization of all the participatingfactors which support the hypothesis that the synovialinflammation has been the cause of cartilage degradation?Is the joint deformation the only consequence of such apathological cascaded of reactions?Studying further the causality mechanism of OA, wecan refine our initial hypothesis H0. In particular, cel-lular biological studies observed that TNF? facilitatesthe catabolic processes of the chondrocytes, includingthe production of matrix metalloproteinases (MMPs),and the production of aggrecanases (members of theADAMTs family) [10, 11]. The MMPs, especially MMP-13 and aggrecanases are proteases responsible for thedegradation of collagen macromolecules and proteogly-cans respectively, as evidenced in literature [12]. Collagensand proteoglycans are the main building blocks of articu-lar cartilage. Accordingly, the excess of TNF? in the jointspace can be associated to the disruption of biochemicalbalance in the cartilage. Factors: Loss of collagen and pro-teoglycan molecules (factors f4, f5 in Fig. 2), are causedby the action of matrix degrading proteases, and can beattached to higher scales in the OA processes, such as themechanical functioning of cartilage. These factors can beevidenced on the tissue level by histology and immuno-histochemistry (evidences of f4, f5 in Fig. 2). Collabora-tions with mechano-biological fields allow the detectionof the changes in cartilage mechanical properties dueto the effect of high levels of MMPs and aggrecanses[13, 14]. It has been shown previously that once the carti-lage suffers collagen loss, it is no longer able to withstandthe mechanical forces in the knee [15, 16]. Consequently,the cartilage, the trabecular bone beneath it, and all sur-rounding tissue components suffer damage, which can beevidenced by imaging [17, 18]. Damage to the joint com-ponents, will cause pain, joint deformation and loss offunction, which is a subject of behavioural scales and canbe evidenced by gait analysis [19].The relationship between inflammation and OA is evenmore complex, than the example brought above. Nonethe-less, collaborations among medical doctors and benchresearchers of various fields can reveal the connectionsbetween molecular evidence and those observed on organscale. Accordingly, we can refine our hypothesis by addingnew causal relationships.Shared hypothesis testing frameworkIn this work we propose a methodology for the translationof biological knowledge on causality relationships ofbiological processes and their effects on conditions to a com-putational framework for hypothesis testing. The method-ology consists of two main points: hypothesis graphFig. 2 Refined causality hypothesis of pro-inflammatory factors leading to loss of building blocks of articular cartilage  collagen and proteoglycan ,which in turn lead to cartilage degeneration and provoking joint deformationAgibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 4 of 22construction from the formalization of the backgroundknowledge on causality relationships, and confidencemeasurement in a causality hypothesis as a normalizedweighted path computation in the hypothesis graph. Inthis framework, we can simulate collection of evidencesand assess confidence in a causality hypothesis by mea-suring it proportionally to the amount of available knowl-edge and collected evidences. We evaluate our methodon an example causality hypothesis of factors which causeand, in turn, may be caused by cartilage degenerationduring osteoarthritis. The results of the evaluation andthe feedback from the domain experts allow us to con-clude that our methodology may simulate the executionof evidence collection, and can be used as a means ofmeasuring the confidence in a causality hypothesis withrespect to the amount of knowledge on causality rela-tionships among participating factors. Such simulationsupports the researchers in the planning and in the pri-oritization of their next studies by identifying impor-tant factors in a causality hypothesis. Our methodologydemonstrates robustness towards the addition of poten-tially inconsistent knowledge by separately representingopposite causality possibilities for complementary biolog-ical scenarios.We would like to emphasize that the contribution ofthis work is the methodology to extract the causalityinformation from the input ontologies into a hypothesisgraph, and perform hypothesis testing on the obtainedhypothesis graph. The ontologies and the ontology map-pings discussed and provided are created together withthe domain experts, and in the context of this work areonly meant to serve as proof of concept.Related workTo the best of our knowledge the proposed method-ology to test a causality hypothesis in a collaborativesetting with respect to the amount of knowledge avail-able for the framework does not have an equivalentmethodology or an implemented system to test against,in its entirety. However, once decomposed, our method-ology can be compared on specific steps and modellingchoices.Formalization of background knowledge on a causalhypothesis as ontologies. Our methodology for causal-ity hypothesis testing relies on the formalization of thebackground knowledge on a hypothesis with ontologies.Indeed, to facilitate knowledge sharing and increaseunderstanding of the method in use, it is common toemploy already existing ontologies that are well agreedon in the biomedical community (e.g., Gene Ontology[20]). The most widely used ontology modeling languageis the (OWL 2) [21], based on formal logic [22]. The mainadvantage of using logic over alternative representationmechanisms is that logic provides an unambiguous mean-ing to ontologies. We assume that the input ontologiesto our framework focus on (biological) processes andfindings (i.e., laboratory tests) that are or may be linkedvia a causality relationship, and other (material) entitiesthat (actively or passively) participate in the process orfinding. In this work we assume that the input ontologiesfollow good practices and relevant ontology classes areeither subsumed by or annotated with, for example,the concept Biological_process (key concept inthe Gene Ontology [20]) or Finding (e.g., commonsemantic type in the UMLS semantic network [23]). Weexpect the following (object) properties or its poten-tial subproperties as source for causality relationships:causes, results in, regulates, positivelyregulates, negatively regulates, increaseslevels of and decreases levels of. Mostof these properties are available in the Relations ontology[24] and are extensively used in biomedical ontologies.We reuse the domain independent categories Continuantand Occurrent, which are commonly used in the literature(e.g., River Flow Model of Diseases (RFM) [25]) and inupper ontologies (e.g., DOLCE [26] and BFO [27]). Forexample, processes and findings are typically classified asoccurrents, while material entities as continuants.Graph projection of OWL ontologies. The hypothesisgraph construction heavily relies on the graph projectionof OWL ontologies. This procedure, at its core, trans-forms an OWL ontology into its graph representation,by studying the axiomatic structure of the ontology andidentifying nodes and edges (arcs) of its equivalent graphrepresentation. Implicitly, Lembo et al. [28] use graph pro-jections of OWL QL to propose ontology classificationalgorithm, which transforms OWL QL ontologies intodirected graphs, and computes subsumption relations viatransitive closure computation. Analogously, Seidenberget al. [29] use graph representation of ontologies to pro-pose a segmentation algorithm based on subgraph extrac-tion procedure. Some of the proposed methodologies forgraph projection of OWL ontologies draw their inspira-tions from Social Network Analysis (SNA) [30] for therepresentation of the encoded semantic information in anOWL ontology. SNA is the process of investigating socialstructures of connected information/knowledge entitiesthrough the use of network and graph theories. SNAtechniques application to ontology analysis has been pio-neered by Hoser et al. [31], where standards in SNAcommunity graph metrics based on: node degree, nodebetweenness and on eigenanalysis of the adjacencymatrix,were used to study properties of ontologies. The con-nection between SNA and ontology analysis have alsobeen studied in a highly cited paper by Mika [32], bridg-ing Social Networks and Semantics. Network partitioningAgibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 5 of 22algorithms have been used by Stuckenschmidt et al. [33]to identify islands of ontology, a notion comparable to amodule of ontology (as used by the graph-based modu-lar extraction community), with the applications to VisualAnalytics. Grontocrawler [34] transforms OWL-EL [35]ontologies into networks by defining a rule-based edgeproduction procedure, which takes into account exis-tential and values restrictions on object relations. For-mal treatment of rule-based graph projection proceduresand their connection to the logical entailment prob-lem for OWL 2 ontologies have been recently proposed[3638]. In our work we use Grontocrawler [34] for graph-based ontology projection, enriched with the projection ofadvanced OWL 2 axioms, as suggested in Soylu et al. [38].Rule-based reasoning with incomplete knowledge inthe biomedical domain. Similarly to previous works[39, 40], we focus on graph-based reasoning with incom-plete knowledge, by analyzing OWL ontologies, to sup-port researchers in the biomedical domain. In particular,Larson et al. [39] propose a method for rule-based reason-ing with a multi-scale neuroanatomical ontology, wherethe authors conclude that OWL is an important technol-ogy formerging disparate data and performingmulti-scalereasoning. They demonstrate how OWL-based ontolo-gies and rule-based reasoning help infer novel facts aboutbrain connectivity at large scale from the existence ofsynapses at a micro scale. Oberkampf et al. [40] pro-pose a methodology for interpreting patient clinical data(medical images and reports), semantically annotated byconcepts from large medical ontologies. They introducean ontology containing lymphoma-related diseases andsymptoms as well as their relations and use it to infer likelydiseases of patients based on annotations.In contrast to Larson et al. [39] our graph-based rea-soning method relies on network analysis of the finalhypothesis graph, which presents an advantage of a fulloverview of all possible conclusions with the quantifica-tion of the confidence measure induced by the numberof evidences that have been collected and the final topol-ogy of the hypothesis graph. Oberkampf et al. [40] focuson the problem of inferring likely diseases in the presenceof patient-specific evidences, represented as symptoms,and the similarity of the diseases is then ranked based ontheir distances to the symptoms. The focus of our workand the methodology are different. We tailor our causalityhypotheses to a single diseases and study causality rela-tionships among the factors, the findings obtained withour methodology may have impact not only in the clin-ical, patient-specific setting, but can be used in generalresearch. Technically, our methodology for graph pro-jections employs a rich set of OWL 2 axioms, and gobeyond the usual taxonomical relationships which can beextracted from the ontologies.Probabilistic methodologies for reasoning withincomplete knowledge and causality inference, withapplications in the biomedical domain. In a more gen-eral setting, not necessarily connected to the biomedicaldomain, there are examples of general theoretical frame-works which marry formal methods (e.g., First-OrderLogic) and probabilistic models (e.g., stochastic processes)[4143]. Application of those methodologies in biology isstudied in Ciocchetta et al. [44] who tune the StochasticProcess Algebra language PEPA [43] to model biologicalpathways and complex biological networks, involvingstochastic processes. This line of works bridge uncer-tainty and formal methods for general frameworks forreasoning with incomplete knowledge in biology, anddifferently with our methodology is not compatible withOWL ontologies, and thus cannot benefit from OWLreasoning tasks (e.g., classification, alignment).Our work is perhaps similar in spirit to that of Pearlet al. [45, 46], where the authors advocate for a paradig-matic shift that must be undertaken in moving from tradi-tional statistical analysis to causal analysis of multivariatedata [45, 46]. Pearl et al. propose a formal treatment anda unified methodology for the graphical representation ofjoint probability distributions along with rules for infer-ring causality directly from such graphical representa-tions. In particular, the directed graphs are introduced asa compact way of representing conditional independencerestrictions for complex multidimensional probability dis-tributions. In contrast, in our work we do not stress theexistence of joint probability distributions between thefactors of a hypothesis. Rather, we rely on expert knowl-edge of causality relationship between the factors, alreadyknown to the community, such as knowledge graphswhich can be obtained from literature sources, and/or canbe formalized in an OWL ontology by the domain experts.MethodsHerein we assume that there exists a universal causal-ity hypothesis H that can be represented as a networkof factors with causality relationships, which we calla hypothesis graph. The background knowledge on thehypothesis graphH is formalized in an ontologyO, which,for instance, may define factors as biological processes andconditions, and the causality relationships may indicatethe connections between them. Moreover, we assume thatdifferent experts formalize the background knowledge onH in ontologiesOi=1...n, such that eachOi highlights a cer-tain subpart of this hypothesis graph H. Consider O1 =?RboxO1 , TboxO1?, O2 = ?RboxO2 , TboxO2? in Fig. 3, theexamples of formalization of the the causality relation-ships among biological processes that participate in OApathogenesis, from two different points of view.The overlaps among the ontologies Oi may or maynot exist and, as the number of ontologies increases,Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 6 of 22Fig. 3 Formalization of knowledge on OA pathogenesis processeswe assume that it is possible to assemble (align) theseontologies. The assembled ontology?ni Oi = O repre-sents the iteratively gathered and formalized biologicaland biomedical knowledge on the hypothesis graph H.Finally, the causality hypothesis graph H  the network offactors interconnected with causality relationships  canbe extracted from the assembled ontology O at any givenpoint in time ti (Ht0 , . . . ,Htn ). As a consequence, the shapeof the causality hypothesis Hti depends on the amountof background knowledge formalized in O at ti. Finally,the hypothesis graph construction from ontologies is per-formed in a three-step process: (1) projection of OWL 2ontologiesO1, . . . ,On into ontology graphsG1, . . . ,Gn, (2)assembly of the ontology graph G from G1, . . . ,Gn, and(3) normalization of the graph G to obtain the hypothesisgraph H (Fig. 4).Graph-based ontology projectionsThe nodes of the ontology-graph are unary predicates andedges are labelled with possible relations between suchelements, that is, binary predicates. The key property ofthis ontology-graph is that every X-labelled edge e = (v,w)is justified by one or more axioms entailed by the ontologywhich semantically relates v to w via X. For exam-ple, edges e of the form A broader????? B are justified bythe OWL 2 axiom: B SubClassOf: A . We rely onthe OWL 2 reasoner HermiT [47] to build the ontologygraph (e.g., extraction of classification) to consider bothexplicit and implicit knowledge defined in the ontologyO. In the following, A,Asup,Asub,B,Bi represent classes,while R, S, Si,R? represent object properties. Edges e ofthe form A R?? B are justified by the following OWL 2axioms:(i) A SubClassOf: R restriction B, where restrictionis one of the following: some (existentialrestriction), only (universal restriction),min x(minimum cardinality),max x (maximumcardinality) and exactly x (exact cardinality).Note that axioms with an union of classes in therestriction (e.g. A SubClassOf: R restrictionB1 or . . . or Bn) or an intersection of classes in therestriction (e.g. A SubClassOf: R restrictionAgibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 7 of 22Fig. 4 Our methodology defines a pipeline to transform background knowledge into a hypothesis graph via sequential application of processingsteps: projection of input Oi ontologies into ontology graphs Gi , assembly of an ontology graph G with input ontology mappingsmi , normalizationof the ontology graph G into a final hypothesis graph HB1 and . . . and Bn) also justify edges of the formA R?? Bi with 1 ? i ? n.(ii) Nesting (one level) with the same object property:A SubClassOf: R restriction (R restriction B),being R transitive.(iii) Nesting (one level) with different properties:A SubClassOf: R restriction (S restriction B),and the role chain axiom of the form:R ? S SubPropertyOf: R.(iv) A combination of range and domain axioms of theform: R Domain: A and R Range: B.(v) Role chain axiom of the form: S0 ? · · · ? SnSubPropertyOf: R when the ontology graphalready includes the edges A S0?? C1 . . .Cn Sn?? B.(vi) R InverseOf: R? when the ontology graph alreadyincludes the edge B R??? A.(vii) Top-down propagation of restrictions:A SubClassOf: Asup when the ontology graphalready includes the edge AsupR?? B.(viii) Entailment among restrictions:Bsub SubClassOf: B when the ontology graphalready includes the edge A R?? Bsub.Assembly of ontology graphsThe ontologies formalizing the hypothesis graph maybe created by different group of experts with dif-ferent modelling (e.g., defining relationships betweenoccurrents, or between ocurrents and continuants)and naming conventions. For example, a group mayuse the concept Cartilage degradation (occur-rent) from SNOMED-CT [48] while another may pre-fer to use the concept negative regulation ofcartilage development (occurrent) from the GO[20]. Furthermore, other groups would rather use the con-cept Cartilage (continuant) and push the semantics ofdegradation into the ontology property.Ontology alignment will enable the integration andassembly of the (sub-)ontology graphs in a larger ontol-ogy graph. An ontology alignment is composed bya set of ontology mappings. An ontology mappingm between two concepts C1,C2 from the vocabularyof two different ontologies O1,O2 can be defined asfollows: m = ?C1,C2, r?, where r is the relationbetween C1 and C2 and, using SKOS vocabulary, itcan be of one of the following types: skos:exactMatch,skos:closeMatch, skos:relatedMatch, skos:narrowMatch orskos:broadMatch.Mappings to guide the assembly (i.e., link factorsfrom different hypothesis) can be discovered in onlineresources like UMLS Metathesaurus [49] and BioPortal[50, 51], or using state of the art ontology alignmentsystems like LogMap [52] and AML [53]. Mappingsin UMLS Metathesaurus or BioPortal typically repre-sent correspondences of the type skos:exactMatch andskos:closeMatch,1 while the output provided by automaticsystems will typically provided mappings of diverse typeand quality.If a mapping exists to link two factors f1 and f?1 fromtwo different (sub-)ontology graphs, then these two fac-tors are merged into one. The weight of the mergedfactor will be according to the type of the ontology map-ping. In our setting, we assume the following weightvalues w (ranging from 0 to 1) depending on the map-ping type: (1) skos:exactMatch mappings are associatedwith a weight value 1.0, (2) skos:closeMatch mappingswith 0.75, while (3) skos:relatedMatch, skos:narrowMatchand skos:broadMatch with a weight of 0.5. The weightassociated to each (merged) factor will play a key rolein our methodology for confidence measurement in ahypothesis.Normalization of the assembled graphThe final step of hypothesis graph construction is thenormalization of the assembled hypothesis graph, whichpushes the rich semantics of causality relationships (e.g.,edges of the type A R?? B ) into, possibly newly created,nodes. Generally speaking, the normalization procedureleads to a simplified representation of all the availablefacts on causality relationships as a directed graph withspecific constraints on the types of nodes and edges.Specifically, we aim to build a 1-mode network where allthe nodes represent the same fundamental metaphysicaltype (occurrent), and all the edges represent the simplifiedcausality relationship defined between two occurrents.Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 8 of 22This is necessary because the general graph projectionstep of our pipeline might produce semantic networks ofconcepts where the concepts and the edges may have dif-ferent types. For instance, the ontology graph may containedges representing causality relationships involving bothan occurrent and a continuant  two fundamentally dif-ferent metaphysical types of concepts. Additionally, thesemantics of causality relations may reflect complemen-tary effect when we consider causal chains in the hypothe-sis graph, for instance negative and positive regulations ofbiological processes. The hypothesis graph normalizationconsists in iterative rewriting of the graph, where we fil-ter all edges and rewrite them according to the followingpatterns:(i) Occurrent R?? Occurrent where R represent theproperty results in or causes justifies the edge in thehypothesis graph Occurrent ? Occurrent. Forexample, if the ontology contains the axiom,Chondrocyte catabolism SubClassOf:results in some Collagen degradationthe ontology graph will include the edgeChondrocytes catabolismresults in??????Collagen degradation and the hypothesisgraph will contain the causality relationshipChondrocytes catabolism ? Collagendegradation.(ii) Occurrent R?? Occurrent where R represent theproperty positively regulates ornegatively regulates. In this case thepositive or negative semantics of the property arepushed to a fresh ocurrent concept. For example, ifthe ontology projection contains the edgeChondrocytes anabolismpositively regulates???????????Collagen production, we will add the causalrelationship Chondrocyte anabolism ?Positive regulation of Collagenproduction.(iii) Occurrent R?? Continuant where R represent theproperty positively regulates,negatively regulates, increaseslevels of or decreases levels of. Forexample if the ontology graph includes the edge TNFalpha overproductiondecreases levels of???????????Collagen the hypothesis graph will include thefresh term Decreased levels of Collagen(or Loss of Collagen) and the causalrelationship TNF alpha overproduction ?Decreased levels of Collagen.In Fig. 5 we illustrate the whole pipeline of construct-ing a hypothesis graph H from the two input ontologiesO1,O2, defined in Fig. 3. The two ontology graphs G1,G2represent the individual extent of background knowledgeof the two specialists on causality relationships of fac-tors between synovial inflammation and cartilage degra-dation (obtained by projecting ontologies O1,O2). Theassembly of the graphs takes as input the ontology map-pings m1 and m2 (see Table 1), which have been manu-ally created by the domain experts, to merge the graphsG1,G2. Overall, the graph projection and the graph assem-bly steps of the pipeline work in couple to entail newcausal links among the factors, which we represent inthe assembled graph G. For instance, once we align thetwo graphs we entail the circular causality relationship,which states that Synovial inflammation may be,simultaneously, the cause and the effect of Cartilagedegradation. Notice that before the alignment the twospecialists were not aware of this circular relationship. Thenormalization of the assembled graphG splits the two bio-logical scenarios of chondrocytes anabolic and catabolicactivities, such that the resulting hypothesis graph H con-tains only unambiguous causality relations among thefactors.Measuring confidence in a hypothesisOnce we obtain the hypothesis graph H, we are readyto form the causality hypothesis and perform evidence-based hypothesis testing. Before we delve into this topic,we briefly introduce the notation that we use for thehypothesis graphs throughout this work.Notation for hypothesis graphs. LetH = (N ,A) be a directed graph, which we call hypothesis graph, with ni ? Nset of nodes. And A is a set of ordered pairs of (s, t) in N,called arcs, where s denote the source of the arc, and t thetarget of the arc [54]. A path ?(s, t) from source node sto the target node t is denoted as ?i(s, t) = (s, ni, . . . , t).We write (s, t) to denote all possible simple paths inthe hypothesis graph from node s to the node t. A sim-ple path is a path which does not have repeating nodes.And we use I(s, t) = {ni|ni ? ?i, ??i(s, t) ? (s, t)} torefer to all the interior nodes which appear in all pathsfrom s to t.Causality hypothesis. A causal hypothesis asks a ques-tion whether some factor (s) has caused another factor (t).There might be a direct causality relationship from s tot, or there might exist an indirect causality relationship,such that s has caused t through some intermediate fac-tors, which might have participated actively or passivelyto the causality chain from s to t. These causal chainsfrom s to t represent different possibilities of how smighthave caused t. We use the notation for hypothesis graphH to represent factors as nodes fi ? N , direct causalityrelationships as arcs (fi, fj) ? E, and causality chains aspaths (s, t).Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 9 of 22Fig. 5 Schematic representation of the three-step pipeline for the hypothesis graph H creation from the two input ontologies O1,O2: i) use graphprojection rules to transform each ontology Oi into its graph representation, ii) assemble the hypothesis graph H from two ontology graphs bymerging concepts for which we have ontology mappingsmi , and finally iii) normalize the hypothesis graph H by extracting only the relevantinformation of causality relationships among the occurrentsAgibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 10 of 22Table 1 Ontology mappings created manually by the domain expertsMapping mi O1 : C1 O2 : C2 r cm1 O1:SynovialinflammationO2:Synovial capsuleinflammationskos:closeMatch 0.75m2 O1:BiochemicalimbalanceO2:Disruption of biochemicalbalanceskos:relatedMatch 0.5Consider an example causality hypothesis that pos-tulates that s = Positive regulation of TNFalpha overproduction caused t = Synovialinflammation in Fig. 6. In our example, we do not havea direct causality relationship between these two factors,however there exist 6 different causal chains, i.e., 6 dif-ferent ways in which s might have caused t. In Fig. 6 wepresent two possible chains of factors (Path 1, Path 2)starting from s and leading to t.We are confident in our causality hypothesis  withinthe domain of the known facts  when we are able toprovide evidences to all the factors that participate incausality chains from s to t. I(s, t) represents the set ofnodes in the hypothesis graph H, which correspond tothe factors that need to be evidenced, E is an indicatorset which denotes factors evidenced so far, and C(s, t, E)be the confidence function. Intuitively, confidence in ahypothesis should grow with the number of factors thatwe are able to evidence, more factors we evidence, moreconfident we are that s did indeed cause t. Since, wemight have several possibilities of s causing t we, first,propose to measure confidence of each causality possibil-ity separately, and then, we propose to measure overallcausality hypothesis as a sum of the confidences of all theknown possibilities (Eq. 1). To this end, our confidencein a causality hypothesis depends on three parameters: i)source of the causality (s), ii) target of the causality (t), andiii) set of evidenced factors (E).Cts (E) =???(s,t)?f??F(f ), (1)Measuring confidence in a causality hypothesis propor-tionally to the number of evidenced factors might not becorrect, there are two sources of uncertainty that mightnegatively effect our confidence in the hypothesis, evenif we collect all the evidences, and should be reflectedin the way we measure confidence in the hypothesis: i)the quality of the evidences, i.e., we can surely state thatFig. 6 Two possible paths from the factor Positive regulation of TNF alpha overproduction to the factor SynovialinflammationAgibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 11 of 22the evidence is not due to errors, and ii) quality of ourmodelling of the hypothesis. The first source of uncer-tainty comes from the fact that during our experimentsor literature search for the justifications of evidences wemight face errors. And the second source of uncertaintycomes from the waywemodel our hypothesis as an assem-bly of sub-hypotheses, which relies on ontology mappingsto merge formalizations of the background knowledge ofthe hypothesis. During this process we might introduceuncertainty for the matched concepts representing factorsof the hypothesis.To this end, we introduce two functions defined on thenodes of the hypothesis graph, ? : N ?[ 0 . . . 1] thatassociates weights of the confidence in the ontology map-ping to every factor, and represents our confidence in thehypothesis modelling, and ? : N ?[ 0 . . . 1] associatesweights of the confidence in evidence for each factor.Equation 2 represents the contribution function for thehypothesis factors.F(f ) =???????????0 f ? Efactor f not evidenced?(f )?(f ) f ? Eweighted contributionif f evidenced(2)Properties of the confidence function. Confidence incausality hypothesis is defined as a sum of weighted con-tributions of factors, that participate in causality possibil-ities. The contributions of factors is a weighted, and mostimportantly a non-negative, function (Eq. 1), thus thus aswe add more evidenced factors the value of the function,can only grow. Confidence depends on the evidenced fac-tors, it has its minimum value (Cts = 0) when we have noevidences (E = ?), and it has its maximum value whenall the factors have been evidenced (argmaxCtswhen E =I(s, t)). To this end, we can normalize our confidencefunction to the maximum possible confidence value wecan obtain, when all the factors have been evidenced, suchthat the confidence is always measured in the [ 0 . . . 1]range (Eq. 3).0 = Cts (E = ?)Cts (E = I)? Cts (E ? I)Cts (E = I)<Cts (E = I)Cts (E = I)= 1. (3)ResultsWith the help of our domain experts in biology andbiomechanical engineering (multi-disciplinary consor-tium of the EU FP7 MultiScaleHuman project [55]) wehave been formalizing the background knowledge aroundfactors participating in the process of cartilage degrada-tion, which can be evidenced across different biologicalscales. This background knowledge has been captured, asa proof of concept, in an OWL 2 ontology O and has beeniteratively validated with our domain experts. This ontol-ogy has been designed to contain a significant amountof axioms which go beyond the usual taxonomical rela-tionships in the biomedical ontologies, and instead, modelcausality relationships with rich ontology concept con-struction operators including nested OWL restrictionsand property chains. During our interviews (t1, . . . , tn)with the domain experts we have been updating the back-ground knowledge formalization (Ot1 , . . . ,Otn ), eitherwith the help of our domain experts or by translatingdiscovered causality relationships from the literature our-selves. Each snapshot of the background knowledge Otihas been presented as the results of our methodology ofhypothesis graph constructionHti for validation and feed-back. To report our results we fix our attention to twospecific snapshots of the causality hypothesis, and we referto them asHsub andHbroader .Hsub has been extracted fromthe state of the ontology Oti , which corresponds to theextent of knowledge of the molecular biologist on causal-ity relationships between the biological processes whichlead to cartilage degradation with a focus on cellular andmolecular biological scales (Hsub is an equivalent hypothe-sis graph to what we presented as a normalized hypothesisgraph in the Methods section). Hbroader was extractedfrom the ontology Otj at time point tj, which correspondsto the ontology Oti updated with more knowledge aboutfactors that lead to cartilage degradation, from organ andbehavior biological scales. Table 2 summarizes Oti ,Otjwith ontology metrics and descriptions, computed withthe Protégé ontology editor.In Fig. 7 we notice that Hsub = ?Nsub,Asub? is a sub-graph of Hbroader = ?Nbroader ,Abroader?, such that Nsub ?Nbroader and Asub ? Abroader . The additional knowledge(Hbroader/Hsub) is not present in the formalization by themolecular biologist, meaning that he might not be awareabout alternative factors that concur during osteoarthri-tis and might have played a significant role in the causalityhypothesis (Fig. 7). The subsequent experiments demon-strate how our methodology supports hypothesis testingby quantifying confidence in a causality hypothesiswith incomplete evidences, and provides means tocompare confidence measures with different depths ofknowledge.Table 2 Oti ,Otj ontology metricsOntology metric Oti OtjAxioms 66 151Logical axiom count 39 92Declaration axiom count 18 34Class count 14 30Object property count 4 4Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 12 of 22Fig. 7 Bold contours show the normalized hypothesis graph known to the molecular biologist Hsub , whereas the dotted contours delineate theadditional knowledge of which the biologist is not aware HbroaderRobustness of the system in presence of complementarycausality relationshipsOur methodology is capable of adequately trackingtwo complementary biological scenarios, where onefactor might stand as a cause of two opposite effects.We tested our methodology for hypothesis graph con-struction with small increments in our knowledge whichmight lead to big changes in the shape of the causalityhypothesis, and what we can understand from it. Inparticular, at the time point ti the knowledge on thehypothesis contained causality path from Mechanicalloading factor to the Chondrocytes catabolismfactor. Indeed, the positive regulation of chondrocytescatabolism by mechanical loading has been demonstratedin the literature [56]. However, it is also known thatthe mechanical loading can also have positive effecton the chondrocytes anabolism (the oppositebiological process of catabolism), and thus facilitateproteoglycan and collagen production [57]. Based onthe complementary causality effects of mechanicalloading on the biochemical balance in cartilage, we canthus hypothesize that mechanical loading might resultin both beneficial and detrimental conditions of thejoint cartilage. This additional knowledge is reflectedin the way our methodology constructs the hypothesisgraph. In particular, the normalization patterns (intro-duced in the Methodology section) split the causalitychains starting in mechanical loading, that span twocomplementary causality possibilities of benign andmalign effect on articular joint (Fig. 7). Validly, all thepossibilities of mechanical loading leading cartilagedegradation pass through the factor positiveregulation of chondrocytes catabolismand we do not have a situation where mechanicalloading leads to cartilage degradationby passing through positive regulation ofchondrocytes anabolism. Conversely all the causalitychains which lead from mechanical loading tocollagen or proteoglycan production pass throughchondrocytes anabolism factor.Relative confidence measurementThis experiment demonstrates how molecular objectivescan measure his confidence in the causality hypothe-sis according to his knowledge on causality relationships(Hsub) and can compare it to the confidencemeasure whenwe add more knowledge Hbroader . We simulate the casewhere the molecular biologist wants to test a hypothesisAgibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 13 of 22that s = Synovial inflammation has caused t =Cartilage degradation. We treat Hbroader as acoarse approximation of our universal knowledge on allpossible causalities which lead from s to t, and Hsub as apersonal view of that universal knowledge by the molecu-lar biologist.Table 3 summarizes network statistics of the two graphs.In particular, in the universal hypothesis graph Hbroaderthere are 24 possible causal chains which lead from s tot, whereas in the subgraph Hsub we have only 6 possiblecausal chains, which means that the molecular biologistis missing a significant amount of knowledge about thecausalities that he is studying. Moreover, in the universalknowledge of causality hypothesis we have 12 (|IHbroader | =12) factors that can potentially be evidenced and wouldcontribute positively to the overall confidence of thehypothesis, whereas in the restricted knowledge case weare aware of only 9 (|IHsub | = 9) factors which need tobe evidenced to obtain the maximum confidence in thesame hypothesis that s has caused t. To study the behav-ior of the confidence function Cts in these two cases weperform the following tests: i) study the evolution of theconfidence function separately for two graphs, ii) normal-ize the confidence function with the maximum possibleconfidence for individual graphs, iii) normalize the twoconfidence functions with themaximum confidence in theuniversal graph. Note that, the parameter for the confi-dence function is the set of evidenced nodes, where eachnode may have different importance value, as defined bythe weighting functionF . To take into account all the pos-sible variability of the confidence function we computethe distributions of the confidence values for a graduallyincreasing number of evidences. That is, we start withthe case where the evidence set is empty, correspond-ing to the initial phase of hypothesis testing and whereour confidence is 0. Then, we compute the distributionof confidences for all evidence sets of size (cardinality) 1,corresponding to different choices of choosing one fac-tor to evidence. For instance, for the universal hypothesisgraph Hbroader we have 12 ways to to prove hypothesis byevidencing only one factor (out of 12 possible), whereasfor Hsub we have 9 factors to choose from. We continuecomputing confidence distributions until we reach the fullevidence set.Figure 8 represents the distribution of confidences com-puted with Cts (Eq. 1) for gradually increasing sizes ofTable 3 Statistics of the graphsStatistic Hsub HbroaderNumber of nodes |N| 15 30Number of arcs |A| 19 57Number of possible causal chains from s to t 6 24Number of possible factors to evidence |I | 9 12evidence sets, with a trivial weighting function of factorsF = const 1  where every factor has equal contributionto the causality chains. The mean values of the con-fidence distributions grow linearly as we increase thenumber of evidences, as expected, the maximum confi-dence value obtained in the universal case is bigger thanin the restricted case because we take into account morepossibilities in the universal case. We now use the individ-ual maximum mean confidence values for each graph toscale our distributions, such that they always stay in the0..1 range.Figure 9 shows the normalized version of the confidencedistributions, namely Cts = Ctsmax(Cts )for Hsub and Hbroader .In particular, it shows that a molecular biologist, relativeto his extent of knowledge, obtains the 100% confidencein his causality hypothesis by evidencing all the possiblefactors which contribute to all the possible ways in whichs might have caused t, however, with the same amountof evidence, but taking into account universal knowledgeabout the causality possibilities, his confidence is less than100%, which shows that he has missed some importantcausality possibilities. To quantify this uncertainty, whichis proportionate to the amount of missed causality pos-sibilities, we scale both confidence distributions by themaximum confidence value that we may obtain in theuniversal case.Figure 10 demonstrates the relative confidence of themolecular biologist to the universal causality hypothesisfor the same evidenced sets. The x-axis is truncated toevidence sets of size 9, since molecular biologist is onlyaware of 9 factors which need to be evidenced to provehis hypothesis. If we collect the mean values of the con-fidence distributions in two vectors x1, x2 then we canquantify the error as their Euclidean distance ?x1 ? x2?.In Table 4 we summarize the errors which quantify theuncertainty in obtained confidence measures with respectto the universal case for different weighting functions Fi.These weighting functions were chosen as follows: i) F1trivial weighting of importance of factors, ii) F2 randomweighting of importance of each factor, iii) F3 gives moreimportance to factors which molecular biologist is awareof, whereas those that he is not aware of are given lessimportance, iv) F4 opposite to F3, we give more impor-tance to factors that molecular biologist is not aware ofand we decrease the importance of factors that he isaware of. The error variation is intuitive, if we evidencethe most important factors, even if we miss other fac-tors and other causality chains, but whose importanceto the overall hypothesis is significantly smaller, then weare more confident even with a restricted knowledge ofthe causality possibilities. Vice-versa, if we evidence lessimportant factors and we miss the important ones, thenour confidence is much more compromised.Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 14 of 22Fig. 8 Confidence distributions for gradually increasing sizes of evidence sets for the two graphs Hsub ,Hbroader , with a trivial weighting functionF (f ) = 1Local importance of factorsImportance of the factors for a causality hypothesis canbe deduced from our confidence measure defined on thehypothesis graph. The factors ranked as the most importantmay help the researchers prioritize their next experi-ments, studies, andmay help in the discovery of the poten-tial collaborations with other scientists. Analogously, thefactors that are identified as the least important for a spe-cific causality hypothesis hint on the lack of knowledgeabout the possibly missing causality relationships, andmight represent an opportunity to focus on an underre-searched topic. In particular, Cts measures our confidencein the causality hypothesis that factor s caused t with agiven set of evidenced nodes E . This function accumu-lates the weighted contribution of all evidenced nodes ineach causality possibility leading from s to t. When we firststart proving our hypothesis we do not have any evidenceand we have a choice of I to evidence from. However,do we need to evidence all the factors in the interior ofthe causality hypothesis I? What if we can only obtainan incomplete set of evidences, which factors should wechoose? Intuitively, we should first focus on evidencingfactors which are most important in our causality hypoth-esis. But how can we assess the importance of each factorin the causality hypothesis? In this experiment, we pro-pose a general approach to assessing the local importanceFig. 9 Confidence distributions for gradually increasing sizes of evidence sets for the two graphs Hsub ,Hbroader , normalized by its maximum possibleconfidence valueAgibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 15 of 22Fig. 10 Confidence distributions for gradually increasing sizes of evidence sets for the two graphs Hsub ,Hbroader , normalized by the maximumpossible confidence value in the universal caseof factors, independently of the weighting function F . Todo so we start with a case where we do not have any evi-dence E = ?, we then rank each factor fi in the causalityhypothesis by its potential contribution to the confidencein the causality hypothesis if it was evidenced |Cts (E ? fi)?Cts (E = ?)|.Figure 11 depicts the variation of potential contribu-tions to the overall confidence measure Cts for each fac-tor fi. In particular, we can observe that in both cases:Hsub restricted personal view of the hypothesis, andHbroader universal causality hypothesis themost importantfactors are: Positive regulation of TNF alphaoverproduction, s =Synovial inflammation,t =Cartilage degeneration and Biochemicalimbalance. Indeed, to prove that s has resulted int our best strategy is to focus on evidencing thosetwo factors, however, given our knowledge of causal-ity relationships, we might choose to evidence alterna-tive factors to obtain the same overall confidence inthe validity of our causality hypothesis. We also observethat by extracting more knowledge on causality relation-ships more important factors to our causality hypoth-esis emerge, i.e., the factors which we did not knowabout before. For instance, Decrease of cartilageelasticity and Water content increase inTable 4 Mean squared error between the confidencedistributions for different weighting functions FWeighting function Fi ErrorF1(f ) = 1 2.17F2(f ) = random(0, 1) 2.09F3(f ) = 1 if f ? IHsub , otherwise 0.1 1.95F4(f ) = 1 if f ? IHbroader , otherwise 0.1 2.96cartilage have relatively low potential confidence con-tributions (< 0.04) and thus our unawareness of thecontribution to causality hypothesis of these factors is notso penalizing. Yet, Diminution of load bearingcapacity of cartilage is capable of contribut-ing more than 10% of the overall confidence measureCts . It is also interesting to observe that adding knowl-edge (Hbroader) reduces the importance of Biochemicalimbalance factor to the point that it is no longer one ofthe most important factors in the causality hypothesis.Generalization of the hypothesis configurationIn the previous experiment we identified the most impor-tant factors, such that evidencing them would maximizeour confidence in the causality hypothesis that s resultedin t. We can use the local importance of factors to thehypothesis configuration to target our evidence collec-tion. Suppose we managed to evidence the four mostimportant factors for the hypothesis graphHsub, which wesummarize in Table 5.For the same evidence set Esub we obtain the normal-ized confidence of Cts = 0.66 for Hsub and Cts = 0.53for Hbroader . Now, we ask ourselves a question with thesame evidence set what other causalities can we prove(with the same confidence)?. If we keep the same evidenceset Esub we are able to prove causalities with a confidence>60% as depicted in Table 6. These causalities correspondto very similar causality chains, as our initial causalityhypothesis that Synovial inflammation has resultsin Cartilage degradation.Intuitively, Table 7 demonstrates that for the same evi-dence set, as we add more knowledge (Hbroader) we areable to prove more causality relationships, with a goodconfidence (>50%).Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 16 of 22Fig. 11 Contributions of the interior factors of the hypothesis s caused t for two hypothesis graphs Hsub ,Hbroader with two different depths ofknowledgeGeneralization of the hypothesis configuration leadsto the scenarios where the seemingly wrong causalityrelationships, might actually be explained with plausibleinterpretations. One such example scenario is when weobtain the significant confidence (0.60) in a causalityhypothesis that Cartilage calcification mightresult in Positive regulation of TNF alphaoverproduction (line 1 in Table 7). First, it is tempt-ing to say that this is a wrong hypothesis, and is dueto the error in the formalization of the backgroundknowledge on causality relationships. Partly, becausecalcification of cartilage entails cell apoptosis and thusshould cause the decrease of levels of TNF alpha cytokinecells. However, we get the high confidence score in thiscausality due to the presence of a path from Cartilagecalcification to Positive regulation ofTNF alpha overproduction (see Fig. 7). This pathrepresents our knowledge that calcified cartilage willresult in degeneration of cartilage tissue, which willTable 5 4 Most important factors for Hsub in the two hypothesisgraphs and their relative confidence values in both Hsub andHbroaderEvidence set Esub Importance for Hbroader Importance for HsubBiochemical imbalance 0.10 0.16Cartilage degeneration 0.14 0.16Positive regulation of TNFalpha overproduction0.14 0.16Synovial inflammation 0.14 0.16Cts(Esub) for Hbroader Cts(Esub) for Hsub0.53 0.66provoke synovial inflammation, and we hypothesized thatsynovial inflammation will result in positive regulation ofTNF alpha. After a discussion with our domain expertswe reached the conclusion that, although this causalityrelationship between calcified cartilage and positiveregulation of TNF alpha might seem contradictory, thereactually might be a plausible explanation. Namely, whilethe calcification causes tissue death in cartilage, it does soonly in a specific region of cartilage. The calcified region,however, will induce the diminution of the load bearingproperties of the whole cartilage, and this will provokethe synovial inflammation, which, in turn, will result inexcessive levels of TNF alpha in the neighbouring regionsof the cartilage (neighbouring to the calcified region).PrototypeWe implemented a prototype (Fig. 12) to interactivelyapply and present the proposed methodology for causalityhypothesis testing on the obtained hypothesis graphs. Thedemo of the prototype is available at http://hypothtest.plumdeq.xyz/test/. Source code for the hypothesis test-ing of the prototype and proof of concept ontologies, aswell as the Jupyter Notebooks (reproducible experimentspresented in this manuscript) are available on GitHub athttps://github.com/plumdeq/hypothtest (see Availabilityof data and materials subsection).The interface of the prototype is divided into 4 logicalblocks, labeled a, b, c, d in Fig. 12.(A) Control over the hypothesis configuration. Theusers can change the hypothesis configuration in twomodes - i) identifying the boundary nodes s, t, ii) selectingthe evidenced nodes E . Each mode is triggered by clickingAgibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 17 of 22Table 6 Other causalities we can prove (>60% confidence) with the same evidence set EsubSource s Target t Cts(Esub) for Hbroader Cts(Esub) for HsubCartilage Biochemical imbalance 0.66 0.66degeneration Negative regulation of Collagen production 0.75 0.75Positive regulation of TNF alpha overproduction 1.00 1.00Loss of Positive regulation of TNF alpha overproduction 0.62 0.80collagenLoss of Positive regulation of TNF alpha overproduction 0.62 0.80proteoglycanSynovial Negative regulation of Chondrocytes anabolic activity 0.66 0.66inflammationNegative regulation of Collagen production 0.66 0.66Negative regulation of Proteoglycan production 0.66 0.66Positive regulation of Chondrocytes catabolic activity 0.66 0.66Positive regulation of TNF alpha overproduction 1.00 1.00Table 7 Causalities we can prove (>50% confidence), as we add more knowledge, and which we cannot prove with our restrictedknowledge of causality relationshipsSource s Target t Cts(Esub) for Hbroader Cts(Esub) for HsubCartilage calcification Positive regulation of TNF 0.60 0.0alpha overproductionDiminution of load bearing Biochemical imbalance 0.57 0.0capacity of cartilage Negative regulation of 0.60 0.0Chondrocytes anabolic activityNegative regulation of 0.60 0.0Collagen productionNegative regulation of 0.60 0.0Proteoglycan productionPositive regulation of Chondrocytes 0.60 0.0catabolic activityPositive regulation of TNF 0.75 0.0alpha overproductionSynovial inflammation 0.66 0.0Meniscal tear Biochemical imbalance 0.57 0.0Negative regulation of 0.60 0.0Collagen productionNegative regulation of 0.60 0.0Proteoglycan productionPositive regulation of Chondrocytes 0.60 0.0catabolic activityPositive regulation of TNF 0.75 0.0alpha overproductionWater content increase Positive regulation of TNF 0.60 0.0in cartilage alpha overproductionAgibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 18 of 22Fig. 12 The interface of the prototype is divided into 4 logical blocks: a) control over the hypothesis configuration h, b) hypothesis summary, c) localimportance of nodes in the hypothesis and d) visualization of the hypothesis graphon an associated button (see Fig. 12a), and then selectingthe specific nodes in the hypothesis graph (Fig. 12d).(B) Hypothesis summary. A textual summary of a cur-rent hypothesis configuration (see Fig. 12b).(C) Local importance of nodes in the hypothesis. Localimportance of each node with respect to the hypothesisconfiguration.(D) Visualisation of the hypothesis graph. Interactivenetwork visualisation with the force directed layout [58]of the hypothesis graph H. The users can interactivelyclick on the nodes and drag them for a visually better spa-tial distribution of the network. The boundary nodes arevisually distinguished as completely opaque nodes in thehypothesis graph (Fig. 12), while all other nodes are semi-opaque. Evidenced nodes are visually distinguished asgreen nodes. Consequently, if a node ni is both evidencedand either a source or a target of the confidence evalua-tion, then it will be opaque green. The backend (server)of the prototype constructs hypothesis graphs, computesimportance measures on each node of the graph, andevaluates confidence in the hypothesis configuration. Thefrontend (client) is responsible for the interactive visu-alisation of the hypothesis graph, and serves as a userinterface. In particular the user can interactively assign theboundary nodes, and mark nodes as evidenced. The userinput is then transmitted to the backend via custom dataexchange protocol, based on JSON files. Each time theuser changes the configuration of the hypothesis (i.e., evi-dences/unevidences node or assigns new source or targetnodes of the confidence evaluation the hypothesis confi-dence is reevaluated and the results are sent back to theclient.DiscussionWe evaluated our methodology on a hypothesis graphwhich covers our use-case scenario of cartilage degrada-tion during osteoarthritis. The obtained hypothesis graphrepresents both contributing factors which may causecartilage degradation and the factors which might becaused by the cartilage degradation. Hypothesis graphconstruction (see Robustness of the system in presenceof complementary causality relationships section) hasproven to be robust to the addition of potentially con-tradictory information on the simultaneously positive andnegative effects, by adequately separating two comple-mentary causality scenarios. By evaluating our method-ology for relative confidence measurement (see Relativeconfidence measurement section) we have observed thefollowing: i) the more evidences we are able to provideAgibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 19 of 22(as E ? I) the bigger is our overall confidence function(confidence grows Cts ?), ii) our relative confidence to theuniversal knowledge of the hypothesis (i.e., the differencein confidences) is proportionate to how much knowledgeon causal possibilities we lack with respect to the uni-versal causality hypothesis, the less causality possibilitieswe take into account in our formalization the smaller isour confidence in the causality hypothesis with respectto the universal knowledge of the causality hypothesis,iii) our confidence in the causality hypothesis increaseswhen we evidence more factors favored byF with respectto the universal formalization of the causality hypothe-sis, even if we do not have full knowledge of the causalitypossibilities. The domain experts found that our compu-tational methodology for assessing confidence in a causal-ity hypothesis proportionally to the amount of availableknowledge, corresponds to their subjective assessmentsof confidences in an investigated hypothesis. Moreover,the obtained confidence measures for the specific causal-ity hypotheses have been validated by our domain experts,and, in some cases, have led to new interpretations of thealready known causality connections (see Generalizationof the hypothesis configuration section).Limits, assumptions and dependencies of methodology.Overall our framework is dependant on the validity, qual-ity and the richness of the modelling, which will inducethe final shape and topology of the hypothesis graphand the way the confidence is assessed by using ourmethodology for confidence assessment. Of course, ourmethodology has its limits and has its assumptions anddependencies. Main assumptions and dependencies of themethodology for hypothesis testing rely on: i) ontologi-cal commitment of the input ontologies Oi that formalizebackground biological knowledge on causality relation-ships, ii) biological validity and logical consistency ofthe formalized knowledge - input to the framework, iii)weighting scheme of factors of the hypothesis that mea-sure the quality of the ontology matching of concepts usedto assemble the final ontology, and the confidence of theobtained evidence for a specific factor fi. Ontological com-mitment of the modelled realities representing causalityrelationships among the factors should follow the gooddesign patterns for modelling causalities, for both con-cepts and relationships that interrelate those concepts. Inparticular, we consider the processual perspective of a dis-ease as a causal chain structure as in River Flow Model ofDiseases [25] as opposed to an object-like perspective ofa whole constituting a disease as in Ontology of GeneralMedical Sciences (OGMS) [59]. As has been argued byRovetto and Mizgouchi [25], the causality in OGMS isunstated, implicit or stated indirectly. The general accountof disease in OGMS draws ideas from Scheuermannet al. [60], and distinguishes diseases from disease courses.Diseases in OGMS are treated as dispositions potentiallyrealizable via pathological processes, and have some dis-orders as their physical basis. In our work, we focus oncausality relationships which constitute a disease course,and reason on these relationships by relying on graphanalysis techniques. Due to this modelling choice weexpect the input ontologies to follow the RFM accountof disease as a causal chain structure. Specifically, ourmethodology for hypothesis graph construction extractscausality relationships from the assembled ontology suchthat the final hypothesis graph contains nodes as occur-rents, either biological processes, as exemplary modelledin the Gene Ontology [20], or as conditions (abnormalstates), according to the guidelines of the RFM. Thecausality relationships should be compliant with the Rela-tion Ontology [24], which, among other types, covers con-current and overlapping causality relationships betweenthe occurrent entities, relying on Allen interval algebracalculus for temporal logic [61]. Strategies toward harmo-nization between disease accounts in OGMS and RFM arebrought up in Rovetto and Mizgouchi [25]. Hypothesisgraph creation with input ontologies following the OGMSmodelling of disease could represent a promising futuredirection for the community.Weighting scheme for the factors of the hypothesisgraph will largely depend on the context (e.g., studieddisease), the quality of the ontology mappings, and theconfidence of the obtained evidence. Mappings to guidethe assembly (i.e., link factors from different hypothe-sis) can be discovered in online resources like UMLSMetathesaurus [49] or BioPortal [50, 51], or using stateof the art ontology alignment systems like LogMap [52]or AML [53]. Confidence in the obtained evidence willdepend on the methodology of the experiment and shouldbe assessed by the executioner of the experiment, whichmight entail subjective importance weight of the factorand might have subjective consequences on the computa-tion of the overall confidence in the causality hypothesiswith our framework.ConclusionsWe have presented a promising and nascent method-ology for the translation of biological knowledge oncausality relationships of biological processes and theireffects on conditions to a computational framework forshared hypothesis testing. Furthermore, we have defineda knowledge-driven, and evidenced-based way of mea-suring confidence in a causality hypothesis proportionallyto the amount of available knowledge and collected evi-dences. The methodology resumes in two points: hypoth-esis graph construction from the formalizations of thebackground knowledge on causality relationships, andconfidence measurement in a causality hypothesis as anormalized weighted path computation in the hypothesisAgibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 20 of 22graph. Lastly, we have made the source code and mate-rials available to the community on GitHub at https://github.com/plumdeq/hypothtest (see Availability of dataand materials subsection).Herein we took advantage of our domain experts tobuild a simplified and a tractable version of a causal-ity hypothesis graph of cartilage degradation during toosteoarthritis, and to validate our methodology for confi-dence assessment of causality hypothesis. The evaluationresults, the feedback from our experts, and the lessonslearnt from this overall experience allow us to concludethat a methodology for shared hypothesis testing couldbe incorporated as an invaluable asset to the online bio-logical knowledge graph mining services. In particular,our hypothesis graph construction methodology couldbe used routinely to enrich biological knowledge graphs(e.g., Knowledge Bio [62]) and online databases (e.g.,Gene Wiki [63]) by extracting the causality relation-ships information from OWL 2 ontologies. Of course,the proposed set of patterns for the normalization ofthe hypothesis graph will have to be augmented andtuned for a specific studied context. We, for instance,defined graph rewriting normalization patterns to dealwith complementary biological scenarios of simultane-ously positive and negative regulations of biological pro-cesses (see Robustness of the system in presence ofcomplementary causality relationships section). In fact,the graph rewriting patterns is a general paradigm forthe transformation of formalized knowledge on a spe-cific biological pattern into its equivalent graph rep-resentation and might open an opportunity for moreresearch and practical contributions from the biomedicalcommunity.Shared hypothesis testing services built on top ofthe confidence measurement (see Relative confidencemeasurement section), and the inference proceduresit induces (see Generalization of the hypothesis con-figuration section), will enhance the biological knowl-edge graphs with advanced simulation functionalitiesfor continuous research. These services could supportresearchers in literature review for their experimentalstudies, planning and prioritizing evidence collectionacquisition procedures, and testing their hypotheses withdifferent depths of knowledge on causal dependenciesof biological processes and their effects on the observedconditions. Measuring confidence in a causality hypothe-sis relatively to the already discovered causality relation-ships might serve in the assessment of the fairness of theobtained results, and its significance to the already knownresults. We believe that the shared hypothesis testingcould serve as an important asset for the costless re-enactment of the experiments, and might eventually con-tribute to the future, purely computational benchmarksfor the validation of the experiments.Endnote1 See https://www.bioontology.org/wiki/index.php/Bio-Portal_MappingsAbbreviationsABox: Description logics ground sentences stating where in the hierarchyindividuals belong; DL: Description logics; FMA: Foundational model ofanatomy ontology; MRI: Magnetic resonance image; OA: Osteoarthritis; OWL:Web ontology language; RDF: Resource description framework; SNA: Socialnetwork analysisAcknowledgmentsWe would like to thank the reviewers for their valuable comments andsuggestions which helped us to substantially improve the paper.FundingThis work was partially funded by the EU Marie Curie, ITN MultiScaleHuman(FP7-PEOPLE-2011-ITN, Grant agreement no.: 289897), the CNR projectDIT.AD009.006 Modelling and Analysis of anatomical shapes for computerassisted diagnosis, the BIGMED project (IKT 259055), the HealthInsight project(IKT 247784), the SIRIUS Centre for Scalable Data Access (Research Council ofNorway, project no.: 237889), the program Investigador of the PortugueseFoundation of Science and Technology (FCT, IF/00423/2012), the EU projectOptique (FP7-ICT-318338), and the EPSRC projects ED3 and DBOnto.Availability of data andmaterialsWe implemented a prototype to apply the proposed methodology forhypothesis testing on an example hypothesis graph. The demo of theprototype is available at http://hypothtest.plumdeq.xyz/test/. Source code forthe hypothesis testing of the prototype and proof of concept ontologies, aswell as the Jupyter Notebooks (reproducible experiments presented in thismanuscript) are available on GitHub at https://github.com/plumdeq/hypothtest.Authors contributionsAA defined most of the theory and did the implementation work. EJRextended the graph projection methodology to a richer subset of OWL axiomsand proposed the hypothesis graph normalization methodology. MOcomposed and validated the biological use-case scenario of cartilagedegradation causality hypothesis. All authors contributed to the definition ofthe framework, and to the writing of the manuscript. All authors read andapproved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Italian National Research Council, Via De Marini 6, 16149 Genoa, Italy.2University of Oslo, Oslo, Norway. 33Bs Research Group, Biomaterials,Biodegradables and Biomimetics, Headquarters of the European Institute ofExcellence on Tissue Engineering and Regenerative Medicine, University ofMinho, Caldas das Taipas, Portugal. 4ICVS/3Bs - PT Government AssociateLaboratory, Braga/Guimarães, Portugal. 5University of Genoa, Genoa, Italy.6Center for Medical Statistics, Informatics, and Intelligent Systems, Institute forArtificial Intelligence and Decision Support, Medical University of Vienna,Spitalgasse 23, 1090 Vienna, Austria. 7Department of Biomedical Data Science,Stanford University School of Medicine, Stanford 94305, California, USA.Received: 1 June 2016 Accepted: 18 January 2018Agibetov et al. Journal of Biomedical Semantics  (2018) 9:9 Page 21 of 22Wang et al. Journal of Biomedical Semantics  (2018) 9:19 https://doi.org/10.1186/s13326-018-0184-yRESEARCH Open AccessAdverse event detection by integratingtwitter data and VAERSJunxiang Wang1, Liang Zhao1, Yanfang Ye4,5 and Yuji Zhang2,3*AbstractBackground: Vaccine has been one of the most successful public health interventions to date. However, vaccinesare pharmaceutical products that carry risks so that many adverse events (AEs) are reported after receiving vaccines.Traditional adverse event reporting systems suffer from several crucial challenges including poor timeliness. Thismotivates increasing social media-based detection systems, which demonstrate successful capability to capturetimely and prevalent disease information. Despite these advantages, social media-based AE detection suffers fromserious challenges such as labor-intensive labeling and class imbalance of the training data.Results: To tackle both challenges from traditional reporting systems and social media, we exploit their complementarystrength and develop a combinatorial classification approach by integrating Twitter data and the Vaccine AdverseEvent Reporting System (VAERS) information aiming to identify potential AEs after influenza vaccine. Specifically, wecombine formal reports which have accurately predefined labels with social media data to reduce the cost of manuallabeling; in order to combat the class imbalance problem, a max-rule based multi-instance learning method isproposed to bias positive users. Various experiments were conducted to validate our model compared with otherbaselines. We observed that (1) multi-instance learning methods outperformed baselines when only Twitter datawere used; (2) formal reports helped improve the performance metrics of our multi-instance learning methodsconsistently while affecting the performance of other baselines negatively; (3) the effect of formal reports was moreobvious when the training size was smaller. Case studies show that our model labeled users and tweets accurately.Conclusions: We have developed a framework to detect vaccine AEs by combining formal reports with social mediadata. We demonstrate the power of formal reports on the performance improvement of AE detection when theamount of social media data was small. Various experiments and case studies show the effectiveness of our model.Keywords: Formal reports, Social media, Multi-instance learning, Vaccine adverse event detectionBackgroundVaccine has been one of the most successful public healthinterventions to date. Most vaccine-preventable diseaseshave declined in the United States by at least 9599%[1, 2]. However, vaccines are pharmaceutical productsthat carry risks. They interact with the human immunesystems and can permanently alter gene molecular struc-tures. For instance, 7538 adverse event reports werereceived between November 2009 and March 2010 inthe Netherlands with respect to two pandemic vaccines,*Correspondence: Yuzhang@som.umaryland.edu2Department of Epidemiology & Public Health, University of Maryland Schoolof Medicine, Baltimore, MD, USA3Division of Biostatistics and Bioinformatics, University of Maryland Marleneand Stewart Greenebaum Comprehensive Cancer Center, Baltimore, MD, USAFull list of author information is available at the end of the articleFocetria and Pandemrix [3]. Serious adverse reactionsmay even lead to death. For example, a woman died ofmulti-organ failure and respiratory distress, which wasthen verified to be caused by a yellow fever vaccina-tion in Spain on October 24, 2004 [4]. Aiming to builda nationwide spontaneous post-marketing safety surveil-lance mechanism, the US Centers for Disease Control andPrevention (CDC) and the Food and Drug Administration(FDA) co-sponsored the Vaccine Adverse Event Report-ing System (VAERS) since 1990, which currently containsmore than 500,000 reports in total. However, such report-ing systems bear several analytical challenges, such asunderreporting, false-causability issues, and various qual-ity of information. In addition, formal reports are recordsof symptom descriptions caused by vaccine adverse events© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Wang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 2 of 10(AEs) and need time-consuming administrative process-ing. As a result, the release of formal reports lags behinddisease trends. For example, the VARES usually releasesnewly-collected report data every three months. A real-time monitoring system to identify potential AEs aftervaccination can serve as complementary surveillance pur-pose aside from VAERS.In recent decades, information extraction from socialmedia data such as Twitter data has demonstrated suc-cessful capability to capture timely and prevalent dis-ease information. These advantages effectively address thedrawbacks of existing reporting systems such as VAERS.However, very little work has been done on the detec-tion of AEs after vaccinations using social media data.There are mainly two challenges of the detection of AEson social media. (1) The costly labeling process: in prin-ciple, it is compulsory to check message by message inorder to label user accurately. Labeling millions of usersis labor-intensive. For instance, if a user has about 100tweets each month, labeling 1,000,000 such users willneed labeling 100,000,000 tweets, which cannot be com-pleted manually. (2) The class imbalance: in practice, theproportion of positive users, whose messages indicatedsymptom descriptions of AEs, is much lower than that ofnegative users. As a result, a classifier biases toward thenegative user class due to its sample majority, causing ahigh false negative rate.To tackle both challenges, we propose to develop a com-binatorial classification approach by integrating Twitterdata and VAERS information aiming to identify Twit-ter users suffering from side effects after receiving fluvaccination. Specifically, in order to reduce the cost ofmanual labeling, we combined formal reports which areaccurately labeled with social media data to form atraining set. A max rule based multi-instance learningapproach was developed to address the class imbalanceproblem. Various experiments were conducted to vali-date our model: we first collected and processed datafrom Twitter users who received flu shots through Twit-ter APIs and AE formal reports from VAERS. Then, weapplied a series of baselines and multi-instance learn-ing methods including our model to investigate whetherformal reports can help improve the classification per-formance in the Twitter setting. We investigated how thechange of the formal report size influenced the classifica-tion performance of our multi-instance learning methodsas well as other baselines. We observed that (1) multi-instance learning methods outperformed baselines whenonly Twitter data were used because baselines need tosum multiple tweets up, most of which are irrelevant tovaccine adverse events; (2) formal reports helped improvethe performance metrics of our multi-instance learningmethods consistently while affecting the performance ofother baselines negatively; (3) the effect of formal reportswas more obvious when the training size was smaller.The reason behind the findings (2) and (3) is related tothe proportion changes of positive users against negativeusers.Related workIn this section, several research fields related to our paperare summarized as follows.AE detection in social media. Recently, social mediahave been considered as popular platforms for health-care applications because they can capture timely andrich information from ubiquitous users. Sarker et al. con-ducted a systematic overview of AE detection in socialmedia [5]. Some literatures are related to adverse drugevent detection. For example, Yates et al. collected con-sumer reviews on various social media site to iden-tify unreported adverse drug reactions [6]; Segura et al.applied a multi-linguistic text analysis engine to detectdrug AEs from Spanish posts [7]; Liu et al. combined dif-ferent classifiers based on feature selection for adversedrug events extraction [8]; OConnor et al. studied thevalue of Twitter data for pharmacovigilance by assessingthe value of 74 drugs [9]; Bian et al. analyzed the con-tent of drug users to build the Support Vector Machine(SVM) classifiers [10]. Others dwell on flu surveillance.For instance, Lee et al. built a real-time system to mon-itor flu and cancer [11]; Chen et al. proposed temporaltopic models to capture hidden states of a user based onhis tweets and aggregated states in geographical dimen-sion [12]; Polgreen et al. kept track of public concerns withregard to h1n1 or flu [13]. However, to the best of ourknowledge, there exists no work which has attempted todetect AEs on vaccines.Multi-instance learning. In the past twenty years,multi-instance learning models have attracted the atten-tion of researchers due to a wide range of applications.In the multi-instance learning problem, a data point, or abag, is composed of many instances. For example, in thevaccine AE detection problem on Twitter data, a user andtweets posted by this user are considered as a bag andinstances, respectively. Generally, multi-instance learningmodels are classified as either instance-level or bag-level.Instance-level multi-instance learning classifiers predictinstance label rather than bag label. For example, Kumaret al. conducted audio event detection task from a col-lection of audio recordings [14]. Bag-level multi-instancelearning algorithms are more common than instance-level. For instance, Dietterich et al. evaluated bindingstrength of a drug by the shape of drug molecules [15].Andrews et al. applied Support VectorMachines (SVM) toboth instance-level and bag-level formulations [16]. Zhouet al. treated instances as independently and identi-cally distributed and predicted bag labels based ongraph theories [17]. Mandel et al. utilized multi-instanceWang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 3 of 10learning approaches to label music tags using many 10-second song clips [18].MethodsIn this section, we first describe the data resources andpreprocessing processes in this work. Then we introduceour multi-instance learning method and present all stepsof the MILR, as shown in Fig. 1. All experiments wereanalyzed in compliance with Twitter policies1. They wereconducted on a 64-bit machine with Intel(R) core(TM)quad-core processor (i3-3217U CPU@ 1.80GHZ) and4.0GB memory.Feature set and datasetFeature set: The feature set consists of 234 common key-words related to AEs which were prepared by domainexperts. These keywords forming different tenses werecommon words to describe adverse events and side effectsin both formal reports and social media messages. Thechoice of keywords is very important because the ter-minology used in formal reports and tweets are differ-ent. Table 1 illustrates the terminology usage differencebetween formal reports and tweets. Keywords are high-lighted in bold types. Specifically, formal reports tendto use professional terms for symptom descriptions likeBENADRYL and hydrocortisone, while simple wordsare more likely used in social media messages. One exam-ple of flu and shot is presented in Table 1. Fortunately,there are keyword overlaps between formal reports andsocial media messages such as swollen shown in Table 1.Twitter dataset: Twitter data used in this paper wereobtained from the Twitter API in the following process:firstly, we queried the Twitter API to obtain the tweetsthat were related to flu shots by 113 keywords includingflu,h1n1 and vaccine. Totally, 11,993,211,616 tweetsbetween Jan 1, 2011 and Apr 15, 2015 in the United StatesTable 1 A formal report and tweet example, respectivelyFormal report TweetT-dap 2 days ago arm As soon as I walkdeveloped itchy and swollen. in my apartment,BENADRYL and 2.5% my swollen armhydrocortisone should be seen decides to remind meby allergist referral sent. I got a flu shot today.Keywords are shown in bold typeswere obtained. Second, among these tweets, the userswho had been received flu shots were identified by theirtweets using the LibShortText classifier that was trainedon 10,000 positive tweets and 10,000 negative tweets[19, 20]. The accuracy of the LibShortText classifier was92% by 3-fold cross-validation. The full text representa-tions were used as features for the LibShortText classi-fier. Then, we collected all tweets within 60 days afterusers had been received flu shots identified by the sec-ond step. The collected tweets formed our dataset in thispaper, which consisted of a total of 41,537 tweets from1572 users. The labels of users were manually curatedby domain experts. among them 506 were positive userswhich were indicative of AEs by their tweets and the other1066 were negative users.VAERS dataset: We downloaded all raw data fromVAERS for the year 2016 in the comma-separated value(CSV) format. The data consisted of 29 columns includ-ing VAERS ID, report date, sex, age and symptom text.We extracted 2500 observations of symptom texts, eachof which was considered as a formal report indicative ofan AE.Multi-instance logistic regressionThe scheme of the proposed framework is illustrated inFig. 1. As an auxiliary data source, formal reports arecombined with social media data to enhance the classi-fication generalization. The training dataset consists ofFig. 1 Overview of the proposed framework. VAERS: Vaccine Adverse Event Reporting System. MILR: Multi-instance Logistic RegressionWang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 4 of 10Twitter training data and formal reports from VAERS,which provide a comprehensive positive labeled datasetto tackle limited sample challenge of social media. Thescheme of the proposed framework is illustrated in FigureAs an auxiliary data source, formal reports are combinedwith Twitter data to enhance the classification general-ization. The training dataset consists of Twitter trainingdata and formal reports from VAERS, which provides anabundance of positive labeled data to reduce the cost ofmanual labeling. The test data are Twitter test data only.They are converted into vectors where each element isthe count of a keyword. Then the Multi-instance LogisticRegression (MILR) is applied to train the model. The ideaof MILR is to build a mapping from users to tweets. Therelation between users and tweets is summarized by themax rule: if at least a tweet from a user indicates an AE,this user is labeled as positive; otherwise, this user is neg-ative. The max rule for classification is asymmetric fromusers to tweets: as for positive users, we only need a tweetthat indicates an AE; but for negative users, none of theirtweets indicates an AE. In reality, a minority of users areaffected by AEs, whereas the remaining users are labeledas negative. The asymmetric property of the max rulebiases toward positive users and diminishes the influenceof the major negative user class. Therefore, the classifiertreats the positive and negative user class equally. Besides,the max rule is resistant to feature noise because tweetsselected by the max rule are determined by all candidatetweets rather than a certain tweet. In this experiment, thelogistic regression with 1 regularization is applied to trainthe classifier.Comparison methodsTwo types of classifiers which were applied to this work,namely baselines and multi-instance learning methods,are introduced in this subsection.BaselinesFor baselines, the vector was summed by column for eachuser, with each column representing a count of keywordfor this user.1. Support Vector Machines (SVM). The idea of SVMis to maximize the margin between two classes [21]. Thesolver was set to be Sequential Minimal Optimization(SMO) [22]. We chose three different kernels for compari-son: the linear kernel (linear), the polynomial kernel (poly)and the radial basis kernel (rbf ).2. Logistic Regression with 1-regularization (LR).Logistic regression is amethodwhichmodels the outcomeas a probability. We implemented this approach by theLIBLINEAR library [23].3. Neural Network (NN). The idea of the Neural Net-work is to simulate a biological brain based on manyneural units [24]. The Neural Network consists of theinput layer, 10 hidden layers and the output layer. Eachlayer has 3 nodes. The sigmoid function is used for theoutput. The layers are fully connected layers, where eachnode in one layer connects the nodes in neighboringlayers.Multi-instance learningmethods4. Multi-instance Learning based on the Vector ofLocally Aggregated Descriptors representation(miVLAD)[25]. In the multi-instance learning problem, a bag isused to represent a set consisting of many instances. Tomake the learning process efficient, all the instances foreach bag were mapped into a high-dimensional vectorby the Vector of Locally Aggregated Descriptors (VLAD)representation. In other words, VLAD representationcompressed each bag into a vector and hence improvedthe computational efficiency. Then a SVM was applied onthese vectors to train the model.5. Multi-instance Learning based on the Fisher Vec-tor representation (miFV) [25]. The miFV was similar tomiVLAD except that each bag was represented instead bya Fisher Vector (FV) representation.MetricsIn this experiment, our task was to detect flu shot AEsbased on Twitter data and VAERS information. Theevaluation was based on 5-fold cross-validation. Severalmetrics were utilized to measure classifier performance.Suppose TP, FP, TN and FN denote true positive, false pos-itive, true negative and false negative, respectively, thesemetrics are calculated as:Accuracy (ACC) = (TP+TN)/(TP+FP+TN+FN)Precision (PR) = TN/(TN+FP)Recall (RE) = TN/(TN+FN)F-score (FS) = 2*PR*RE/(PR+RE).The Receiver Operating Characteristic (ROC) curvemeasures the classification ability of a model as discrimi-nation thresholds vary. The Area Under ROC (AUC) is animportant measurement of the ROC curve.ResultsIn this section, experimental results are presented indetail. We found that (1) multi-instance learning meth-ods outperformed baselines when only Twitter data wereused; (2) formal reports improved the performance met-rics of multi-instance learning methods consistently whileaffected the performance of baselines negatively; (3) theeffect of formal reports was more obvious when the train-ing size was smaller.Performance comparison between baselines andmulti-instance learning methodsWe compared model performance between multi-instance learning methods and baselines, which is shownWang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 5 of 10in Table 2. The results demonstrated that the MILR per-formed better than any other comparison method whenno formal report was available. The MILR exceeded 0.86in the AUC, while none of other classifiers attained morethan 0.84. The ACC of the MILR was 0.8034, 0.15 higherthan the SVM with the polynomial kernel. When it cameto the FS, theMILR achieved the result that was 0.6 higherthan the SVM with the radial basis kernel. It surpassed0.78 in the PR metric, whereas the PR of the LR was only0.6765. As for the RE, the performance of the MILR was0.57 better than the SVM with the radial basis kernel.The ACCs of the miFV and miVLAD were around 0.77and their AUCs reached over 0.83, which were superiorto any other baseline. The AUCs of the NN and LR werecompetitive among baselines, reaching 0.8196 and 0.7524,respectively. As for the SVM, the kernel choice made a bigdifference. The linear kernel and the radial basis kernelwere superior to the polynomial kernel in almost everymetric: the ACCs and the AUCs of these two kernels wereover 0.65 and 0.79, respectively, whereas these of the poly-nomial kernel were only 0.6412 and 0.5697, respectively.The PR, RE and FS of the linear kernel were 0.01, 0.25 and0.36 better than the polynomial kernel, respectively.Figure 2 illustrates ROC curves for adding differentnumber of formal reports. X axis and Y axis denote FalsePositive Rate (FPR) and True Positive Rate (TPR), respec-tively. Overall, multi-instance learning methods outper-formed baselines, which was consistent with the Table 2.The MILR performed the best however many formalreports were added in the training set, with ROC curvescovering the largest area above the X axis. The miVLADalso performed well in Fig. 2a and c while inferior to theMILR in four other figures. The miFV was inferior to themiVLAD and the MILR, when the FPR was greater than0.2. When it came to baseline classifiers, the performanceof the SVM with the polynomial kernel was a randomguess in Fig. 2a, b and c. As more formal reports wereadded, its performance was improved, as shown in Fig. 2d,e and f. The NN and LR were the worst among all meth-ods when no less than 1500 formal reports were added.The SVMwith the linear kernel and the radial basis kernelachieved a competitive performance among all baselines.The reason behind the superiority of multi-instancelearning methods over baselines is that vector compres-sion by summation for each user which serve as the inputof baselines lose important information. In reality, onlya few tweets are related to vaccines, and the summationincludes many AE-irrelevant tweets, which usually resultsin a noisy data input.Performance comparison for different formal reportnumbersTo examine the effect of formal reports on classificationperformance, we made a comparison between no formalreport and 2500 formal reports. It indicated from Table 2that most multi-instance learningmethods were benefitedTable 2 Model performance between no formal report and 2500 formal report based on five metrics (the highest value for eachmetric is highlighted in bold type): multi-instance learning methods outperformed baselinesMethod Formal ACC PR RE FS AUC#ReportSVM(linear) 0 0.7793 0.7309 0.6100 0.6644 0.79162500 0.7296 0.6241 0.6370 0.6294 0.7234SVM(poly) 0 0.6412 0.7231 0.3611 0.3069 0.56972500 0.5478 0.5311 0.5497 0.4443 0.6416SVM(rbf) 0 0.6507 0.6948 0.0572 0.1035 0.80692500 0.5897 0.4652 0.9344 0.6210 0.7754LR 0 0.7665 0.6765 0.6641 0.6700 0.75242500 0.7322 0.6209 0.6576 0.6384 0.7303NN 0 0.7924 0.7408 0.6273 0.6790 0.81962500 0.7411 0.6414 0.6396 0.6394 0.7366miFV 0 0.7818 0.7269 0.6352 0.6775 0.83482500 0.7856 0.7331 0.6403 0.6833 0.8361miVLAD 0 0.7691 0.7261 0.5832 0.6461 0.83902500 0.7863 0.7055 0.6999 0.7018 0.8201MILR 0 0.8034 0.7858 0.6231 0.6947 0.86762500 0.8054 0.7871 0.6291 0.6984 0.8902Wang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 6 of 10Fig. 2 Receiver operating characteristic (ROC) curves adding different formal reports: multi-instance learning methods outperformed baselines nomatter how many formal reports were added. a No formal report, b 500 formal reports, c 1000 formal reports, d 1500 formal reports, e 2000 formalreports, f 2500 formal reportsfrom 2500 formal reports. The AUCs of the MILR and themiFV were improved by 0.025 and 0.002, respectively. ThemiVLAD was only an exception because its AUC declinedby 0.02. However, most baselines were affected nega-tively by formal reports in the AUC, while other metricsremained stable. For example, after 2500 formal reportswere added into the training set, the AUCs of the NN andthe SVM with the linear kernel were dropped drasticallyby 0.07 and 0.08, respectively. Compared with these con-siderable tumbles, the AUCs of the LR and the SVM withthe radial basis kernel dropped slightly, which was about0.02, whereas the AUC of the SVM with the polynomialkernel increased by 0.07.Figure 3 shows tendencies of five metrics on differ-ent number of formal reports. Overall, formal reportsimproved the performance of multi-instance learningmethods whereas leading to decline of baselines. Allmethods were categorized as three classes. The perfor-mance of the SVM with the linear kernel, LR and NNwas deteriorated by adding more formal reports: theirAUCs dropped from 0.79, 0.75 and 0.82 to 0.73, 0.73 and0.75, respectively. Trends of their ACCs, PRs and FSeswere similar while their REs improved significantly withmore formal reports. The SVM with the radial basis ker-nel and miFV were independent of the change of formalreports. The remaining classifiers, namely, the SVM withFig. 3Metric trends of all classifiers adding different formal reports: formal reports improved the performance metrics of multi-instance learningmethods consistently while affected the performance of baselines negatively. a SVM(linear), b SVM(poly), c SVM(rbf), d LR, e NN, fmiFV, gmiVLAD,hMILRWang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 7 of 10the polynomial kernel, miFVLAD and the MILR, bene-fited from the introduction of formal reports: the AUC ofthe SVM with the polynomial kernel was below 0.6 whilethis result increased to 0.65 with 1500 formal reports; theRE of the miVLAD first elevated from 0.58 to 0.75, thendeclined smoothly to 0.7; there was a slight increase from0.87 to 0.89 in the AUC of the MILR.The huge performance discrepancy between baselinesand multi-instance learning methods after the inclusionof formal reports came from the proportion of positiveusers against negative users. For instance, for baselines,the proportion of positive users was 32% (i.e., 506/1572)in the Twitter data only. However, the ratio increased dra-matically to 73.82% (i.e., 3006/4072) after we added 2500formal reports. In other words, since formal reports (i.e.,positive users) were introduced into the dataset, the pro-portion of positive users surpassed that of negative users,and baselines predicted most users as positive. However,negative users greatly outnumber positive users in ourdataset. Different from baselines, multi-instance learningmethods focused on the mappings from tweet labels touser labels. Since tweet labels were unavailable, assum-ing the predictions of the MILR were accurate, the pro-portion of tweets related to positive users was 4% (i.e.,1545/39037), while this ratio changed slightly to 9.73%(i.e., 4045/41537) after we added 2500 formal reports.Therefore, the introduction of formal reports benefitedmulti-instance learning methods by providing enoughpositive user samples and avoiding the label proportionchange problem.MILR performance with small training sizesTable 3 shows the effect of the size of the Twitter train-ing data on model performance using MILR. Overall,formal reports have a more obvious effect on model per-formance when the training size of the Twitter data wassmall.When the training size was 314, 786, 1048 and 1179,the corresponding AUC improvement by adding formalTable 3 Model performance using MILR with smaller training sizes (the highest value for each metric is highlighted in bold type): theeffect of formal reports was more obvious when the training size was smallerTwitter data Formal ACC PR RE FS AUC#Training #Report314 (20%) 0 0.7731 0.7278 0.5923 0.6525 0.8446500 0.7812 0.7323 0.6212 0.6713 0.85391000 0.8112 0.7993 0.6356 0.7076 0.88881500 0.8136 0.7935 0.6524 0.7151 0.89232000 0.8114 0.7812 0.6612 0.7156 0.89162500 0.8112 0.7824 0.6590 0.7147 0.8904786 (50%) 0 0.7939 0.7689 0.6141 0.6816 0.8646500 0.7920 0.7651 0.6125 0.6790 0.86841000 0.8041 0.7682 0.6567 0.7064 0.88341500 0.8034 0.7720 0.6482 0.7031 0.88342000 0.8092 0.7968 0.6312 0.7044 0.88972500 0.8066 0.7711 0.6615 0.7108 0.88661048 (67%) 0 0.7952 0.7841 0.5953 0.6767 0.8646500 0.7850 0.7615 0.5915 0.6645 0.86531000 0.7983 0.7948 0.5937 0.6795 0.88431500 0.7996 0.7944 0.5992 0.6830 0.88802000 0.8034 0.7984 0.6080 0.6903 0.88992500 0.8060 0.8016 0.6133 0.6949 0.89101179 (75%) 0 0.7952 0.7845 0.5927 0.6752 0.8664500 0.7933 0.7695 0.6010 0.6743 0.88461000 0.8034 0.7881 0.6172 0.6915 0.89481500 0.8041 0.7913 0.6154 0.6915 0.89632000 0.8041 0.7940 0.6119 0.6901 0.89832500 0.8041 0.7940 0.6119 0.6901 0.8985Wang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 8 of 10reports was 0.0477, 0.0251, 0.0264 and 0.015, respectively.The same trend was applied to the PR, RE and FS. Forexample, the FS improvement with 314 training sampleswas 0.0622, while that with 1179 training samples was only0.0149. Different from other metrics, the ACCwas around0.8 no matter how the size of the Twitter training dataand formal reports changed. The label proportion changesmentioned in the previous section can account for whythe effect of formal reports is more obvious with smallerTwitter training data.Keyword frequenciesIn this section, to illustrate the effect of formal reports onthe keyword set, we compare the semantic patterns of AEtweets between no formal report and 2500 formal reportsimplemented by MILR, as shown by Fig. 4. In each wordcloud, the frequencies of keywords in each set of tweetswere in proportion to their sizes. Keywords headache,sore, sick, arm and pain were the largest keywordsin Fig. 4a and b. The keyword cheeks became more fre-quent while the keyword vaccines was much smaller afteradding 2500 formal reports. To conclude, most frequentkeywords remained stable after the introduction of 2500formal reports.Case studiesWe found that most users were accurately labeled by ourproposed approach. For example, Table 4 gives two exam-ple users and their corresponding tweets. Keywords aredisplayed in bold types. For the first user labeled as pos-itive, the first tweet showed that he/she received a flushot. Then a headache happened indicated by the sec-ond tweet. The third tweet was irrelevant to AEs. When itcame to the second positive user, none of three tweets wasAE-irrelevant. Our approach correctly labeled both usersand selected the tweet accurately by the max rule. There-fore, the effectiveness of our model was validated by thesetwo users.DiscussionsTraditional AE reporting systems bear several analyticchallenges, which lead to the rise of information extrac-tion from social media. However, the costly labeling pro-cess and class imbalance problem put barriers to theapplication of social media on the AE detection. To tacklethese challenges, we developed a combinatorial classifica-tion approach to identify AEs by integrating Twitter dataand VAERS information. Note that the difference of datacollection timeframe between Twitter data and VAERSdata was not considered in our approach. Our findingsindicated that multi-instance learning methods benefitedfrom the introduction of formal reports and outperformedbaselines. In addition, the performance improvement ofmulti-instance on the formal reports was more obviouswith smaller training sizes. The integration of social mediadata and formal reports is a promising approach to iden-tify AEs in the near future.ConclusionIn this paper, we propose a combinatorial classificationapproach by integrating Twitter data and VAERS informa-tion to identify potential AEs after influenza vaccines. Ourresults indicated that (1) multi-instance learning meth-ods outperformed baselines when only Twitter data wereused; (2) formal reports improved the performance met-rics of our multi-instance learning methods consistentlywhile affected the performance of other baselines neg-atively; (3) the effect of formal report was more obvi-ous when the training size was smaller. To the best ofour knowledge, this is the first time that formal reportsare integrated into social media data to detect AEs.Formal reports provide abundant positive user samplesand improve classification performance of multi-instancelearning methods.In this work, we omitted the differences between socialmedia and formal reports, which introduced may extrabias to the dataset. In the future, a domain adaptationFig. 4 Keyword frequencies of tweets which indicated AEs between no formal report and 2500 formal reports: frequent keywords remained stable.a No formal report, b 2500 formal reportsWang et al. Journal of Biomedical Semantics  (2018) 9:19 Page 9 of 10Table 4 Two users and their corresponding tweetsUser Id Corresponding tweets Indicative or not246090881 Got my annual employer-paid flu shottoday.NotNow I have a headache. ARGH. IndicativeStarting to yawn. Might be sleepy. GOOD! Ineed sleep!Not206180021 Getting a flu shot, I realized how amazingthe CDC is even though most people arecompletely unaware of all theways they helpus.NotOr Gamera! Gamera flies through the air likea spinning firework. Anyone who hatesGamera is dead to me.NotPersonally, I dont like something about thesound of The Tower Heist movie. Yup,something about that makes me nervous.NotKeywords are displayed in bold typesmethod can be considered to address this issue. We alsoneed to deal with other limitations of social media. Forexample, it is difficult to differentiate a new AE from pre-vious AEs for the same Twitter user. Moreover, identifyingserious AEs is very challenging because scarce seriousAE cases lead to severe class imbalance problem, i.e.,the proportion of serious AEs is far lower than that ofgeneral AEs.Endnote1 https://dev.twitter.com/overview/terms/agreement-and-policyFundingThis project was supported by the National Cancer Institute grant P30 CA134274 to the University of Maryland Baltimore.Availability of data andmaterialsThe experimental data and source codes are accessible.Authors contributionsJW led the experimental design and analysis and drafted the manuscript. LZand YZ participated the design, provided support and manuscript editing. YYconducted the data acquisition. All authors read and approved the finalmanuscript.Ethics approval and consent to participateNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Department of Information Science and Technology, George MasonUniversity, Fairfax, VA, USA. 2Department of Epidemiology & Public Health,University of Maryland School of Medicine, Baltimore, MD, USA. 3Division ofBiostatistics and Bioinformatics, University of Maryland Marlene and StewartGreenebaum Comprehensive Cancer Center, Baltimore, MD, USA. 4LaneDepartment of Computer Science and Electrical Engineering, West VirginiaUniversity, Morgantown, WV, USA. 5Benjamin M. Statler College of Engineeringand Mineral Resources, West Virginia University, Morgantown, WV, USA.Received: 2 February 2018 Accepted: 10 May 2018RESEARCH Open AccessUsing predicate and provenanceinformation from a knowledge graph fordrug efficacy screeningWytze J. Vlietstra1* , Rein Vos1,2, Anneke M. Sijbers3, Erik M. van Mulligen1 and Jan A. Kors1AbstractBackground: Biomedical knowledge graphs have become important tools to computationally analyse thecomprehensive body of biomedical knowledge. They represent knowledge as subject-predicate-object triples, inwhich the predicate indicates the relationship between subject and object. A triple can also contain provenanceKolyvakis et al. Journal of Biomedical Semantics  (2018) 9:21 https://doi.org/10.1186/s13326-018-0187-8RESEARCH Open AccessBiomedical ontology alignment: anapproach based on representation learningProdromos Kolyvakis1* , Alexandros Kalousis2, Barry Smith3 and Dimitris Kiritsis1AbstractBackground: While representation learning techniques have shown great promise in application to a number ofdifferent NLP tasks, they have had little impact on the problem of ontology matching. Unlike past work that hasfocused on feature engineering, we present a novel representation learning approach that is tailored to the ontologymatching task. Our approach is based on embedding ontological terms in a high-dimensional Euclidean space. Thisembedding is derived on the basis of a novel phrase retrofitting strategy through which semantic similarityinformation becomes inscribed onto fields of pre-trained word vectors. The resulting framework also incorporates anovel outlier detection mechanism based on a denoising autoencoder that is shown to improve performance.Results: An ontology matching system derived using the proposed framework achieved an F-score of 94% on analignment scenario involving the Adult Mouse Anatomical Dictionary and the Foundational Model of Anatomyontology (FMA) as targets. This compares favorably with the best performing systems on the Ontology AlignmentEvaluation Initiative anatomy challenge. We performed additional experiments on aligning FMA to NCI Thesaurus andto SNOMED CT based on a reference alignment extracted from the UMLS Metathesaurus. Our system obtained overallF-scores of 93.2% and 89.2% for these experiments, thus achieving state-of-the-art results.Conclusions: Our proposed representation learning approach leverages terminological embeddings to capturesemantic similarity. Our results provide evidence that the approach produces embeddings that are especially welltailored to the ontology matching task, demonstrating a novel pathway for the problem.Keywords: Ontology matching, Semantic similarity, Sentence embeddings, Word embeddings, Denoisingautoencoder, Outlier detectionBackgroundOntologies seek to alleviate the Tower of Babel effectby providing standardized specifications of the intendedmeanings of the terms used in given domains. Formally, anontology is a representational artifact, comprising a tax-onomy as proper part, whose representations are intendedto designate some combinations of universals, definedclasses and certain relations between them [1]. Ideally,in order to achieve a unique specification for each term,ontologies would be built in such a way as to be non-overlapping in their content. In many cases, however,domains have been represented by multiple ontologiesand there thus arises the task of ontology matching, which*Correspondence: prodromos.kolyvakis@epfl.ch1École Polytechnique Fédérale de Lausanne (EPFL), Route Cantonale, 1015Lausanne, SwitzerlandFull list of author information is available at the end of the articleconsists in identifying correspondences among entities(types, classes, relations) across ontologies with overlap-ping content.Different ontological representations draw on the dif-ferent sets of natural language terms used by differentgroups of human experts [2]. In this way, different andsometimes incommensurable terminologies are used todescribe the same entities in reality. This issue, known asthe human idiosyncrasy problem [1], constitutes the mainchallenge to discovering equivalence relations betweenterms in different ontologies.Ontological terms are typically common nouns or nounphrases. According to whether they do or do not includeprepositional clauses [3], the latter may be either com-posite (for example Neck of femur) or simple (for exampleFirst tarsometatarsal joint or just Joint). Such grammati-cal complexity of ontology terms needs to be taken intoaccount in identifying semantic similarity. But account© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Kolyvakis et al. Journal of Biomedical Semantics  (2018) 9:21 Page 2 of 20must be taken also of the ontologys axioms and defini-tions, and also of the position of the terms in the ontol-ogy graph formed when we view these terms as linkedtogether through the is_a (subtype), part_of and otherrelations used by the ontology.The primary challenge to identification of semantic sim-ilarity lies in the difficulty we face in distinguishing truecases of similarity from cases where terms are merelydescriptively associated1. As a concrete example, theword harness is descriptively associated with the wordhorse because a harness is often used on horses [4].Yet the two expressions are not semantically similar. Thesorts of large ontologies that are the typical targets ofsemantic similarity identification contain a huge numberof such descriptively associated term pairs. This difficultyin distinguishing similarity from descriptive association isa well-studied problem in both cognitive science [5] andNLP [6].Traditionally, feature engineering has been the predom-inant way to approach the ontology matching problem[7]. In machine learning, a feature is an individual mea-surable property of a phenomenon in the domain beingobserved [8]. Here we are interested in features of terms,for instance the number of incoming edges when a termis represented as the vertex of an ontology graph; or atermss tf-idf value  which is a statistical measure ofthe frequency of a terms use in a corpus [9]. Featureengineering consists in crafting features of the data thatcan be used by machine learning algorithms in order toachieve specific tasks. Unfortunately determining whichhand-crafted features will be valuable for a given task canbe highly time consuming. To make matters worse, asCheatham and Hitzler have recently shown, the perfor-mance of ontology matching based on such engineeredfeatures varies greatly with the domain described by theontologies [10].As a complement to feature engineering, attempts havebeen made to develop machine-learning strategies forontology matching based on binary classification [11].This means a classifier is trained on a set of align-ments between ontologies in which correct and incor-rect mappings are identified with the goal of usingthe trained classifier to predict whether an assertionof semantic equivalence between two terms is or isnot true. In general, the number of true alignmentsbetween two ontologies is several orders of magni-tude smaller than the number of all possible mappings,and this introduces a serious class imbalance prob-lem [12]. This abundance of negative examples hindersthe learning process, as most data mining algorithmsassume balanced data sets and so the classifier runs therisk of degenerating into a series of predictions to theeffect that every alignment comes to be marked as amisalignment.Both standard approaches thus fail: feature engineer-ing because of the failure of generalization of the engi-neered features, and supervised learning because of theclass imbalance problem. Our proposal is to addressthese limitations through the exploitation of unsuper-vised learning approaches for ontology matching drawingon the recent rise of distributed neural representations(DNRs), in which for example words and sentences areembedded in a high-dimensional Euclidean space [1317]in order to provide a means of capturing lexical andsentence meaning in an unsupervised manner. The waythis works is that the machine learns a mapping fromwords to high-dimensional vectors which take accountof the contexts in which words appear in a pluralityof corpora. Vectors of words that appear in the samesorts of context will then be closer together when mea-sured by a similarity function. That the approach canwork without supervision stems from the fact that mean-ing capture is merely a positive externality of contextidentification, a task that is unrelated to the meaningdiscovery task.Traditionally, corpus driven approaches were based onthe distributional hypothesis, i.e. the assumption thatsemantically similar or related words appear in simi-lar contexts [18]. This meant that they tended to learnembeddings that capture both similarity (horse, stallion)and relatedness (horse, harness) reasonably well, but dovery well on neither [6, 19]. In an effort to correctfor these biases a number of pre-trained word vectorrefining techniques were introduced [6, 20, 21]. Thesetechniques are however restricted to retrofitting singlewords and do not easily generalize to the sorts of nom-inal phrases that appear in ontologies. Wieting et al.[22, 23] make one step towards addressing the task oftailoring phrase vectors to the achievement of high per-formance on the semantic similarity task by focusing onthe task of paraphrase detection. A paraphrase is a restate-ment of a given phrase that use different words whilepreserving meaning. Leveraging what are called univer-sal compositional phrase vectors [24] for the purposesof paraphrase detection provides training data for thetask of semantic similarity detection which extends theapproach from single words to phrases. Unfortunately,the result still fails as regards the problem of distin-guishing semantic similarity and descriptive associationon rare phrases [22]  constantly appearing on ontolo-gies  which thus again harms performance in ontologymatching tasks.In this work, we tackle the aforementioned challengesand introduce a new framework for representation learn-ing based ontology matching. Our ontology matchingalgorithm is structured as follows: To represent the nounsand noun-phrases in an ontology, we exploit the con-text information that accompanies the correspondingKolyvakis et al. Journal of Biomedical Semantics  (2018) 9:21 Page 3 of 20expressions when they are used both inside and out-side the ontology. More specifically, we create vectorsfor ontology terms on the basis of information extractednot only from natural language corpora but also fromterminological and lexical resources and we join thiswith information captured both explicitly and implicitlyfrom the ontologies themselves. Thus we capture con-texts in which words are used in definitions and in state-ments of synonym relations. We also draw inferencesfrom the ontological resources themselves, for exam-ple to derive statements of descriptive association  theabsence of a synonymous statement between two termswith closely similar vectors is taken to imply that asa statement of descriptive association obtains betweenthem. We then cast the problem of ontology matchingas an instance of the Stable Marriage problem [25] dis-covering in that way terminological mappings in whichthere is no pair of terms that would rather be matchedto each other than their current matched terms. InREVIEW Open AccessThe eXtensible ontology development(XOD) principles and tool implementationto support ontology interoperabilityYongqun He1* , Zuoshuang Xiang1, Jie Zheng2, Yu Lin3, James A. Overton4 and Edison Ong5AbstractOntologies are critical to data/metadata and knowledge standardization, sharing, and analysis. With hundreds ofbiological and biomedical ontologies developed, it has become critical to ensure ontology interoperability and theusage of interoperable ontologies for standardized data representation and integration. The suite of web-basedOntoanimal tools (e.g., Ontofox, Ontorat, and Ontobee) support different aspects of extensible ontology development. Bysummarizing the common features of Ontoanimal and other similar tools, we identified and proposed an eXtensibleOntology Development (XOD) strategy and its associated four principles. These XOD principles reuse existing terms andsemantic relations from reliable ontologies, develop and apply well-established ontology design patterns (ODPs), andinvolve community efforts to support new ontology development, promoting standardized and interoperable data andknowledge representation and integration. The adoption of the XOD strategy, together with robust XOD tooldevelopment, will greatly support ontology interoperability and robust ontology applications to support data to beFindable, Accessible, Interoperable and Reusable (i.e., FAIR).Keywords: Ontology, Interoperability, eXtensible ontology development, Software, Ontoanimal tools, Ontofox, Ontobee,Ontorat, Semantic alignment, And ontology design patternBackgroundIn informatics, an ontology is a set of computer- andhuman-interpretable terms and relations that represententities and their relations in a specific domain of theworld. Hundreds of biological/ biomedical ontologieshave been developed in the last two decades. The OpenBiological and Biomedical Ontologies (OBO) Foundry isa collaborative initiative aimed at establishing a set ofontology development principles and incorporatingontologies following these principles in an evolving non-redundant and interoperable suite [1]. The OBO librarycurrently includes over 160 ontologies covering >3 mil-lion terms in biological and clinical domains. NCBOBioPortal [2] has over 400 ontologies including bothOBO and non-OBO ontologies. Given hundreds ofontologies developed, a critical issue is the lack of ontol-ogy interoperability, preventing the seamless under-standing and exchange of semantic information betweendifferent resources.Ontologies are widely used in different areas [3, 4],including: (1) Naming things; (2) Knowledge base con-struction, e.g., the Ontology of Vaccine Adverse Events(OVAE) representing the knowledge of adverse eventsinduced by FDA-licensed vaccines [5]; (3) Data ex-change, e.g., BioPAX for representing molecular and cel-lular pathways and facilitating the exchange of biologicalpathway data [6]; (4) Data integration, e.g., the Ontology forBiomedical Investigations (OBI) [7] for integrativerepresentations of data in various areas of life-science andclinical investigations; (5) Data analysis, as exemplified bythe wide usage of the Gene Ontology (GO) [8] to supporthigh-throughput gene expression data analyses; (6) Naturallanguage processing [9, 10]; (7) Metadata standard gener-ation [1113]. (8) Information retrieval and new knowledgediscovery [1416].* Correspondence: yongqunh@med.umich.edu1Unit for Laboratory Animal Medicine, Department of Microbiology andImmunology, Center for Computational Medicine and Bioinformatics,University of Michigan Medical School, Ann Arbor, MI, USAFull list of author information is available at the end of the article© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.He et al. Journal of Biomedical Semantics  (2018) 9:3 DOI 10.1186/s13326-017-0169-2To support various needs in ontology development andapplications, different software programs have been devel-oped. The Protégé OWL editor [17] is likely the mostpopular tool for manual processing and editing of ontologyOWL documents. However, manual ontology developmentis typically tedious and inefficient, especially when thestructure of ontology is enormous. Over the years, we havedeveloped a collection of web-based Ontoanimal toolsincluding Ontofox [18], Ontodog [19], Ontorat [20], Onto-bee [21], Ontobeep [22], Ontobull [23], Ontokiwi [24], andOntobat [20]. Each Ontoanimal tool has its specific func-tions, and the collective use of these tools enables users tocover the full development of ontology and linkeddata (i.e., data published on the Web that it is expli-citly defined, machine-readable, and interlinked withexternal data sets [25]), including: extracting ontologysubsets for term reuse and semantic alignment, pro-viding ontology community views, adding and editingmultiple ontology terms, visualizing and comparingontology terms, supporting community editing anddiscussion, and creating ontology-based linked data.The back-end He group RDF triple store serves asthe default ontology RDF triple store for the OBOFoundry ontologies [21]. Complementary to Protégé,Ontoanimal tools are widely used for efficient andflexible ontology development without requiring pro-gramming skills. For example, according to GoogleAnalytics and Google Scholar, Ontobee has been usedby over 77,000 users from 181 countries, Ontofox hasbeen used by over 17,000 users from 147 countries,and Ontoanimal tools have been cited by >400 publi-cations in the last 5 years.The Ontoanimal tools and other similar tools havesignificantly enhanced the speed and quality of ontologydevelopment and improved ontology interoperability.Given an increasing number of these tools, it would beimportant to identify the common features of these tools.After retrospective examination and careful summary ofthese tools, we realized the most common feature of thesetools being their support for extensible ontology develop-ment. Such extensibility is crucial to increase the interoper-ability among the ever increasing number of ontologies.However, a systematic view of extensible ontology devel-opment is not available. Thus, we propose the eXtensibleOntology Development (XOD) strategy and four XODprinciples in this paper. Such an XOD strategy is comple-mentary to the OBO principles and the OBO goal ofachieving interoperable ontology suite [1], and it is alsocomplementary to the ten simple rules proposed for bio-medical ontology development [26]. We also believe thatthe adoption of the XOD strategy and principles supportthe FAIR Guiding Principles proposal that all research datashould be Findable, Accessible, Interoperable and Reusable(FAIR) for both machine and human users [27].XOD: eXtensible ontology developmentIn information technology, extensible describes some-thing (e.g., a program or protocol) that is designed sothat users/developers can expand or add to its capabil-ities with no or minimal change in the systems internalstructure and data flow. For example, extensibility is aprimary feature of the eXtensible Markup Language(XML) system. Being eXtensible, XOD contains fourkey principles that are extensible at different levels ofontology development (Fig. 1):(i) Ontology term reuse. Instead of reinventing thewheel when generating new ontologies, XODemphasizes the reuse of terms from existing reliableontologies that are well constructed and commonlyused by the ontology community [1, 28, 29].(ii) Ontology semantic alignment. For ontologyinteroperability, it is important to align importedterms from existing ontologies and newly addedterms with the same semantics.(iii) ODP usage for new term generation and existingterm editing. Instead of adding one term at a time,XOD emphasizes the addition or editing of a groupof terms based on ontology design patterns (ODPs).(iv) Community extensibility. While the development ofan ontology might be initiated by a small group withone or a few use cases, the ontology should beco-developed and applied to more use cases by morepeople in a broader community.Ontoanimal tools (Fig. 1) and many other programssupport XOD principles (Table 1). In the followingsections, different principles and associated tools aredescribed with details.XOD 1: Ontology term reuseReusing terms from reliable reference ontologies isbetter than reinventing the wheel to generate new termsin an ontology [18, 30]. The reference ontology shouldbe registered in an ontology library (e.g., OBO Foundry)to make these terms more findable, accessible, andreusable. To improve reusability and extensibility, ontol-ogy terms in the reference ontologies should be expres-sive and generalizable and endure consistency checkingand evaluation. To maintain ontology interoperability,ontology term mapping is often used to map terms fromdifferent ontologies with the same meaning [31]. Com-pared to ontology term reuse, term mapping is less idealsince it is time-consuming, often inaccurate, redundant,and increases maintenance cost and confusion. Givenmultiple ontologies without using the ontology termreuse strategy, ontology mapping becomes a core taskfor ontology interoperability [32]. The wide usage of theHe et al. Journal of Biomedical Semantics  (2018) 9:3 Page 2 of 10term reuse principle would make the mapping amongdifferent ontologies unneeded.An initial method of ontology term reuse was to import afull ontology, which was not ideal since it might import toomany unrelated terms. Instead of importing external ontol-ogies as a whole, the Minimum Information to Referencean External Ontology Term (MIREOT) strategy, introducedby OBI developers [30], proposes the usage of the minimalinformation of an external ontology term that is of directinterest to a target ontology [30]. Specifically, MIREOT sug-gests the following minimal set: (1) source ontology URI;(2) source term URI; and (3) target direct superclass URI.With the set of information, the source ontology term canbe extracted to under the target direct superclass. Since it isoften hard to maintain semantic consistency among ontol-ogies, the popular MIREOT strategy provides a simple solu-tion with possible semantics loss.Ontofox, Ontodog, and Ontobull support term reuse.Originally named OntoFox, Ontofox was the first webtool to support the MIREOT strategy (Fig. 2) [18].Ontofox is able to quickly and easily fetch user-specifiedterms and their annotations from source ontologies andassign them under defined superclass(es) in target ontol-ogies (Fig. 2a and b). Ontofox also extends MIEROT byretrieving semantical axioms with different options (seenext section). Ontodog is also able to extract a subset ofontology terms and axioms [19]. Unlike plain text defin-ition in Ontofox, Ontodog uses Excel input files to iden-tify terms to retrieve. To match possible updates ofsource ontologies, e.g., the upper-level Basic FormalOntology (BFO) [33], Ontobull is developed for auto-matic conversion and updating [23].Several other tools also support ontology term reuse(Table 1). The Protégé MIREOT plugin [34] andTable 1 XOD principles and supporting software programsXOD principle # XOD principle names Tool nameXOD 1 Ontology term reuse Ontodog, Ontofox, OntoMATON, Protégé MIREOT plugin, ROBOTXOD 2 Semantic alignment Ontobeep, Ontofox, ROBOTXOD 3 ODP usage MappingMaster, Ontorat, Populous, ROBOT, TermGenie, WebulousXOD 4 Community extensibility Ontodog, Ontokiwi/Ontobedia, WebProtegeFig. 1 Summary of Ontoanimal tools and their features. Ontofox supports ontology reuse by extracting terms and axioms. Ontodog providesontology community views by allowing community-preferred annotations. Ontorat automatically generates new ontology terms and edits existingterms based on ontology design patterns. Ontobee is an ontology linked data server for OBO library ontologies and many non-OBO ontologies. TheOntobee-based Ontobeep program supports ontology comparison and identification of redundant terms. Ontokiwi is a Wiki-like ontology editing anddiscussion program. Ontobedia is an application of Ontokiwi. Ontobat supports ontology-based data processing (e.g., conversion from Excel to OWL)and analysis. These tools support different XOD principlesHe et al. Journal of Biomedical Semantics  (2018) 9:3 Page 3 of 10OntoMaton [35] support term reuse as a plugin of theProtégé OWL editor or Google Spreadsheets, respect-ively. ROBOT is a command-line Java tool supportingthe extraction of ontology terms and subsets [36].ROBOT also has many other features and supports mul-tiple XOD principles (Table 1) as described below [36].To better support ontology reuse and community-based ontology development, it would also be valuableto have the authors of the source ontology know of thereuse of a term in their ontology. The Ontobee program[21] includes a feature in the web page of an ontologyterm that shows all the other ontologies reusing theterm, which supports ontology interoperability.XOD 2: Ontology semantic alignmentThe XOD 2 principle proposes to align imported ontol-ogy terms and newly added terms with the same orcompatible semantics. Such semantic alignment has twospecific meanings. First, in addition to the term reuse inXOD 1, the semantic relations among reused termsshould also be reused and aligned. If different relationtypes (i.e., object properties) mean the same thing, theyshould be merged. Correspondingly, the axioms ofreused terms and any additional terms specified in theaxioms should also be retrieved and imported. Second,the semantics related to newly developed terms shouldbe aligned and compatible with imported semantics, andthe same or compatible relations be used in the newontology. If a well-defined relation already exists, weshould reuse the relation instead of defining anotherrelation with the same meaning. Such semantic align-ments support ontology semantic interoperability.Ontofox and Ontodog support ontology semanticalignment. Ontofox and Ontodog extract semantic ax-ioms and terms related to user-specified terms from thesource ontologies. Given different options, Ontofox al-lows the computation and extraction of (i) intermediateterms that are the shared parent terms of multiple lowlevel terms (Fig. 2c), or (ii) all intermediate terms be-tween the required terms and a top level term (Fig. 2d).These subset semantic axioms and terms can then beretrieved and become a part of the new ontology. Notethat manual intervention and judgment may still beneeded now to ensure the semantic alignment betweenretrieved subset and target ontology semantics [37]. Itwill also be important to have computer-supportedFig. 2 Ontofox retrieval of an NCBITaxon subset. Input data includes 3 species of organisms (human, mouse, and rat) and Ontofox settings. The inputdata and settings can be entered via web-based forms (a). The Ontofox results can be shown using Protégé (b-d). Different results may appear basedon different settings: The setting IncludeNoIntermediates implements MIREOT (b). The setting includeComputedIntermediates extracts computedintermediates which that are closest ancestors of more than one low level source terms (c). The setting includeAllIntermediates outputs allpossible intermediatesHe et al. Journal of Biomedical Semantics  (2018) 9:3 Page 4 of 10semantic capture and synchronization of ontology evolu-tion and updates. To foster reliability, an overall formalevaluation and consistency checking would be needed.XOD 3: ODP-based ontology developmentAn Ontology Design Pattern (ODP) represents areusable solution to solve a recurrent modeling problemin the context of ontology engineering [20, 38, 39].ODPs provide extensible representations of entities andrelations, make ontologies more maintainable, and im-prove ontology quality. This XOD 3 principle requiresan ODP-based strategy to develop and edit new terms,annotations, and relations. This principle extends XOD1 and XOD 2 and provides a specific, feasible, androbust mechanism to achieve interoperable ontologyterm generation/annotation and semantic consistency.The Ontorat program (http://ontorat.hegroup.org)supports ODP-based creation of new ontology terms,annotations and logical axioms [20]. Fig. 3 illustratesan example of using Ontorat to add new terms, anno-tations, and axioms to the Ontology of AdverseEvents (OAE) [40]. Ontorat uses reusable ODPs(Fig. 4a) to automatically generate and edit ontologyterms and axioms and provides term annotations. Aspecific ODP can be used to derive an Excel templateof different terms/annotations and a set of rules thatdefine the relations among those terms/annotations(Fig. 3b). The Ontorat template, similar to a QTT(Quick Term Template) originated by OBI developers[41], can be populated with specific terms or annota-tions to define or annotate specific ontology terms, orgenerate axioms illustrating logic relations betweenontology terms. With the support of the Ontoratsettings (Fig. 3c), the populated template spreadsheetcan then be converted into an OWL file with newlygenerated ontology terms and axioms (Fig. 3d and e).The setting and template files can also be saved andreused.Fig. 3 New OAE term generation and annotation using Ontorat. First an ODP was identified to define new AE terms (a). The ODP guided thegeneration of an Excel template and Ontorat settings. The template file was populated with detailed contents (one row for one new term; onlytwo rows shown in this example) (b). The Ontorat settings were matched to the Excel data format (c). The settings and populated Excel file werethen used as Ontorat inputs to generate an OWL format output file containing newly created ontology terms together with their annotations.The output could be displayed using the Protégé OWL editor (d). After merging the output file to existing OAE file, the detailed information ofimported ontology terms (e.g., discomfort AE OAE_000081) seen in (d) will be obtained from and aligned to existing OAE (e)He et al. Journal of Biomedical Semantics  (2018) 9:3 Page 5 of 10Other ODP-based XOD tools include MappingMaster[42], TermGenie [43], Populous [44], Webulous [45],and ROBOT [36] (Table 1). Developed as a Protégé plu-gin, MappingMaster can only be used with old versionProtégé 3.4 and is not available for newer Protégé 4 and 5[42]. Targeting domain experts, TermGenie provides aweb application that supports new GO term generationbased on predefined patterns [43]. Populous requires soft-ware installation but provides a user-friendly interface[44]. Webulous is Google Add-On application usable withGoogle Spreadsheets [45]. ROBOT also has a templatesystem for converting spreadsheets of terms to OWL files.XOD 4: Community extensibilityThe communitys involvement during the developingphase of an ontology is the key for wide adoption of theontology in the future. However, this step is often abottleneck for ontology development, since the widerthe community is, the more difficult it is to reach agree-ments on term definitions and classifications. In thereality, an ontology is often initiated by a small groupand often driven by one or more use cases. To enhanceits quality and broad recognition, XOD 4 recommendsthat a broader community with more developers andusers participate in the ontology development andapplications. This XOD community extensibilityprinciple emphasizes the community participation tofurther extend, develop, and apply an ontology. With theprinciple of community extensibility, one ontology canbe extended to cover different use cases in the same pro-ject and different projects from a wide range of researchcommunities. The nature of such a practice will requiremore people to participate, make the ontology commu-nity bigger, and achieve better data interoperability.WebProtege [46] and Ontokiwi [24] support community-based ontology development. WebProtege is a web-basedontology editor that supports collaborative OWL ontologydevelopment [46]. WebProtege has been used by manygroups. It includes full change tracking and revision history,and many community collaboration features such assharing and permissions, threaded notes and discussions,watches and email notifications. Ontokiwi is the user-friendly Wiki-like web program that supports community-wide ontology editing, annotation, discussion, and distribu-tion [24]. Ontobedia is an Ontokiwi application preloadedwith existing biomedical ontologies [24]. The Wiki-like addition and editing of text that is not part ofthe ontology makes Ontokiwi/Ontobedia a unique platformfor community-wide ontology discussion and distribution.For community-wide ontology development and applica-tions, tools to support ontology query, comparison, andevaluations are also needed. NCBO BioPortal [2], OLS [47],Ontobee [21], and AberOWL [48] are commonly usedontology registry and repositories that also provide ontol-ogy visualization, queries and analysis features, which facili-tates the community involvement principle. The SPOTontology toolkit (http://www.ebi.ac.uk/spot/ontology/) alsoprovides a list of community-driven open source ontologytools. For example, Ontobee (http://www.ontobee.org) is anontology browser and a linked ontology data server fordereferencing ontology terms [21]. Ontobeep is an ontologycomparison program that compares ontologies and identi-fies common terms existing in two or three ontologies byaligning 23 ontologies from the roots of these ontologies[22]. Ontobeep also detects inconsistency and term duplica-tion in one or more ontologies.Demonstrations of XOD implementation for interoperableontology developmentFigure 4 outlines a simple pipeline of how the XODprinciples can be used together for productive ontologydevelopment. Basically, a new ontology can be initiatedby reusing existing terms from different ontologies(XOD 1) and aligning these terms in a semanticFig. 4 A general ontology development pipeline using XOD principles. To initiate a new ontology, needed terms from existing ontologies areimported and reused (XOD 1) and aligned together with other ontology terms in a consistent semantic framework (XOD 2). To add more termsand semantics afterwards, we can use the same XOD 1/2 methods to add terms from existing ontologies, and for new terms, we can either useODP-based term generation strategy (XOD 3) and manually align and add terms to the new ontology. Community extensibility (XOD 4) shouldbe considered and applied during the whole ontology development pipelineHe et al. Journal of Biomedical Semantics  (2018) 9:3 Page 6 of 10framework (XOD 2), and new terms can be added by ex-tending the semantic framework (XOD 2) and if ODPsidentifiable, applying ODP-based approach (XOD 3).Ontology development often uses top-down andbottom-up approaches simultaneously [49]. The ontol-ogy initiation step is usually achieved by the top-downapproach, i.e., developing the top level semantic frame-work by reusing and aligning upper level terms andsemantics from existing ontologies (XOD 1/2). The sametop-down approach can also be used to generate theupper level new terms commonly identified in the newontology. Meanwhile, the bottom-up approach is usecase driven and focuses on adding new terms to addressspecific use cases. For the bottom-up approach, XOD 13 principles are all important, and if possible, ODP-based design and term generation (XOD 3) is often crit-ical to ensure development efficiency and consistency.Here we will demonstrate our pipeline by using thecomplete use case of developing the community-basedVaccine Ontology (VO) [10, 50, 51]. As outlined in theextensive ontology development pipeline (Fig. 4), at theearly stage of the VO development, we performed ontol-ogy survey and reused terms from several existing ontol-ogies including BFO [33], OBI [7], GO [8], and theInformation Artifact Ontology (IAO) [52]. The originalVO version reported in 2009 included ~1000 importedterms from 10 existing ontologies and ~1000 VO-specific terms [50]. Since then more terms have beenadded to VO. As of November 20, 2017, out of 6541terms in VO, approximately 1600 terms were importedand reused from approximately 30 ontologies (http://www.ontobee.org/ontostat/VO).Many VO-specific terms were added to VO by seman-tically alignment with the upper BFO ontology or middlelevel ontologies (e.g., OBI) (XOD 2). For example, VOterm vaccine (VO_0000001) is asserted as a subclass ofOBI term processed material (OBI_0000047). Thisassertion means that any non-processed material (e.g.,an infectious bacterium that exists in the air) that causesan infection in human and eventual immune responsesand protection in the human is not counted as a vaccine.Similarly, the VO term vaccination (VO_0000002) isasserted as a subclass of OBI term administeringsubstance in vivo (OBI_0600007). The alignment withadministering substance in vivo differentiates VOvaccination (i.e., administering a vaccine to in vivo) fromimmunization (i.e., to make one immune to something).In comparison, vaccination is considered as the synonymof immunization in MedDRA, a controlled terminologysystem commonly used for representation of regulatoryactivities [53].In many cases, we can generate a number of newterms simultaneously by developing and following spe-cific ODPs (XOD 3). For example, the VO developersretrieved from the US Department of Agriculture(USDA) and other public databases the information ofapproximately 800 licensed animal vaccines. Manuallyadding these animal vaccines to VO would be timeconsuming. To speed up the inclusion of the largenumber of licensed animal vaccines to VO, an ODP wasdeveloped to include different entities (e.g., vaccinename, manufacturer, animal species, animal pathogen,and disease), annotations, and the semantic relationsamong these entities. Such an ODP was further used todesign an Excel template which was then applied toinclude the categorized information of these vaccines.Ontorat was finally used to automatically transfer theODP and the information recorded in the Excel file toan OWL file and then imported to VO [20]. Further-more, the same ODP could be used later to add newanimal vaccines to VO. The Ontorat use case of ODP-based VO addition of veterinary vaccines was firstpresented in the 2012 International Conference forBiomedical Ontology (ICBO) [54]. Since then otherODPs were also developed for further VO development[51]. Meanwhile, it is noted that not all new terms canbe fit under identifiable common design patterns. In thiscase, we can generate the term by aligning it with exist-ing framework (XOD 2) (Fig. 4).As a community-based open source ontology, the VOdevelopment has involved the broader community in itscontinuous development (XOD 4). The communityparticipation helps further extend the VO and its inter-operability with other biomedical ontologies. For example,according to BioPortal and Ontobee, the VO term vaccine(VO_0000001) has been reused by more than ten other on-tologies such as OBI and Apollo Structured Vocabulary(https://github.com/apollodev/), and the VO term vaccin-ation (VO_0000002) has been reused by ten other ontologiessuch as the Prescription of Drugs Ontology (https://github.-com/OpenLHS/PDRO). In addition, the VO community in-volvement makes it achieve better data interoperability withother ontologies. Meanwhile, the community involvementextends the applications of VO, such as vaccine-related Tcelland B cell response analysis and queries [55], epitope datamanagement [56], vaccine-related literature mining [10], andvaccine-related network analysis [57, 58].In addition to VO, many other ontologies, e.g., Beta CellGenomics Ontology (BCGO) [37], MicrO ontology forrepresenting microorganism phenotypic and metaboliccharacters [59], and BioAssay Ontology (BAO) [60], havebeen developed using the same or similar strategies.Discussion and perspectivesThe XOD strategy and principles reflect the growing ma-turity of biological and biomedical ontology development.When only a small number of ontologies were developed,such XOD strategy was not needed. However, withHe et al. Journal of Biomedical Semantics  (2018) 9:3 Page 7 of 10hundreds of ontologies developed now, it is critical toensure ontology interoperability, and the XOD principlesprovide a practical solution. Given the importance ofontologies in the integration, sharing, and analysis of theincreasing large and heterogeneous data/metadata andknowledge, the XOD strategy is very significant andcritical to meet the challenges in the current big data era.Among the four XOD principles, the first three princi-ples emphasize the requirements to reuse ontologyterms, extend and align semantic structures, and buildnew terms and semantics among terms using design pat-terns. XOD 2 is a more general principle which coversthe semantic interoperability among terms includingterms from the target ontology and terms newly gener-ated or imported from source ontology. Extending XOD1 and 2, XOD 3 provides a more specific mechanism(i.e., ODP-based term generation and editing) to achieveconsistent ontology term generation and annotation.While the first three principles provide more technicalguidance, XOD 4 emphasizes the community collabor-ation and involvement in new ontology development.XOD is complementary to the OBO principles [1] andthe ten simple rules proposed for ontology development[26]. The OBO principles (e.g., open, common format, ver-sioning, scope, relations, users, collaboration, and locus ofauthority) provide general principles for the developmentof an ontology (http://obofoundry.org/principles/fp-000-summary.html). The ten simple rules proposed by Maloneet al. include ontology term reuse, design patterns, andcommunity engagement [26], which are directly associatedwith XOD principles. The other 7 rules (e.g., scope, license,versioning) are not directly related. In comparison to theOBO principles and the ten simple rules, the XOD princi-ples address the single important point of ontology extensi-bility and emphasize different scales of extensible relationsamong ontologies, with the aim to achieve ontology inter-operability. Since different ontologies extend and arealigned with existing reliable ontologies, applying the XODprinciples will support the OBO aim of establishing non-redundant and interoperable suite of ontologies.The XOD strategy supports the FAIR Guiding Princi-ples, which propose that various data be Findable,Accessible, Interoperable and Reusable [27]. Ontologieslay out the basic foundation for the data FAIRness.Adopting the XOD strategy will lead to the developmentof extensible ontologies and the generation of ontology-extended data and metadata representations. Suchontology-supported data sharing and integration willresult in natural data access, interoperability, and usabil-ity, query, and advanced analysis. For example, theKaBOB knowledge base uses the OBO ontologies to se-mantically integrate data from 18 prominent biomedicaldatabases [61]. Millions of RDF triples were also gener-ated in KaBOB, enabling findable, accessible,interoperable, and reusable queries of the underlyingdata from these databases. Therefore, the XOD strategysupports the eventual achievement of the FAIRness ofdata.Many challenges exist in adopting and achieving thegoals defined in the XOD strategy and principles. First,the interoperability among current hundreds of ontol-ogies is still limited and challenging [32, 62, 63]. Termredundancy among ontologies cannot be solved easily,leading to issues of achieving data FAIRness. Second,only a small amount of data resources (including a largenumber of databases) adopt ontology-guided strategy,which restricts data interoperability and analysis. Third,while many linked data systems [25] standardize datausing ontologies, the ontologies underlying linked data areoften non-interoperable, making linked data systemsbecome individual silos and difficult to integrate [64, 65].To address these challenges, it is important to adopt theXOD strategy and XOD principles. Active ontology train-ing and outreach will be beneficial.The suite of Ontoanimal tools has provided differentfeatures to address several real issues in ontology devel-opment. Each of these tools focuses on one or moreprimary tasks, and all together they are combined tostrongly support XOD principles. Given the complexityof these tools, there are concerns about their usabilityand sustainability. Since these tools are more aboutresearch in ontology development, it is important tohave a strong evaluation system to be used to betterunderstand the strengths and limitation of each tool.While Ontoanimal tools and other similar XOD toolshave already supported the XOD strategy, existing toolsrequire further improvements, and more user-friendlyintegrative tools are needed. Tools are critical to makemore efficient extensible ontology development. Forexample, although it was recognized that term reuse wasa better strategy, the term reuse principle was not widelyimplemented until Ontofox and other tools were devel-oped. Currently XOD tool usage often requires extensivetraining. More easy-to-use and integrative XOD toolsare desired for ontology developers and users with no orlimited programming background. We believe that theadoption of the XOD principles together with robustXOD tools would greatly support interoperable ontologydevelopment and data FAIRness.ConclusionOur examination of Ontoanimal tools and similar programsdiscovered their shared features of extensibility. We pro-posed the eXtensible Ontology Development (XOD) strat-egy and four XOD principles to support extensible ontologydevelopment and usage. We propose to adopt these XODprinciples for active development and usage of extensibleontologies and tools, leading to better data FAIRness.He et al. Journal of Biomedical Semantics  (2018) 9:3 Page 8 of 10AbbreviationsBFO: Basic formal ontology; GO: Gene ontology; MIREOT: MinimumInformation to reference an external ontology term; NCBO: National Centerfor Biomedical Ontology; OAE: Ontology of adverse events; OBO: Openbiological and biomedical ontologies; ODP: Ontology design pattern;OWL: Web ontology language; RDF: Resource description framework;XML: eXtensible markup language; XOD: eXtensible ontology developmentAcknowledgementsWe acknowledge the OBO Foundry consortium and OBO ontologydevelopers for their support and collaborations.FundingThe Ontoanimal tool research was partly supported by the NIH NationalInstitute of Allergy and Infectious Diseases grant 1R01AI081062.Availability of data and materialsNot applicable.Authors contributionsYH Ontoanimal tool project design and management, initial XOD conceptproposer, preparation of the first manuscript draft; ZX Primary developers ofOntoanimal tools Ontofox, early version of Ontobee, Ontobeep, Ontodog,Ontorat, and Ontobat; JZ Co-developer of Ontodog, Ontorat, and Ontobee;YL Ontoanimal tool testing, discussion, and evaluation; JO ROBOT tooldeveloper, Ontoanimal tool testing and evaluation; EQ Primary developers ofOntokiwi/Ontobedia, later version of Ontobee, Ontobull, and Ontoanimaltool maintenance. All authors participated in discussion, manuscriptpreparation and editing, and approved the manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Unit for Laboratory Animal Medicine, Department of Microbiology andImmunology, Center for Computational Medicine and Bioinformatics,University of Michigan Medical School, Ann Arbor, MI, USA. 2Department ofGenetics, University of Pennsylvania Perelman School of Medicine,Philadelphia, PA 19104, USA. 3Center for Computational Science, University ofMiami, Coral Gables, FL, USA. 4Knocean Inc., Toronto, ON, Canada.5Department of Computational Medicine and Bioinformatics, University ofMichigan Medical School, Ann Arbor, MI, USA.Received: 1 May 2017 Accepted: 7 December 2017RESEARCH Open AccessGGDonto ontology as a knowledge-basefor genetic diseases and disorders ofglycan metabolism and their causativegenesElena Solovieva1, Toshihide Shikanai1,2, Noriaki Fujita1,2 and Hisashi Narimatsu1,2*AbstractBackground: Inherited mutations in glyco-related genes can affect the biosynthesis and degradation of glycansand result in severe genetic diseases and disorders. The Glyco-Disease Genes Database (GDGDB), which providesinformation about these diseases and disorders as well as their causative genes, has been developed by the ResearchCenter for Medical Glycoscience (RCMG) and released in April 2010. GDGDB currently provides information on about 80genetic diseases and disorders caused by single-gene mutations in glyco-related genes. Many biomedical resourcesprovide information about genetic disorders and genes involved in their pathogenesis, but resources focused on geneticdisorders known to be related to glycan metabolism are lacking. With the aim of providing more comprehensiveknowledge on genetic diseases and disorders of glycan biosynthesis and degradation, we enriched the content of theGDGDB database and improved the methods for data representation.Results: We developed the Genetic Glyco-Diseases Ontology (GGDonto) and a RDF/SPARQL-based user interface usingSemantic Web technologies. In particular, we represented the GGDonto content using Semantic Web languages, such asRDF, RDFS, SKOS, and OWL, and created an interactive user interface based on SPARQL queries. This user interfaceprovides features to browse the hierarchy of the ontology, view detailed information on diseases and related genes,and find relevant background information. Moreover, it provides the ability to filter and search information by facetedand keyword searches.Conclusions: Focused on the molecular etiology, pathogenesis, and clinical manifestations of genetic diseases anddisorders of glycan metabolism and developed as a knowledge-base for this scientific field, GGDonto providescomprehensive information on various topics, including links to aid the integration with other scientific resources.The availability and accessibility of this knowledge will help users better understand how genetic defects impactthe metabolism of glycans as well as how this impaired metabolism affects various biological functions and humanhealth. In this way, GGDonto will be useful in fields related to glycoscience, including cell biology, biotechnology, andbiomedical, and pharmaceutical research.Keywords: Semantic web technologies, Ontology, RDF/SPARQL-based user interface, Glycan metabolism, Geneticdiseases and disorders* Correspondence: h.narimatsu@aist.go.jp1Glycoscience and Glycotechnology Research Group, National Institute ofAdvanced Industrial Science and Technology (AIST), Tsukuba, Japan2GlycoBiomarker Leading Innovation Co. Ltd. (GL-i), Tsukuba, Japan© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Solovieva et al. Journal of Biomedical Semantics  (2018) 9:14 https://doi.org/10.1186/s13326-018-0182-0BackgroundGlycoscience refers to the study of the structures andfunctions of glycans and glycoconjugates and covers awide range of topics, including the role of carbohydratesin disease development. Glycans play essential roles inmany cellular functions and biological processes, andabnormalities in their metabolism lead to impaired func-tions of multiple organ systems and eventually result inthe development of diseases and disorders. Inheritedmutations in glyco-related genes can affect the biosynthesisof glycans as well as their degradation, and this impairedmetabolism can result in severe genetic diseases anddisorders, such as congenital disorders of glycosylation(CDG) and lysosomal storage diseases (LSD).Genetic diseases and disorders of glycan metabolismare currently the subject of research in many scientificfields, including glycoscience and biomedical research.Many relevant information resources and databases arenow publicly available, including the Online MendelianInheritance in Man (OMIM) [1], the Genetics HomeReference [2], the Genetic and Rare Diseases InformationCenter (GARD) resources [3], the Genetic Testing Registry(GTR) [4], and the Orphanet portal [5], among others.These resources provide information on a wide range ofknown hereditary diseases and disorders as well as theircausative genes, and, moreover, their contents are notrestricted to certain types of metabolic products. Incontrast to these broadly focused information resources,the Glyco-Disease Genes Database (GDGDB) [6] has beencreated by the Research Center for Medical Glycoscience(RCMG) to provide information about hereditary diseasesand disorders caused by defects of single glyco-relatedgenes. The GDGDB provides detailed information aboutinborn errors of glycan metabolism in the context of theirresponsible genes and pathogenetic processes. It is arelational database with a web-based user interface andis available in English and Japanese.Semantic Web technologies allow us to describe andorganize scientific information, to create a knowledge-base for a particular scientific field, to share informationacross services, and to build links between related informa-tion resources. They have been widely applied to biomedicaland life sciences information in recent years.As a part of the Life-Science Database IntegrationProject of the National Bioscience Database Center(NBDC) sponsored by the Japan Science and TechnologyAgency (JST), we developed a knowledge-base of geneticdiseases and disorders of glycan metabolism and theircausative genes. We created an ontology named theGenetic Glyco-Diseases Ontology (GGDonto) [7]. Forits development, we performed the following steps: 1)we created the schema and main content of GGDonto;2) organized the GDGDB information, RDFized it, andintegrated it into GGDonto; 3) added information aboutother genetic diseases and disorders sharing a similaretiology; 4) created a classification scheme for thediseases recorded in GGDonto; and 5) enriched thecontent of this ontology by linking it to other biomedicalresources and integrating the information into GGDonto.Our GGDonto ontology is based on Semantic Webtechnologies and is thus represented in Resource Descrip-tion Framework (RDF) format using the RDF Schema(RDFS), Simple Knowledge Organization System (SKOS),and Web Ontology Language (OWL) vocabularies. We alsodeveloped a RDF/SPARQL-based user interface, whichallows users to navigate the ontology, perform searches,and view detailed information in a user-friendly manner.These ontology and RDF/SPARQL-based user interfaceare available at http://acgg.asia/db/diseases/.In this article, we introduce the topics of GGDonto,describe the structure of this ontology, and explain thefunctionality of its user interface, including browsing,searching, and filtering functionalities.Current knowledge on genetic diseases anddisorders of glycan metabolism and theircausative genesOver the past several decades, associations between manyhuman genetic disorders and mutations in genes involvedin the biosynthesis and degradation of glycans have beennewly identified [811].Current knowledge on genetic diseases and disorders ofglycan biosynthesisGlycosylation is the enzymatic process by which glycansare created, altered, and attached to proteins and lipids[11]. Genetic defects in glycosylation lead to a variety ofinherited metabolic disorders known as CDG [10, 1214].In the last decade, several disorders of N-linked proteinglycosylation, O-linked protein glycosylation, and glyco-lipid and glycosaminoglycan biosynthesis have been newlydescribed, and this process of discovery is just beginning[11, 1317]. While many glycosylation disorders arecaused by defects in the N-glycosylation pathway, othersresult from defects in the O-glycosylation pathway, boththe N- and the O-glycosylation pathways, or other path-ways. However, defects in C-glycosylation have not yet beenreported [13, 15]. Obviously, the discovery and detaileddescriptions of novel CDG provide new knowledge aboutthe role of glycans in human physiology and health anddemonstrate a wide range of biological processes that aredependent on proper glycosylation [16].The traditional classification of CDG was proposed atthe First International Workshop on CDG and RelatedDisorders in Leuven in 1999; it is based on the serumtransferrin pattern obtained by the isoelectric focusingtest [11, 15, 18, 19]. In this classification, CDG are dividedinto two groups, I and II, and lowercase letters are used toSolovieva et al. Journal of Biomedical Semantics  (2018) 9:14 Page 2 of 14indicate subtypes of disorders in chronological order oftheir discovery. In this classification, the CDG-I groupincludes disorders characterized by the under-occupancy ofN-glycosylation sites, and the CDG-II group includes disor-ders caused by defects of N-glycan trimming and elongation[10, 13, 19, 20]. Based on the transferrin isoelectric focusinganalyses, the traditional nomenclature has included onlyN-glycosylation disorders. O-Glycosylation and glycolipidbiosynthesis disorders have not been included in this classi-fication, and they have been assigned trivial or biochemicalnames not associated with the family of CDG [18, 19].As the number of CDG disorders increased, the trad-itional classification became more complex and difficult.Moreover, this classification does not indicate the defectivegenes and enzymes that are responsible for the developmentof disorders [14, 15, 18]. In 2008, Jaeken et al. proposed anew nomenclature for all types of CDG [18], and in 2009,the traditional classification of CDG was revised [15]. In thenew nomenclature, the name Congenital Disorders ofGlycosylation (CDG) is used not only for all types ofprotein glycosylation disorders, but also for lipid glyco-sylation disorders [15, 18, 21]. In this nomenclature,glycosylation disorders are named by the official symbolof defective genes, followed by the abbreviation CDG. Ifthe disorder had a letter-based name in the previousclassification, this name is also used and follows in paren-theses. For example, in the new nomenclature, CDG typeIa is named PMM2-CDG (CDG-Ia) [12, 13, 15, 18].Disorders that are caused by defects of glycosylationhave broad, diverse, and severe clinical phenotypes andunderline the significant roles of glycosylation in humancells and tissues [1114]. These clinical features are highlyvariable, ranging from infantile lethality to moderate intel-lectual disabilities in adults [14, 17, 19, 20]. Impaired glyco-sylation usually has a pathophysiological impact in multipleorgan systems, but is particularly likely to affect brain devel-opment and functions of the nervous, musculoskeletal,digestive, and immune systems [10, 19]. Clinically, mostpatients with these disorders have a general failure tothrive, developmental delay, psychomotor retardation,neurological and neuromuscular impairments, and variablefeatures, like musculoskeletal and eye abnormalities, coagu-lopathies, hormone dysfunction, and many other symptoms[10, 17, 19, 20]. However, the phenotypes of some glycosyla-tion disorders are not completely understood and theirclinical descriptions are limited owing to the smallnumber of reported cases [12, 19].Patients with each of these disorders are characterizedby diverse clinical phenotypes, and therefore the clinicalfeatures cannot be used to identify a mutated gene [13, 19].However, similar clinical features can point to commoncauses and common underlying mechanisms, and pheno-typic similarities may be helpful for the identification ofadditional genes related to glycan biosynthesis andmodifications [16]. Moreover, the discovery of novel glyco-sylation disorders with unexpected clinical findings as wellas the description of clinical variability with additional find-ings for known disorders may help to understand the func-tions of glyco-related genes [13, 16]. Certainly, an increasedknowledge of glycosylation disorders will contribute to abetter understanding of the mechanism and impact ofgenetic defects on glycosylation and also the impact ofglycosylation on various biological functions and humanhealth [13, 14, 16, 20].Current knowledge on genetic diseases and disorders ofglycan degradationInherited genetic defects that result in the absence ordeficiency of specific lysosomal hydrolases cause LSD.Missing or deficient enzymes cause the accumulationand storage of intermediate compounds in cells, tissues,and organs, and the macromolecules that are not prop-erly degraded lead to cellular damage and the onset ofsymptoms [9, 2224].Most glycans are degraded and recycled in lysosomes[9, 23, 25]. In this manuscript, we discuss LSD that areassociated with the impaired degradation of glycans andglycoconjugates. These LSD are generally classified accord-ing to the type of glycoconjugate whose catabolism isimpaired, and they are usually divided into three groups[9, 23]. The first group contains diseases caused by geneticdefects in enzymes that are involved in the lysosomal deg-radation of oligosaccharide chains of glycoproteins. Thesediseases are also called oligosaccharidoses. The secondgroup contains diseases associated with genetic deficienciesin lysosomal enzymes that degrade the polysaccharidechain (glycosaminoglycan) of proteoglycans. These types ofdiseases are called mucopolysaccharidoses (MPS). Thethird group contains diseases associated with geneticdefects in enzymes that are involved in glycolipid degrad-ation. These types of diseases are called sphingolipidoseswhen they are associated with the catabolism of sphingoli-pids and other lipid storage diseases when they aregenerally not classified as sphingolipidoses, such as Wolmandisease, which affects the metabolism of cholesteryl estersand triglycerides [9, 23, 26].Glycan-related LSD are characterized by a progressivemultisystem pathology and a variety of progressive phys-ical impairments and mental deterioration [22, 24]. Thephenotypes associated with these LSD include the follow-ing clinical characteristics: brain pathology with centralnervous system manifestations, hepatomegaly, splenomeg-aly, skeletal abnormalities, and heart and lung pathology[2, 24]. For many LSD, different degrees of severity maybe present. If genetic defects result in the complete oblit-eration of enzyme activity, affected individuals tend tohave earlier onsets of symptoms. If genetic defects resultin the significant reduction of enzyme activity, clinicalSolovieva et al. Journal of Biomedical Semantics  (2018) 9:14 Page 3 of 14signs and symptoms may manifest later in childhood, ado-lescence, or adult life, and patients with these juvenile,childhood, or adult onsets usually display more moderatesymptoms than those of patients with earlier onsets[9, 22]. Moreover, the organ systems involved may differwith respect to the time of onset [9].Inherited genetic defects in the lysosomal catabolismof glycans show the importance of the balance betweenglycan synthesis and degradation for proper biologicalfunctions of cells and tissues [25, 27]. The lysosomaldegradation of glycans and glycoconjugates are orderedand highly specific processes and many new insights inour understanding of these complex pathways have beenobtained in studies of LSD [9].MethodsAs described above, both glycan biosynthesis and lysosomalcatabolism are very complicated and highly regulated pro-cesses, playing important roles in tissue homeostasis. For abetter understanding of the etiology and clinical character-istics of these diseases and disorders, more comprehensiveinformation is needed, and this is the aim of GGDonto.Designing the structure and content of the GGDontoontologyTo provide comprehensive information about geneticdiseases and disorders of glycan metabolism, we designedthe structure and content of GGDonto.Most of the illnesses may be described by variouscharacteristics, including their names, etiology, pathogen-esis, clinical manifestations, nosological classifications, thedescriptions and codes from coding systems, and identifiersfrom biomedical sources. We used the same approach todescribe the diseases and disorders included in GGDonto.Later in this section, we have described the types of infor-mation included in the content of GGDonto, the classesand properties used for data representation, and thesources that were used to obtain the information forontology creation.As the metabolism of glycans is very complicated, forthe development of a knowledge-base of genetic diseasesand disorders caused by defects in this metabolism, itwas necessary to describe these impairments in detail inthe context of metabolic pathways. Moreover, because theseimpairments lead to significant clinical manifestations, itwas important to organize phenotype-related informationin a way that is easy to navigate and understand. For thispurpose, we decided to create classifications of theGGDonto diseases and disorders.In nosology (the branch of medical science dealingwith the classification of diseases), diseases are usuallyclassified by etiology (cause), pathogenesis (mechanism bywhich the disease is caused), the presenting symptoms, orthe organ systems involved. We also designed and createdsimilar types of classifications of the genetic diseases anddisorders of glycan metabolism.The causes of all diseases and disorders included inGGDonto are single-gene mutations in glyco-relatedgenes that result in the absence or deficiency of enzymesinvolved in the metabolism of glycans. As a conse-quence, the mechanism underlying these diseases anddisorders is associated with impaired glycan metabolism,which leads to glycan deficiency or accumulation. Asthe cause and mechanism are both associated with themetabolic pathways of glycans, we decided to design thePathway classifications that represent the etiological andpathogenetic aspects of the GGDonto diseases and disor-ders. These Pathway classifications are the main advantageof our ontology. We designed this type of classifications onthe basis of scientific literature, and to our knowledge,this is the first time classifications with such a structureand content, including all subcategories, are being pre-sented in an ontology. The details of the structure of ourPathway classifications are presented in Additional file 1.Along with Pathway classifications, we designedPhenotype classifications that grouped diseases anddisorders by their symptoms and involved organ systems.All of these classifications have been described in thesubsection Classifications of the GGDonto diseases anddisorders.To provide more detailed knowledge about the cause andmechanism, symptoms, and phenotypes of the GGDontodiseases and disorders, we decided to add the correspondinginformation from related biomedical resources, such asvarious descriptions of causative genes and correspondingenzymes, and definitions of diseases and phenotypic char-acteristics. In the subsection Linking GGDonto to relatedbiomedical resources, we have explained how theseresources were integrated into the GGDonto ontology.Application of semantic web technologies for thedevelopment of the GGDonto ontologyThe GGDonto ontology was developed for the particularsubject domain using the Semantic Web standards, suchas RDF, RDFS, and SKOS in combination with OWL.OWL was used to represent the semantic structure andsemantic relations of this ontology. To describe theinformation in GGDonto, we used many elements fromthe OWL vocabulary as well as our self-defined classesand properties that were also defined by rdf:type propertyas instances of owl:Class or owl:ObjectProperty and owl:DatatypeProperty.To organize distinct concepts (ideas or meanings) ofthe GGDonto ontology into concept schemes (a set ofconcepts that may include hierarchical, associative, andsemantic relationships between them) we used the SKOSvocabulary. As it is described in the SKOS Reference(https://www.w3.org/TR/skos-reference/) and SKOS PrimerSolovieva et al. Journal of Biomedical Semantics  (2018) 9:14 Page 4 of 14(https://www.w3.org/TR/skos-primer/), the notion of aconcept scheme is useful when dealing with two or moredifferent knowledge organization systems, such as thesauri,taxonomies, classification scheme, and subject headingsystems. We created the instance of skos:ConceptSchemeclass for each of our classifications, in accordance with thefact that the origins of their structure are different informa-tion sources, including the Medical Subject Headings(MeSH), UMLS Metathesaurus, and scientific literature thatdescribes the classifications of CDG and LSD. Moreover,the information from other sources, such as the GDGDBdatabase and some of the NCBI databases, is also includedin the content of our system. For each of these, we alsocreated the instance of the skos:ConceptScheme class.We defined the main elements (meanings) of our ontol-ogy, such as the diseases, symptoms, or terms in the diseaseclassifications as instances of skos:Concept class, and theirsemantics as instances of our self-defined classes. Using theskos:inScheme property we linked each of these conceptswith one or more of our concept schemes. Because in ourcontent each of the diseases or disorders is included in two(Pathway and Phenotype for LSD) or three (traditionaland new Pathway and Phenotype for CDG) classifica-tions, each disease concept is linked with two or three ofour concept schemes. As it is described in the SKOS Primer,for the SKOS concepts, it is possible to participate in severalconcept schemes at the same time. Moreover, by using theskos:broader, skos:narrower, and skos:related properties,as well as their subproperties defined in GGDonto, weaggregated the set of concepts from each of our schemesinto distinct structures with their own hierarchical, associ-ated, and semantic relationships.As we organized our information into multiple conceptschemes, it was necessary to indicate how these conceptsare related to each other. For this purpose, along with usingsemantic relation properties defined in our ontology, weused the SKOS mapping properties, such as skos:close-Match and skos:exactMatch. As it is described in the SKOSPrimer, the SKOS mapping properties helped us to indicatethat two concepts from different schemes have comparablemeanings, and to specify these meanings. Linking conceptsby mappings (skos:closeMatch and skos:exactMatch) andthe possibility of a concept to participate in different conceptschemes (skos:inScheme) are the important featuresand key advantages of SKOS. This approach helps avoidthe taxonomical conflicts that may be occurring throughthe integration different information sources. For example,CDG are classified in our ontology by the traditional andnew Pathway classifications at the same time.Creating the schema and main content of the GGDontoontologyTo provide comprehensive information about geneticdiseases and disorders of glycan metabolism, definedthese genetic diseases and disorders as the main elements(concepts) of our ontology. The GGDonto ontology wasdeveloped on the basis of the scientific literature andPanyam et al. Journal of Biomedical Semantics  (2018) 9:7 DOI 10.1186/s13326-017-0168-3RESEARCH Open AccessExploiting graph kernels for highperformance biomedical relation extractionNagesh C. Panyam, Karin Verspoor*, Trevor Cohn and Kotagiri RamamohanaraoAbstractBackground: Relation extraction from biomedical publications is an important task in the area of semantic mining oftext. Kernel methods for supervised relation extraction are often preferred over manual feature engineering methods,when classifying highly ordered structures such as trees and graphs obtained from syntactic parsing of a sentence.Tree kernels such as the Subset Tree Kernel and Partial Tree Kernel have been shown to be effective for classifyingconstituency parse trees and basic dependency parse graphs of a sentence. Graph kernels such as the All Path Graphkernel (APG) and Approximate Subgraph Matching (ASM) kernel have been shown to be suitable for classifyinggeneral graphs with cycles, such as the enhanced dependency parse graph of a sentence.In this work, we present a high performance Chemical-Induced Disease (CID) relation extraction system. We present acomparative study of kernel methods for the CID task and also extend our study to the Protein-Protein Interaction (PPI)extraction task, an important biomedical relation extraction task. We discuss novel modifications to the ASM kernel toboost its performance and a method to apply graph kernels for extracting relations expressed in multiple sentences.Results: Our system for CID relation extraction attains an F-score of 60%, without using external knowledge sourcesor task specific heuristic or rules. In comparison, the state of the art Chemical-Disease Relation Extraction systemachieves an F-score of 56% using an ensemble of multiple machine learning methods, which is then boosted to 61%with a rule based system employing task specific post processing rules. For the CID task, graph kernels outperform treekernels substantially, and the best performance is obtained with APG kernel that attains an F-score of 60%, followed bythe ASM kernel at 57%. The performance difference between the ASM and APG kernels for CID sentence level relationextraction is not significant. In our evaluation of ASM for the PPI task, ASM performed better than APG kernel for theBioInfer dataset, in the Area Under Curve (AUC) measure (74% vs 69%). However, for all the other PPI datasets, namelyAIMed, HPRD50, IEPA and LLL, ASM is substantially outperformed by the APG kernel in F-score and AUC measures.Conclusions: We demonstrate a high performance Chemical Induced Disease relation extraction, withoutemploying external knowledge sources or task specific heuristics. Our work shows that graph kernels are effective inextracting relations that are expressed in multiple sentences. We also show that the graph kernels, namely the ASMand APG kernels, substantially outperform the tree kernels. Among the graph kernels, we showed the ASM kernel aseffective for biomedical relation extraction, with comparable performance to the APG kernel for datasets such as theCID-sentence level relation extraction and BioInfer in PPI. Overall, the APG kernel is shown to be significantly moreaccurate than the ASM kernel, achieving better performance on most datasets.Keywords: Relation extraction, Graph kernels, APG kernel, ASM kernel*Correspondence: karin.verspoor@unimelb.edu.auSchool of Computing and Information Systems, University of Melbourne,Melbourne, Australia© The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 2 of 11BackgroundAutomated text mining has emerged as an importantresearch topic for effective comprehension of the fastgrowing body of biomedical publications [1]. Within thistopic, relation extraction refers to the goal of automatedextraction of relations between well known entities, fromunstructured text. Chemical-induced-Disease (CID) rela-tion extraction is motivated by critical applications suchas toxicology studies and drug discovery. The impor-tance of CID relations is evident from a recent study ofPubmed search logs [2], that observed that Chemicals,Diseases and their relations are the most popular searchtopics.Relation extraction: sentence vs non-sentence levelA large corpus of annotated Pubmed abstracts for CIDrelation extraction is now available from BioCreative-V[3] for furthering research and comparison of differentmethods. This is known as the Chemical-Disease Rela-tions (CDR) corpus. The main objective of the CID rela-tion extraction task defined by BioCreative-V CDR task[3], is to infer Chemical-Disease relations expressed by aPubmed document (Title and Abstract only). A sampleannotated article from this corpus is illustrated in Table 1.More generally, relation extraction from text refers tothe task of inferring a relationship between two entitiesmentioned in the text.Within this corpus, many relations may be inferred byanalyzing a single sentence that bears the mentions of therelevant entities (Chemical and Disease). We refer to suchrelations as sentence level relations. For example, the rela-tion between Propylthiouracil and hepatic damage canbe inferred by analyzing the single sentence in the title.non-sentence level relations, such as the relation betweenpropylthiouracil and chronic active hepatitis, are thosein which the entity mentions are separated by one or moresentence boundaries. These relations cannot be inferredTable 1 Illustration of an annotated Pubmed abstract from theCDR corpusTitle Propylthiouracil-induced hepatic damageAbstract Two cases of propylthiouracil-induced liver damagehave been observed. The first case is of an acute type ofdamage, proven by rechallenge; the second presents aclinical and histologic picture resembling chronic activehepatitis, with spontaneous remission.Entity D011441, Chemical, Propylthiouracil, 0-16Entity D011441, Chemical, propylthiouracil, 54-70Entity D056486, Disease, hepatic damage, 25-39Entity D056486, Disease, liver damage, 79-91Entity D006521, Disease, chronic active hepatis, 246-270Relation (CID) D011441 - D006521Relation (CID) D011441 - D056486by analyzing a single sentence. We refer to such relationsas the non-sentence level relations.Prior research has shown that relation extraction canbe addressed effectively as a supervised classificationproblem [4], by treating sentences as objects for classi-fication and relation types as classification labels. Clas-sifiers such as Support Vector Machines (SVMs) aretypically used for high performance classification by firsttransforming a sentence into a flat feature vector ordirectly designing a similarity score (implemented as akernel function) between two sentences. Kernel meth-ods allow us to directly compute a valid kernel score(a similarity measure) between two complex objects,while implicitly evaluating a high dimensional featurespace.The approach of using a kernel is favored for work-ing with syntactic parses of a sentence which are highlystructured objects such as trees or graphs. Tree or graphkernels are known to be efficient in exploring very highdimensional feature spaces via algorithmic techniques.Deep learning [5, 6] based efforts are other alternatives,whose goal is to enable discovery of features (represen-tation learning) with little or no manual intervention.However, we limit our scope in this work, to exploring ker-nel methods for CID relation extraction.We first illustrateparse structures and then describe the kernels developedfor using these parse structures.Parse trees vs parse graphsSimple approaches that use a bag of words model fora sentence, ignore the inherent order within a sentence.However, a sentence can be mapped to an ordered objectsuch as a tree or a graph by using a syntactic parser [7].We illustrate the syntactic parse structures of a samplesentence in Fig. 1. A constituency parse tree, encodes asentence as a hierarchical tree, as determined by the con-stituency grammar. The internal nodes of this tree carrygrammatical labels such as noun phrase (NP) and verbphrase (VP) and the leaf nodes have as labels the wordsor tokens in the sentence. In contrast, a dependency graphexpresses grammatical relationships such as noun sub-ject (nsubj) and verb modifier (vmod) , as directedand labelled edges between the tokens in the sentence.The nodes of this graph correspond one-to-one with thetokens of the sentence. The undirected version of a depen-dency graph, obtained by dropping edge directions, mayor may not result in a cycle free graph. For example,the basic version of dependency graphs produced by theStanford Parser [7] is guaranteed to be cycle free, inits undirected form. However, the enhanced dependencyparses produced by the Stanford Parser may containcycles in its undirected form. In the example illustratedin Fig. 1, note the cycle between the nodes caused andfatigue in the enhanced dependency graph.Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 3 of 11Fig. 1 Illustration of different parse structures for the sentence :Seizures were caused by Alcohol and FatigueKernelsIn NLP, tree kernels such as the Subset Tree Kernel (SSTK)[8, 9] and Partial Tree Kernel (PTK) [10] have been usedeffectively for related tasks such as sentence classification[11]. Tree kernels are applied over syntactic parses such asconstituency parse or basic dependency parses [12]. Thesetree kernels cannot handle edge labels directly and thereforetransform the original dependency trees to special treeswithout edge labels, referred to as the Location CenteredTree [13]. A further limitation is that other forms of parsessuch as enhanced dependency parses which are arbitrarygraphs with cycles, cannot be used with tree kernels.This limitation is overcome with graph kernels such asthe All Path Graph (APG) [14] kernel that can work witharbitrary graph structures. However, APG kernel is pri-marily designed to work with edge weighted graphs andrequires special transformation of the dependency graphsoutput by the parser. APG kernel requires the conversionof edge labels into special vertices and it assigns a heuris-tically determined weight value to the edges. In contrast,the Approximate Subgraph Matching (ASM) kernel isdesigned to work directly with edge labels in the graph.We present a detailed discussion of the APG and the ASMgraph kernels in APG kernel and ASM kernel sections.Relation to prior workIn this section, we relate and contrast the contributionsof this paper with closely related prior work. In our priorwork, we proposed a graph kernel based on approxi-mate subgraph matching (ASM) [15]. ASM kernel adoptsan approach to graph similarity that is derived froma subgraph isomorphism based event extraction system[16] developed for biomedical relation extraction [17].In the first step, ASM seeks to match vertices betweenthe two input graphs. Then, the set of all pair shortestpaths from the two input graphs are compared, based onthe matched vertices. The similarity estimation is basedon the counts of edge labels along the shortest path.In our previous work [18], we evaluated the effective-ness of Subtree (STK) and Subset-tree kernels (SSTK)Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 4 of 11[8, 19] with constituency parse trees for the CID relationextraction task.In the current work, we introduce a modified form ofASM kernel that incorporates edge weights in the graph.Note that the ASM kernel as presented in prior work [15]considered edge-unweighted graphs only. This ability toincorporate edge weights enables the ASM kernel to posi-tively discriminate between the shortest dependency pathbetween the entities and other paths in the graph, there-fore boosting its performance further. For instance, theCID sentence level relation extraction with ASM kernel asreported in [15] is 58%, but improved to 63% in currentwork. Secondly, we have extended the evaluation for theCID task with other tree kernels namely the Partial TreeKernel (PTK) [10] and graph kernels ASM and APG [20]with dependency parse graphs.ContributionsA summary of the main contributions of this paper are : We demonstrate a high performance CID relationextraction system, reaching an F-score of 60.3%.This performance is achieved using an effectivemethod for non-sentence relation extraction, bycombining multiple sentence level parse structuresinto larger units, and then applying the kernelmethods on the aggregate parse structures. Oursystem compares favorably with prior art [21], wherean ensemble of machine learning methods was usedto achieve an F-score of 56% and then boosted to61.3% using task specific post-processing rules. Incontrast, our system is a general purpose relationextraction system, that does not employ any task ordomain specific rules. We present a novel graph kernel, namely the ASMkernel with modifications to incorporate edgeweights in the graph. We provide a comparativestudy of the performance of the ASM kernel with thestate of the art tree and graph kernels, over twoimportant biomedical relation extraction tasks, theChemical-Induced Disease (CID) and the Protein-Protein Interaction (PPI) tasks. We demonstrate thatthe ASM kernel is effective for biomedical relationextraction, with comparable performance to thestate of the art APG kernel on several datasets suchas CID-sentence level relations and BioInfer in PPI. All software for reproducing the experiments in thispaper, including our implementation of the APG andthe ASM graph kernels in the Java based Kelp [22]framework, is available in the public domain1.MethodsIn this section, we describe the 3 main kernel methodsthat are studied in this paper, namely the Tree Kernels[10, 19, 23], the All Path Graph (APG) Kernel and theApproximate Subgraph Matching (ASM) Kernel [15].Tree kernelsTree kernels [8] using constituency parse or dependencyparse trees have been widely applied for several rela-tion extraction tasks [13, 18, 24]. They estimate simi-larity by counting the number of common substructuresbetween two trees. Owing to the recursive nature of trees,the computation of the common subtrees can be effi-ciently addressed using dynamic programming. Efficientlinear time algorithms for computing tree kernels arediscussed in [10].Different variants of tree kernels can be obtained, basedon the definition of a tree fragment, namely subtree, sub-set tree and partial tree. A subtree satisfies the constraintthat if a node is included in the subtree, then all its descen-dents are also included in the subtree. A subset tree onlyrequires, that for each node included in the subset tree,either all of its children are included or none is includedin the subtree. A partial tree is the most general tree frag-ment, which allows for partial expansion of a node, i.efor a given node in the partial tree fragment, any subsetof its children nodes may be included in the fragment.Subset trees are most relevant with constituency parsetrees, where the inner nodes refer to grammatical produc-tion rules. Partial expansion of a grammatical productionrule leads to inconsistent grammatical structures. As such,subset trees restrict the expansion of a node to includeall of its children or none. For dependency parse treeswith no such grammatical constraints, partial trees aremore suitable to explore a wider set of possible tree frag-ments. We experiment with subset tree kernels (SSTK)with constituency parses and partial tree kernels (PTK)with dependency parses and report the results on both.We illustrate the constituency parse tree for a samplesentence in Fig. 1.Here, we present the formal definition of tree kernels.Let T1 and T2 denote two trees and let F = {f1, f2, . . .}denote the set of all possible tree fragments. Let Ii(n) be anindicator function that evaluates to 1 when the fragmentfi is rooted at node n and 0 otherwise. The unnormalizedkernel score is given by:K(T1,T2) =?n1?NT1?n2?NT2(n1, n2) (1)where NT1 and NT2 are the sets of nodes of T1 and T2respectively and (n1, n2) = ?|F|i=1 Ii(n1)Ii(n2).Efficient algorithms for computing tree kernels in lineartime in the average case are presented in [10].We used theimplementation of tree kernels provided in Kelp [22].Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 5 of 11APG kernelThe APG kernel [14] is designed to work with edgeweighted graphs. A given dependency graph G needs tobe first modified, to remove edge labels and introduceedge weights. Let e = l(a, b) denote an edge e with labell, from the vertex a to vertex b. For every such edge inthe original graph, we introduce a new node with label land two unlabeled edges (a, l) and (l, b) in the new graph.The APG kernel recommends a edge weight of 0.3 asa default setting for all edges. To accord greater impor-tance to the entities in the graph, the edges along theshortest path between the two entities are given a largerweight of 0.9. This constitutes the subgraph derived fromthe dependency graph of a sentence. Another subgraphderived from the linear order of the tokens in the sentenceis constructed. In this subgraph, n vertices are created torepresent the n tokens in the sentence. The lemma of atoken is set as the label of the corresponding node. Thesevertices are connected by n ? 1 edges, for the n tokensfrom left to right. That is, edges are introduced betweentoken i and token i+1. These two disconnected subgraphstogether form the final edge weighed graph over which theAPG kernel operates.Let A denote the adjacency matrix of the combinedgraph. Let connectivity of a path refer to the product ofedge weights along the path. Intuitively, longer paths orpaths with lesser edge weights, have connectivity closerto 0 and shorter paths or paths with greater edge weightshave a connectivity closer to 1. Note that the matrix Airepresents the sum of connectivity of all paths of lengthi, between any two vertices. The matrix W is defined asthe sum of the powers of A, I.e W = ??i=1 Ai. It is effi-ciently computed asW = (I?A)?1. Therefore,W denotesthe sum of connectivity over all paths. Any contributionto connectivity from self loops is eliminated by settingW = W ? I. Finally, the APG kernel computes the matrixGm = LWLT , where L is the label allocation matrix, suchthat L[ i, j]= 1 if the label li is present in the vertex vj and0 otherwise. The resultant matrix Gm represents the sumtotal of connectivity in the given graphG between any twolabels. Let Gm1 and Gm2 denote the matrices constructed asdescribed above, for the two input graphs G1 and G2. TheAPG kernel score is then defined as :K(G1,G2) =|L|?i=1|L|?j=1Gm1[li, lj] × Gm2[li, lj](2)Impact of linear subgraphWenoticed substantially lower performance with the APGkernel when the labels marking the relative position ofthe tokens with respect to the entities, i.e. labels such asbefore, middle and after in the linear subgraph areleft out. For example, the F-score for AIMed in PPI taskdrops by 8 points, from 42 to 34%, when these labels areleft out. This highlights the importance of the informationcontained in the linear order of the sentence, in additionto the dependency parse graph.ASM kernelThe ASM kernel [15] is based on the principles of graphisomorphism. Given two graphs G1 = (V1,E1) and G2 =(V2,E2), graph isomorphism seeks a bijective mappingof nodes M : V1 ? V2 such that, for every edge ebetween two vertices vi, vj ? G1, there exists an edgebetween the matched nodes M(vi),M(vj) ? G2 and viceversa. The ASM kernel though, seeks an approximatemeasure of graph isomorphism between the two graphs,that is described below. Let L be the vocabulary of nodelabels. In the first step, ASM seeks a bijective mappingM1 : L ? V1, between the vocabulary and the nodes,such that M1(li) = vj, vj ? V1 when the vertex vj has thenode label li. To enable this, all nodes in the graph areassumed to have distinct labels. For every missing label liin the vocabulary, a special disconnected (dummy) nodevj with the label li is introduced. Next, ASM does not seekmatching edges between matching node pairs. Instead, itevaluates the similarity of the shortest path between them.Consider two labels li, lj. Let x, y be the vertices in thefirst graph with these labels respectively. I.e M1 (li) =x,M1(lj) = yandx, y ? V1. Let P1x,y be the shortest pathbetween the vertices x and y in the graph G1. Similarly, letx?, y? denote the matching vertices in the second graph. I.eM2 (li) = x?,M2(lj) = y?andx?, y? ? V2. Let P2x?,y? denotethe shortest path between the vertices x? and y? in thegraph G2. The feature map ? that maps a shortest pathP into a feature vector is described following the ASMkernel definition below.The ASM kernel score is computed as:K(G1,G2) =|L|?i=1|L|?j=1?(P1x,y)· ?(P2x?,y?)s.tM1(li) = x,M1(lj) = y and x, y ? V1andM2(li) = x?,M2(lj) = y? and x?, y? ? V2(3)Feature spaceThe feature space of ASM kernel is revealed by examin-ing the feature map ? that is evaluated for each short-est path P. ASM kernel explores path similarity along 3aspects, namely structural, directionality and edge labels,as described below. We use the notationWe to denote theweight of an edge e. An indicator function Ile is used toindicate if an an edge e has an edge label l. Similar to theAPG graph, we set the edge weights to 0.9 for edges on theshortest dependency path between two entities and 0.3 forthe others.Structural similarity is estimated by comparing pathlengths. Note that similar graphs or approximatelyPanyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 6 of 11isomorphic graphs are expected to have similar pathlengths for matching shortest paths. Therefore, a singlefeature ?distance(P) = ?e?P We , is computed to incorpo-rate structural similarity, where We denotes the weight ofan edge e in the path P.Directional similarity is computed like structural sim-ilarity, but unlike structural similarity, edge directionsare considered. ASM kernel computes two features,?forward edges(P) = ?f?P Wf and ?backward edges(P) =?b?P Wb, where f and b denote a forward facing andbackward facing edge respectively, in the path P.Edge directions may themselves be regarded as specialedge labels of type forward or backward. Edge labelsimilarity generalizes the above notion to an arbitraryvocabulary of edge labels E. In particular, E is the set ofdependency types or edge labels generated by the syntac-tic parser. For each such edge label l ? E, ASM kernelcomputes the feature ?l(P) =?e?P WIlee , where Ile denotesan indicator function that takes a value 1 when the edge ehas a label l and 0 otherwise.The full feature map ?(P) is the concatenation of theabove described features for structural, directionality andedge label similarity. We illustrate this feature map for asample enhanced dependency graph illustrated in Fig. 1.For the label pair seizures, fatigue, the shortest path P isthrough the single intermediate vertex caused. For thispath, the non-zero features are : ?(P) = {?distance =(0.9)2,?forward edge = 0.9,?backward edge = 0.9,?nsubj =0.9,?nmod:by = 0.9, }.Implementation detailsWe implemented the APG and ASM kernel in the Javabased Kelp framework [22]. The Kelp framework pro-vides several tree kernels and an SVM classifier that weused for our experiments. We did not perform tuningfor the regularization parameter for SVM, and used thedefault settings (C-Value= 1) in Kelp. Dependency parseswere generated using Stanford CoreNLP [7] for the CDRdataset. For the Protein-Protein-Interaction task, we usedthe pre-converted corpora available from [14]. The cor-pus contains the dependency parse graphs derived fromCharniak-Lease Parser, which was used as input for ourgraph kernels. All software implemented by us for repro-ducing the experiments in this paper, including the graphkernels APG and ASM implementations are available in apublic repository.ResultsWe evaluate the performance of the ASM and APG ker-nels. We first describe our experimental setup and thendiscuss the results of our evaluation of the different ker-nels for relation extraction.CID relation extractionThis experiment follows the Chemical-Induced DiseaseRelation Extraction subtask of [3]. The CDR corpus madeavailable by [3] contains three datasets, namely the train-ing set, development set and the test set. Each datasetcontains 500 PubMed documents (title and abstract only)with gold standard entity annotations. More details aboutthis corpus is available at [3]. A sample Pubmed documentis illustrated in Table 1.Classifier setupWe build separate relation extraction subsystems for sen-tence level relations and non-sentence level relations.That is, for any relation (C,D) in a document (where C,Ddenotes a chemical and disease identifier respectively), wesearch for any single sentence that bears mentions to boththe relevant entities C,D. If such a sentence is found, it isadded as an example into sentence level relation extrac-tion subsystem. When no such sentence can be found,such a (C,D) pair is regarded as a non-sentence relation.For these relations, we retrieve all sentences bearing amention to either C or D. All such sentences are paired toform examples for the relation (C,D). That is, an examplefor a non-sentence relation (C,D) is a pair of sentences,one containing the mention of entity C and the secondcontaining the mention of entity D.Entity focusNote that a single sentence can carry multiple entitypair mentions, with different relations between then. Forexample, the sentence The hypotensive effect of alphamethyldopa was reversed by naloxone, carries two entitypair mentions, namely alpha methyldopa, hypotensiveand naloxone, hypotensive. The first entity pair is related(alpha methyldopa causes hypotension) whereas the sec-ond entity pair is unrelated. Therefore, the above sen-tence should be suitably processed to extract two differenttraining or testing examples for classification, that servetwo different entity-pairs, namely alpha methyldopa,hypotensive and naloxone, hypotensive. To distinguishbetween the two cases, we attach special vertices with thelabels Entity1 and Entity2, that are connected to theentity-pair in focus, in the given graph.Examples for the classifierFor sentence level relations, we transform each sentenceinto tree or graph by retrieving its constituency parsetree or dependency parse graph. For non-sentence rela-tion examples, we first retrieve the underlying pair ofsentences representing the example and transform eachsentence to a tree or graph. The resultant pair of treesor graphs are then connected at the root node, with aspecial edge labelled Sentence Boundary, to result in asingle tree or a graph, that can then be input to a tree orPanyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 7 of 11graph kernel based classifier. The relations retrieved fromthe two subsystems for sentence and non-sentence levelrelations are merged (union) together, to form the finalset of retrieved Chemical-Disease relations for the wholePubMed document.Results for the CID taskThe CID Relation extraction performance of the differ-ent kernels is characterized by measuring the Precision,Recall and F1measures. These are presented in Table 2 forthe CDR dataset. All the measurements listed in Table 2are based on relation extraction with gold standard entityannotation. Further, we have provided the performancemeasurements for sentence-level relations only and non-sentence level relations only, which characterizes the per-formance of our two relation extraction subsystems. Thecolumn All Relations represents the performance of thefinal relation extraction system over the full CDR test data,that corresponds to the subtask of BioCreative-V [3].Comparisonwith prior artA suitable comparison from prior art is the CID relationextraction system by [21]. Similar to our system, they usegold standard entity annotations and do not employ anyexternal knowledge source or knowledge base. This priorwork by [21] consists of a hybrid system or an ensembleof classifiers based on feature-based model, a tree kernel-based model and a neural network model. Their systemis designed for sentence level relations only and ignoresnon-sentence relations. The F-score of this hybrid systemis reported to be 56%. To further boost the performance,the authors in [21], propose the use of custom or CIDtask specific post processing rules, such as associating theChemical mentioned in the title with the Diseases men-tioned in the abstract. These heuristics were found to helpboost the performance of their system to 61.3%.In our work, we do not employ any custom heuris-tics and instead rely on machine learning techniquesonly. Interestingly, when we removed our subsystem fornon-sentence level relation extraction, we observed thatour final CID relation extraction performance, drops to55.7% and 54.0% respectively, for the APG and ASMkernel based systems. In other words, our final perfor-mance of 60.3%, is due to the substantial contribution(+5% points in F-score), from the non-sentence relationextraction.To summarize, our main findings from the CID relationextraction task are: The APG and ASM graph kernels substantiallyoutperform the tree kernels for relation extraction. APG kernel offers the best performance, with anF-score of 65% for sentence level relation extraction,45% for non-sentence level relation extraction and60% for the full CID test relations. ASM kernel is effective for relation extraction andits performance approaches that of the state of theart APG kernel, with an F-score of 63% for sentencelevel relation extraction, 37% for non-sentencerelations and 57% for the full CID test relations. Our system achieves a close to state of the artperformance for CID relation extraction (60% vs 61%),without employing heuristics or task specific rules. Effective non-sentence level relation extractionprovides a substantial boost (+5 points) to the finalF-score for our CID relation extraction task.Protein-protein interaction extractionThe Protein-Protein Interaction (PPI) extraction task,involves extracting Protein-pairs that interact with eachother, from Biomedical Literature. We used the PPI cor-pora from [25], that consists of 5 datasets, namely AIMed,BioInfer, HPRD50, IEPA and LLL. These are collectionsof sentences sourced from biomedical publications aboutprotein interactions. The goal of the PPI task is to ana-lyze these sentences, such as Isolation of human delta-catenin and its binding specificity with presenilin 1 andextract interacting Protein-pairs such as (delta-catenin,presenilin 1). We used the derived version of the PPIcorpora [25], that contains sentences together with theirTable 2 Performance measurements for chemical induced disease relation extractionMethod Sent-Rel. only Non-Sent-Rel. only All relationsP R F P R F P R FSSTK with CP-Tree 43.1 73.7 54.4 36.9 14.2 20.5 42.5 56.0 48.3PTK with LCT 42.2 75.3 54.1 30.5 40.1 34.6 39.5 64.8 49.0APG with Dep. Graphs 54.7 80.6 65.1 47.8 43.8 45.7 53.2 69.7 60.3ASM with Dep. Graphs 51.6 80.8 63.0 38.8 36.0 37.3 49.0 67.4 56.8Hybrid (Prior art [21]) - - - - - - 64.9 49.2 56.0Hybrid+Rules (Prior art [21]) - - - - - - 55.6 68.4 61.3(Key: P,R,F denotes Precision, Recall and F1 score respectively. Sent-Rel. and Non-Sent-Rel. denotes sentence level relations and Non-Sentence level relations respectively.CP-Tree and LCT denote constituency parse tree and location centered tree. Dep. Graph denotes dependency graph. The best performance is highlighted in italicized font)Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 8 of 11Charniak-Lease Parser based tokenization, part of speechtagging and dependency parse in a standardized XMLformat. The corpus contains the list of protein-pairs ineach sentence with a label True for interacting pairsand False otherwise. We used the dependency parsesin the corpus to produce graphs that serve as inputsfor our graph kernel with SVM based classification. Weexperiment with graph kernels, specifically the APG andASM kernels. From prior work [26], we know that APGkernel substantially outperforms tree kernels for the PPItask. Therefore, our main objective in this experiment isto characterize the performance of the ASM and APG(our implementation) kernels for the PPI task, and con-trast these to the state of the art APG kernel based PPIperformance.Results for the PPI taskWe evaluate our implementation of the APG and ASMkernels in the cross-learning setting, that involves group-ing 4 out of the 5 datasets into one training unit andtesting on the one remaining dataset. These results arepresented in Table 3.We have also listed the state of the artperformance measurements for PPI with the APG kernel,as reported in prior art (see Table 3 of [26]).Comparisonwith prior artThe PPI task, is characterized by the measures Precision,Recall and F-score and the AUC or the Area Under theROC Curve. As indicated in prior art [26], AUC is invari-ant to the class distribution in the dataset and is thereforeregarded as an important measurement to characterizethe PPI extraction performance.To summarize, our findings from the PPI experimentare: We expect the AUC measurements for our APGimplementation to match that of the APGimplementation in prior art (Table 3 of [14]). TheAUC measurements are nearly equal for the largerdatasets, AIMed and BioInfer, but differ noticeablyfor the smaller datasets, HPRD50 and LLL. A likelycause for this variation is the differing classifierframeworks (SVMs vs Regularized Least Squares)used in these two experiments. Our APG implementation varies substantially withthe prior art, in Precision and Recall and moderatelyin F-score. These measurements are known to besensitive to parameter setting of the classifier andless dependent on the kernel characteristics itself.However, due to computational costs, we have notperformed any parameter tuning in this work. ASM kernel outperforms APG for BioInfer (AUC of74.1 vs 69.6), which is a large dataset. However, APGkernel outperforms ASM by a substantial differencefor all the remaining datasets, namely AIMed,HPRD50, IEPA and LLL. We conclude that ASM isoutperformed by APG for the full PPI task.Statistical significance testingThe CID and PPI relation extraction tasks, considereddifferent measurements, such as F-score and AUC, thatare considered relevant for relation extraction task. Interms of classification accuracy, a better comparison ofthe two kernels can be performed with the McNemarstest [27]. McNemars test estimates the statistical signifi-cance for the null hypothesis that the two classifiers areequally accurate. The P-values for the null hypothesis, cor-responding to different classification tasks, are listed inTable 4. The datasets for which the null hypothesis can berejected (P-value < 0.01) are highlighted. This test con-firms that the APG and ASM kernels are significantly dif-ferent in classification accuracy, over several large datasetssuch as AIMed, BioInfer and CID non-sentence relations.DiscussionIn this section, we present a detailed comparison of thetwo graph kernels, namely ASM and APG kernel. Wefocus our study on the graph kernels only as we sawabove for CID relation extraction, that they substantiallyTable 3 Performance measurements for protein-protein interaction extractionMethodAIMed BioInfer HPRD50P R F A P R F A P R F ASOA 30.5 77.5 43.8 77.6 58.1 29.4 39.1 69.6 64.2 76.1 69.7 84.0APG 28.6 81.6 42.3 76.8 68.6 28.6 40.4 69.7 62.3 69.9 65.9 79.7ASM 26.3 78.0 39.3 72.9 67.2 22.6 33.8 74.1 66.0 58.3 61.9 76.2IEPA LLLP R F A P R F ASOA. 78.5 48.1 59.6 82.4 86.4 62.2 72.3 86.4APG 78.2 41.8 54.5 80.2 84.7 57.3 68.3 83.4ASM 82.8 17.3 28.6 77.7 79.3 28.0 41.4 75.3(Key: P,R, F and A denotes Precision, Recall, F score and area under curve respectively. SOA denotes State of the art performance with APG as reported in [26]). The bestperformance is highlighted in italicized font)Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 9 of 11Table 4 Statistical significance (McNemars) tests for the ASM and APG classifiers, for the null-hypothesis being that the two classifiersare equally accurate and a significance threshold of 0.01DatasetNumber of examples AccuracyP-valueTraining Testing APG ASMAIMed 11,246 5,834 58.6 53.1 3.8e-7BioInfer 7,414 9,666 77.8 76.8 0.0011HPRD50 16,647 433 70.9 68.1 0.999IEPA 16,263 817 73.6 66.5 3.9e-6LLL 16,750 330 75.4 65.1 2.2e-6CID: Sentence level relations. 9,913 5,099 72.2 71.2 0.0969CID: Non Sentence level relations 21,656 11,562 84.9 84.1 0.0002P-values less than the threshold are shown in italicized fontoutperform the tree kernels. Interestingly, both kernelsfollow the approach of comparing node pairs betweentwo graphs to estimate graph similarity. However, thekey difference between the two kernels is their treat-ment of edge labels in the graph. In APG, edge labelsin the graph are transformed into intervening nodes.Therefore, the node label vocabulary in APG is a het-erogeneous set which is the union of the vocabulary ofword lemmas, V in the corpus and the vocabulary ofedge labels D defined by the dependency parser [7]. Thatis, the set of node labels considered by APG kernels isL = V ? D. The features explored by the APG ker-nel can be indexed by pairs of node labels, of the form(V ? D) × (V ? D).In ASM kernel, edge labels (dependency types D) andnode labels (word lemmas V ) are treated separately.ASM associates a node label pair with a rich featurevector, where each feature is a function of the edge labelsalong the shortest path between the nodes. Therefore,its feature space can be indexed by triplets of the form(V × V × D). This is an important difference fromthe APG kernel, which associates a single scalar (graphconnectivity) value with a node label pair. The higherfeature space dimensionality for the ASM kernel is alikely cause for its lower performance than the APGkernel. The other main difference between the two ker-nels is that the APG kernel considers all possible pathsbetween a pair of nodes, whereas ASM kernel considersonly the shortest path. This is another likely factor, thatis disadvantageous to ASM kernel in comparison toAPG kernel.Error analysisWe manually examined a few error samples to identifythe likely causes of errors by APG and ASM kernel inCID and PPI relation extraction tasks. We noticed thatan important characteristic of the CDR dataset, which isthe presence of many entity pairs in a single sentence, tobe a likely cause for the high false positive rate. Considerthe example: Susceptibility to seizures produced bypilocarpine in rats after microinjection of isoniazid orgamma-vinyl-GABA into the substantia nigra. Here,pilocarpine and seizures are positively related, whichis correctly recognized by our classifiers. However, ourclassifiers also associate the disease seizures with thechemicals isoniazid and gamma-vinyl-GABA. The graphexamples corresponding to different entity pairs arisingout of the above sentence, share many common subgraphand are likely to be close enough in the feature space ofthe classifiers. We hypothesize that a sentence simplifi-cation step, that trims the sentences into shorter phrasesspecific to entity-pairs, or a specific treatment of coordi-nation structure in the sentences [28], is likely to reducethe error rates.Another source of errors is in preprocessing. Considerthe following sentence from the PPI corpora: We tookadvantage of previously collected data during a random-ized double-blind, placebo-controlled clinical trial to con-duct a secondary analysis of the RBP/TTR ratio and itsrelationship to infection andVA status.. In cases like these,the tokenization offered as part of the PPI corpora recog-nizes the string RBP/TTR as a single token. This errorin preprocessing causes the corresponding dependencygraph to have a single node with the label RBP/TTR,instead of two different nodes , corresponding to the pro-teins RBP and TTR. Improving preprocessing accuracyis likely to improve the relation extraction performancefor PPI.Future workEnriching edge labelsThe main strength of ASM kernel is that it handlesedge labels distinctly from node labels in the graph. Thisstrength can be exploited by designing informative fea-tures for edges or paths, that are representative of thecorresponding sub-phrase in the sentence, for example,phrase level measurements of sentiment polarity, nega-tion and hedging [29].Panyam et al. Journal of Biomedical Semantics  (2018) 9:7 Page 10 of 11Custom edge similarityASM computes the similarity of shortest paths, based ontheir edge label composition. As the dependency edgelabels have well defined semantics, designing custom sim-ilarity measures between these edge labels is likely toimprove performance. These edge labels are grouped ina well defined hierarchical fashion, which the similarityfunction can exploit. For example, the edge labels vmod(verb modifier) and advmod (adverbial modifier) aremore closely related to each other than to the edge labelnsubj (nominal subject).Semantic matchingASM relies on comparing shortest paths between twoinput graphs, whose start and end nodes have identicallabels. Currently, node labels are set to be word lemmasinstead of tokens, to improve generalization and addressminor variations such as cured and curing. In future,we aim to explore setting node labels to word classes thatgroup words with similar meanings together. For example,node labels may be set to cluster ids, post word cluster-ing. Semantic matching of lemmas using distributionalsimilarity [30], may allow matching different lemmas withsimilar meanings (For example, lemmas such as cure andimprove). Similar approaches to tree kernels [31] hasbeen shown to improve performance.ConclusionWe demonstrated a method for extracting relations thatare expressed in multiple sentences, to achieve a highperformance Chemical-Induced Disease relation extrac-tion, without using external knowledge sources or taskspecific heuristics. We studied the performance of stateof the art tree kernels and graph kernels for two impor-tant biomedical relation extraction tasks, namely theChemical-Induced Disease (CID) relation extraction andProtein-Protein-Interaction (PPI) task. We showed thatthe Approximate Subgraph Matching (ASM) kernel iseffective and comparable to the state of the art All PathGraph (APG) kernel, for CID sentence level relationextraction and PPI extraction from BioInfer dataset. Thedifference in performance between the two kernels isnot significant for CID sentence level relation extraction.However, for the full CID relation extraction and mostother datasets in PPI, ASM is substantially outperformedby the APG kernel.Endnote1 https://bitbucket.org/readbiomed/asm-kernelAbbreviationsASM Kernel: Approximate subgraph matching kernel; APG Kernel: All pathgraph kernel; CID: Chemical-Induced-Disease; PTK: Partial tree kernel; SSTK:Subset tree kernel; SVM: Support vector machineAcknowledgementsWe thank the anonymous reviewers for their valuable suggestions.FundingThis work is supported by the ARC Discovery Project DP15010155. Nagesh C.Panyam is a Graduate research student at the University of Melbourne,Melbourne, Australia and is supported by the Australian Government ResearchTraining Program Scholarship. This research was supported by use of theNectar Research Cloud, a collaborative Australian research platform supportedby the National Collaborative Research Infrastructure Strategy (NCRIS).Availability of data andmaterialsThe CID corpus used in our experiments is made available from theBioCreative-V [3] at the following url: http://www.biocreative.org/tasks/biocreative-v/track-3-cdr. Stanford dependency parser [7] can be installedfrom http://stanfordnlp.github.io/CoreNLP/. The Java based KeLP [22]framework was used for custom graph kernel implementation and can bedownloaded from https://github.com/SAG-KeLP. All software developed forthis paper, including our implementation of the APG and the ASM graphkernels in the Java based Kelp [22] framework, can be accessed from https://bitbucket.org/readbiomed/asm-kernel.Authors contributionsAll authors contributed to the analysis and design of experiments. NCPimplemented the experiment scripts and kernel implementations and wrotethe manuscripts. All authors read and approved the final manuscript.Ethics approval and consent to participateNot Applicable.Consent for publicationNot Applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Received: 15 February 2017 Accepted: 1 December 2017Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 DOI 10.1186/s13326-017-0172-7REVIEW Open AccessMIRO: guidelines for minimuminformation for the reporting of an ontologyNicolas Matentzoglu1*, James Malone2, Chris Mungall3 and Robert Stevens1AbstractBackground: Creation and use of ontologies has become a mainstream activity in many disciplines, in particular, thebiomedical domain. Ontology developers often disseminate information about these ontologies in peer-reviewedontology description reports. There appears to be, however, a high degree of variability in the content of thesereports. Often, important details are omitted such that it is difficult to gain a sufficient understanding of the ontology,its content and method of creation.Results: We propose theMinimum Information for Reporting an Ontology (MIRO) guidelines as a means to facilitate ahigher degree of completeness and consistency between ontology documentation, including published papers, andultimately a higher standard of report quality. A draft of the MIRO guidelines was circulated for public comment in theform of a questionnaire, and we subsequently collected 110 responses from ontology authors, developers, users andreviewers. We report on the feedback of this consultation, including comments on each guideline, and present ouranalysis on the relative importance of each MIRO information item. These results were used to update the MIROguidelines, mainly by providing more detailed operational definitions of the individual items and assigning degrees ofimportance. Based on our revised version of MIRO, we conducted a review of 15 recently published ontologydescription reports from three important journals in the Semantic Web and Biomedical domain and analysed them forcompliance with the MIRO guidelines. We found that only 41.38% of the information items were covered by themajority of the papers (and deemed important by the survey respondents) and a large number of important items arenot covered at all, like those related to testing and versioning policies.Conclusions: We believe that the community-reviewed MIRO guidelines can contribute to improving significantlythe quality of ontology description reports and other documentation, in particular by increasing consistent reportingof important ontology features that are otherwise often neglected.Keywords: Ontologies, Reporting guidelines, Minimum information, Ontology reportingBackgroundThe need for a common understanding of the entities ina field of interest has led to the widespread adoption ofontologies as a means of representing knowledge [1]. Thisis particularly true in biology, medicine and healthcare[1, 2]. We also see the use of semantic technologies,including ontologies, increasing outside research in areassuch as business and commerce; see, for example, the listof PoolParty customers [3]. Ontologies attempt to rep-resent our knowledge such that inclusion of an entity in*Correspondence: nicolas.matentzoglu@manchester.ac.uk1School of Computer Science, University of Manchester, Oxford Road,Manchester, UKFull list of author information is available at the end of the articlea category can be recognised by both humans and com-puters, for example by using automated reasoners. Thedefinitions and descriptions of every entity in a categorymay be done in the form of natural language or logicalaxioms that describe the relationship of one category ofobjects to objects in another category [4]. Groups of dataannotators use ontologies to describe entities; commit-ting to use that ontology seeks to facilitate a commonunderstanding of entities across data sources [1].Several journals regularly publish ontology descriptionreports (ODR), for example, the Semantic Web Journal(SWJ), the Journal of Web Semantics (JWS) and the Jour-nal of Biomedical Semantics (JBMS). An ODR, in thesense of the current focus of our work, is a published,© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 2 of 13peer-reviewed report regarding the development of asingle ontology represented in a formal language such asthe Web Ontology Language (OWL) [5]. Descriptions ofan ontology need not, however, be restricted to tradi-tional papers. The original motivation for developing theMIRO guidelines comes from the perceived inadequacyof ODR in the form of published papers, but the use ofthe MIRO guidelines need not be restricted to traditionalODR. The traditional paper is currently the main routefor reporting about an ontology. This should not be thecase and, just as research publishing is diversifying in itsform, such as through the growth of preprint archivesand moving beyond facsimiles of printed paper publish-ing [6], so should the documentation of an ontology. Anontology itself can and should be the vehicle that dis-seminates information about its development and status.Indeed, it is plausible that at least some of the report-ing could be automated and incorporated in the formof annotations of the ontology. An ontology could, forinstance, carry its own descriptions as part of its anno-tations, and other types of documentation should alsocarry descriptions of the ontology. This is already the casefor datasets that choose to use the W3Cs Vocabulary ofInterlinked Datasets (VoID) ) [7] which describes meta-data about RDF datasets and can be published alongsidethose datasets to act as a bridge between the publishersand users of data.While ODRs certainly vary in scope, there is also ahigh degree of commonality with respect to the pro-cess of ontology engineering. Some commonly recur-ring aspects of the engineering process are, for example,the necessity for some form of knowledge elicitation,formalisation and evaluation. Moreover, there are somecommonly shared attributes of the ontology itself: Everyontology has a size, a degree of coverage and a name.As the goal of ontology authors is usually to publish theontology for community use, another important branchof information items relates to the publishing process,such as licenses, details about the versioning and locationon the web.Unfortunately, and perhaps surprisingly, given thatontologies are a shared conceptualisation about a domain,there is no standard or common understanding aboutwhat should be reported in a good, meaningful ODR orother documentation. As a consequence, the contents ofpublished articles appear highly inconsistent. This can bea problem for a variety of reasons. Reviewers for journalsand conferences faced with large numbers of ODRs haveno reliable guidelines to help them assess the quality of thereports making consistent reviewing between reviewersharder.There are a large number of (domain-specific) agree-ments for example when it comes to novel algorithmiccontributions (the necessity of reporting a performancebenchmark, etc.) but it is still largely unclear how to dis-tinguish reliably between Here ismy ontology ODRs andsubstantial contributions that constitute an advancement,such as in modelling, usage or scope, to their respec-tive community. Another problem concerns the ability ofa potential consumer in understanding what an ontol-ogy is intended to capture. Since traditional ODRs oftenserve as the main documentation of an ontology, they areoften important in building trust among potential usersand outlining the intended applicability of an ontology, asmerely looking at the ontology may be misleading for avariety of reasons (e.g. intended use, degree of complete-ness, etc.).In ontology building, reproducibility is probably a some-what unrealistic goal; given the same motivation andcommunity, it is unlikely the exact same ontology be pro-duced as a result. Knowing themotivation, the communityof interest, the requirements gathered for the ontology,whence the knowledge came to put in the ontology, theaxiom patterns used, testing, evaluation, and so on would,however, appear a priori to be reasonable features to knowabout an ontologys development, along with aspects suchas numbers of classes and so on.Languages such as OWL have annotation propertiesthat support some aspects of ontology description. Edi-tors such as Protégé [8] enable vocabularies such asDublin Core [9] to be imported so that the ontologyand its entities may have dates, creators, descriptions andso on supplied as part of the ontology. Such metadataare, however, insufficient to report on an ontology. Out-side research articles describing ontologies, prior workin describing ontologies has been in the area of ontol-ogy libraries (registries, repositories and so on) [1012]wheremetadata is primarily used for discovery and associ-ated characterisation. TheOntologyMetadata Vocabulary(OMV) [10] and the Metadata for Ontology Descriptionand Publication (MOD) [11] are both ontologies that cap-ture aspects of reporting about an ontology. Both seek topromote ontology discovery and re-use. MOD incorpo-rates many aspects about how an ontology was developedwithin its metadata, such as method, principal classes,and so on and used an open-ended questionnaire togather material. MOD is more extensive than the OMV,though its primary purpose is still discovery motivated bypromoting re-use.The Minimum Information for the Reporting of anOntology (MIRO) guidelines aim to guide that whichis reported in narratives reporting an ontology such asODR, as well as other documentation. As such, MIROis likely to be more extensive than these vocabularies forontology metadata, despite covering some of the sametopics.We take a broader perspective than discovery, lookingat what needs to be reported about an ontology such thatMatentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 3 of 13its development and status can be understood. Just asthe MOD provides more than the OMV, we expect thatthe MIRO extends the content of the MOD. Our objec-tive is not to create a new ontology, but to establish a setof guidelines for that which should be reported about anontology. These guidelines may be captured in an ontol-ogy itself or other documentation, but the main purposeof MIRO is for use to guide authors and reviewers ofpapers about ontologies.The main contributions of this paper are as follows: The Minimum Information for the Reporting of anOntology (MIRO) guidelines, which have the aim ofimproving the quality and consistency of theinformation content of ontology descriptions. A survey with more than 100 respondents to evaluateand refine the guidelines. We present the results ofthis survey and use participant ratings to prioritiseinformation items by importance. A systematic review of the compliance of recent,high-quality ODR published as papers with theMIRO guidelines.Given the prominence of ontologies in the bio-healthcommunity, the MIRO guidelines are of particular inter-est; they are, however, of general applicability to anyontology description.Materials andmethodsMIRO guideline developmentThe first version of the MIRO was written by a teamof three ontology experts (authors of this paper, exceptMatentzoglu). All three experts have extensive knowl-edge of building ontologies, reviewing and authoringODRs, managing ontology collections such as the OBOfoundry [13] and organising international ontology-related conferences and workshops such as SemanticWeb Applications and tools for the Life Sciences, theInternational Conference on Biomedical Ontology andthe Bio-Ontologies SIG at the Intelligent Systems forMolecular Biology conference. One of the motivationsfor producing the guidelines stemmed from the difficultyof setting reviewing standards when acting as conferencechairs.Apart from extensive expert knowledge in the ontologydomain, the original MIRO information items also tookinput from reviews of existing ontology metadata vocabu-laries, the OBO principles [14], and fruitful discussions atevents, such as the yearly Ontology Summit [15] and theUK Ontology Network [16].After gathering an informal list of best practices onreporting, the three experts reviewed each of theminternally and organised them into cognate sections,which resulted in the first MIRO draft.Survey on the importance of ontology reportinginformation itemsThe first draft of the MIRO guidelines was offered toa broad community of ontology paper authors, review-ers, developers and users via a typeform survey [17].Broadly, the survey had seven sections: (1) basic ontol-ogy facts such as the name and URL, (2) motivationfor why the ontology was being developed, (3) scope,requirement and community for which the ontology wasbeing developed, (4) knowledge elicitation around howthe knowledge included was extracted, (5) ontology con-tent describing technical facts of the ontology such asnumber of classes, properties, (6) managing change onhow the ontology is maintained, and, (7) quality assurancearound testing and evaluation.In this survey, we presented (1) the information item,such as Ontology name or Ontology evaluation, (2) aLikert scale between 1 (unimportant) and 5 (very impor-tant) to rate the subjective relative importance of eachitem to the respondent and (3) a comment field. Impor-tantly, we did not provide a detailed operationalisationof the MIRO items, i.e. details on how we envisioneda particular information item to be realised in a givenODR or other documentation. We did this for two rea-sons: (1) We wanted to provide an opportunity for thecommunity to present their position on how a particularitem should be realised without too much upfront bias;(2) Some of the items, such as Testing and Evaluationcan mean significantly different things across cases. Aswe wanted to avoid the impression that we only spec-ified certain cases, we did not include descriptions ofhow a guideline would be operationalised. In additionto comment fields on each item, we asked, for everysection of MIRO, which items or aspects of items wereof particular importance to the respondent. Our goalwas to create a more detailed characterisation of impor-tant items and to use this information to further specifythe operationalisation of information items in the finalMIRO guidelines, in particular, to emphasise importantdetails in the item description. Towards the end of thesurvey, we asked the respondents for the single mostimportant criterion when deciding on whether to use anontology.Participants viewed ontologies and ontology papersfrom a variety of perspectives for which different infor-mation items may be important to different degrees. Toaccount for these differences in our analysis, we askedparticipants to indicate their main roles (multiple roleswere permitted), i.e. ontology developer, ontology user,reader of papers on ontologies and reviewer of papers onontologies. We furthermore collected information on therespondents professional background, i.e. whether theywere student, academic employee, public sector/not-for-profit employee, private sector employee or Other.Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 4 of 13The questionnaire was sent to email lists read by a widevariety of actors involved with ontologies that would havean interest in how ontologies are reported. Email lists werenot limited to only those used by biological and medi-cal ontology developers and users, but to a range of listsreaching a range of domains and technologies. The listsused were: The Protég´e User email listprotege-user@mailman.stanford.edu. The Open Biomedical Ontologies Discussion listobo-discuss@lists.sourceforge.net. The Health Care and Life Sciences Semantic WebDiscussion list public-semweb-lifesci@w3.org. The Semantic Web email list semantic-web@w3.org. The Web Ontology Language email listpublic-owled@w3.org. The UK Ontology Network email listontology-uk@googlegroups.com. The European Ontology Network email listeuon@googlegroups.com. The European Bioinformatics Institute ontologymailing list ontology@ebi.ac.uk.After the survey had closed, we analysed the results inthe following way: We calculated descriptive statistics for theimportance ratings given to each item. We coded the comments for each item to determineinformation items of particular interest torespondents and elicit information items potentiallybeyond the current coverage of MIRO. Thecomments were coded in a bottom-up fashion, byfirst collecting the information items mentioned ineach comment in a list, then reconciling potentiallyredundant terminology and finally grouping thecomments into categories. We analysed the responses to our question for thesingle most important criterion in the same way asthe comments.Systematic review of MIRO complianceTo determine to what extent current high-quality ODRswould have adhered to the MIRO guidelines, we per-formed a systematic review of MIRO compliance [18].We selected three important journals that regularly pub-lish high-quality ODR AS : the Semantic Web Journal(SWJ), the Journal of Web Semantics (JWS) and the Jour-nal of Biomedical Semantics (JBMS). We decided to focuson recently published work and restricted our search topapers published between March 2015 and May 2016.This time frame was chosen for convenience, to ensurethat our sample contained between 15 and 20 relevantODRs.First, we retrieved all research papers (168) publishedby the three journals within the time frame and had threeindependent researchers filter out obviously unrelatedtitles according to the following inclusion criteria: (1)ontology description paper and (2) primarily about anontology and its development (where 1 simply describesan ontology and 2 extends this scope with a description ofits development and use) and according to the followingexclusion criteria: (i) primarily system USING ontology,(ii) review about ontologies, (iii) primarily a use casedescription (study on how the ontology generated value),(iv) an update or extension of an existing ontology. At thisstage, we considered all those papers that were thoughtpotentially relevant by at least one reviewer (36). In thesecond phase, three independent researchers reviewedthe abstracts of the remaining papers, after which 19papers remained. In the last phase, two independentresearchers reviewed the remaining papers in depth,which resulted in the exclusion of another 4 papers. Thefinal set of 15 papers was coded according to the 35information items of MIRO. All codes except for ontologyname and ontology owner, which were coded on a three-point Likert scale (absent, mentioned, explicit)here,explicit means that the description of an informationitem in the paper was present in the narrative withexplicit indicators such as the motivation for developingthis ontology was . . . , were coded simply with absentand present. Note that many information items suchas coverage or need can be addressed in a varietyof ways and to varying levels of detail. The goal of thisreview was not to determine the quality of the papers,which would require a coding granularity covering theseaspects, but merely to see whether certain items arecovered at all.Ontology development reporting guidelinesIn the following, we call MIRO the document thatdescribes the guidelines, information item a particularitem in the guidelines such as Ontology name or Ontol-ogy coverage and section a block of items that belong toa single cognate category such as Quality Assurance orMotivation. An information item consists of a (1) label,such as Ontology name, (2) a description with a defini-tion and details on the operationalisation, (3) a level ofimportance using the RFC 2119 keywords often used bythe W3C [19] MUST, SHOULD and OPTIONAL and (4)an example or a reference to an example.The guidelines are divided into reporting areas, eachwith a list of guidelines. The MIRO guidelines in its cur-rent state, 5 March 2017, are presented below. For spacereasons, we omit the example text here. It can be found inthe official guidelines on GitHub [20]. Information itemsmarkedwith an asterisk were introduced as a consequenceof the survey responses.Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 5 of 13A. The basicsA.1 Ontology name (MUST): The full name of theontology, including the acronym and the versionnumber referred to in the report.A.2 Ontology owner (MUST): The names,affiliations (where appropriate) and contact details ofthe person, people or consortium that manage thedevelopment of the ontology.A.3 Ontology license (MUST): The licence whichgoverns the permissions surrounding the ontology.A.4 Ontology URL (MUST): The web locationwhere the ontology file is available.A.5Ontology repository (MUST): The web location(URL) of the version control system where currentand previous versions of the ontology can be found.A.6 Methodological framework* (MUST):A name or description of the steps taken to developthe ontology. This should describe the overallorganisation of the ontology development process.B. MotivationB.1 Need (MUST): Justification of why the ontologyis required.B.2 Competition (MUST): The names and citationsfor other ontology or ontologies in the same generalarea as the one being reported upon, together with adescription on why the one being reported is neededinstead or in addition to the others.B.3 Target audience (MUST): The community ororganisation performing some task or use for whichthe ontology was developed.C. Scope, requirements, development community (SRD)C.1 Scope and coverage (MUST): The domain orfield of interest for the ontology and the boundaries,granularity of representation and coverage of theontology. State the requirements of the ontology,such as the competency questions it should satisfy. Avisualisation or tabular representation is optional, butoften helpful to illustrate the scope.C.2 Development community (MUST): Theperson, group of people or organisation that actuallycreates the content of the ontology. This is distinctfrom the Ontology Owner (above) that is concernedwith the management of the ontologys development.C.3 Communication (MUST): Location, usuallyURL, of the email list and/or the issue trackingsystems used for development and managing featurerequests for the ontology.D. Knowledge acquisition (KA)D.1 Knowledge acquisition method (MUST): Howthe knowledge in the ontology was gathered, sorted,verified, etc.D.2 Source knowledge location (SHOULD); Thelocation of the source whence the knowledge wasgathered.D.3 Content selection (SHOULD): Theprioritisation of entities to be represented in theontology and how that prioritisation was achieved.Some knowledge is more important or of greaterpriority to be in the ontology to support therequirements of that ontology.E. Ontology contentE.1 Knowledge Representation language (MUST):the knowledge representation language used and whyit was used. For a language like OWL, indicate theOWL profile and expressivity.E.2 Development environment (OPTIONAL): Thetool(s) used in developing the ontology.E.3 Ontology metrics (SHOULD): Number ofclasses, properties, axioms and types of axioms, rulesand individuals in the ontology.E.4 Incorporation of other ontologies (MUST):The names, versions and citations of externalontologies imported into the ontology and wherethey are placed in the host ontology.E.5 Entity naming convention (MUST): Thenaming scheme for the entities in the ontology,capturing orthography, organisation rules, acronyms,and so on.E.6 Identifier generation policy (MUST): What isthe scheme used for creating identifiers for entities inthe ontology. State whether identifiers aresemantic-free or meaningful.E.7 Entity metadata policy (MUST): Whatmetadata for each entity is to be present. This couldinclude, but not be limited to: A natural languagedefinition, editor, edit history, examples, entity labeland synonyms, etc.E.8 Upper ontology (MUST): If an upper ontologyis used, which one is used and why is it used? If notused, then why not?E.9 Ontology relationships (MUST): Therelationships or properties used in the ontology,which were used and why? Were new relationshipsrequired? Why?E.10 Axiom pattern (MUST): An axiom pattern is aregular design of axioms or a template for axiomsused to represent a category of entities or commonaspects of a variety of types of entities. An axiompattern may comprise both asserted and inferredaxioms. The aim of a pattern is to achieve aconsistent style of representation. An importantfamily of axiom patterns are Ontology Design pattern(ODP) which are commonly used solutions for issuesin representation.Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 6 of 13E.11 Dereferencable IRI* (OPTIONAL): Statewhether or not the IRI used are dereferenceable to aWeb resource. Provide any standard prefix (CURIE).F. Managing changeF.1 Sustainability plan (MUST): State whether theontology will be actively maintained and developed.Describe a plan for how the ontology will be kept upto date.F.2 Entity deprecation strategy (MUST): Describethe procedures for managing entities that becomeremoved, split or redefined.F.3 Versioning policy (MUST): State or makereference to the policy that governs when newversions of the ontology are created and released.G. Quality Assurance (QA)G.1 Testing (MUST): Description of the procedureused to judge whether the ontology achieves theclaims made for the ontology. State, for example,whether the ontology is logically consistent, answersthe queries it claims to answer, and whether it cananswer them in a time that is reasonable for theprojected use case scenario (benchmarking).G.2 Evaluation (MUST): A determination ofwhether the ontology is of value and significance.An evaluation should show that the motivation isjustified and that the objectives of the ontologysdevelopment are met effectively and satisfactorily.Describe whether or not the ontology meets itsstated requirements, competency questions andgoals.G.3 Examples of use (MUST): An illustration of theontology in use in its an application setting or usecase.G.4 Institutional endorsement* (OPTIONAL);State whether the ontology is endorsed by the W3C,the OBO foundry or some organisation representinga community.G.5 Evidence of use* (MUST): An illustration ofactive projects and applications that use the ontology.ResultsWe sent our first call for participation in the survey onthe MIRO guidelines proposal on 11th April 2016 andclosed the survey on 12th May 2016. After two weeks fromthe announcement, reminder emails were sent out to theselected email lists. There were 110 responses in total tothe survey. This large number of responses gives us a goodlevel of confidence of a reasonable representation from theontology community. The R analysis documentation forthe survey data can be found at [21].Demographics of respondersFigure 1 shows the jobs responders declared. The high-est responders were academic employees (76 out of 110)with the second largest group being public sector/not-for-profit employees (12). From the top level domains (TLD)of the email addresses given by the responders, we createda geographical profile (Fig. 1, right). We witnessed a broadspread of TLDs, which indicates that our advertisementstrategymade the survey widely visible. Figure 2 shows theroles declared by responders. Almost half of the respon-dents (44%) reported to act in all 5 roles, with another12% acting in all roles except reviewer of ontology papers.From the correlation matrix on the right, there appear tobe roughly threemajor groups of responders: (1) users andreaders, (2) paper authors, reviewers and readers and (3)developers and authors.Importance of MIRO information itemsFigure 3 shows the mean importance rating given to eachMIRO information item. Inspection of the figure showsthree major step changes in the importance that we havemapped to categories of importance: features that mustbe given; those that should be given; and those that areoptional as to whether or not they are given. The major-ity of the MIRO information items are deemed to bemandatory: Only the editor used for creating the ontology(optional), the location of the source knowledge (should),the ontology content selection (should) and the basicontology metrics (should) were not. That ontology met-rics were thought of as having lower importance wasFig. 1 Demographics of respondents. Left: Jobs of respondents, overall counts. One job per respondent. Right: Institutional spread of respondents,overall counts of email top level domain. One email per respondentMatentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 7 of 13Fig. 2 Demographics of respondents. Left: Roles of respondents, overall counts. Multiple roles per respondent. Right: Correlation matrix for roles ofusers. The darker, the more highly correlatedsurprising to us, as metrics are a relatively simple mech-anism for communicating (for example) the scale of theontology, and complexity of the modelling (for example inthe form of a breakdown of axiom type counts, or simpleOWL 2 profile memberships) and can usually be auto-matically computed. Another low priority item was theexplanations of how the content was selected, i.e. howthe entities and classes were chosen that should be partof the ontology. In practice, they are often implicit in therequirements of the ontology and are perhaps thereforedeemed of lower importance.Table 1 shows the descriptive statistics of all 30 originalinformation items, sorted by standard deviation. Thestandard deviation can be seen as a measure of dis-agreement: the lower it is, the more respondents agreedon a rating. It is notable that the standard deviation isstrongly negatively correlated (-0.85) with the mean: Thehigher the average rating, the lower the disagreement. Forexample, basics such as the URL and the ontology namehave very high mean ratings and the lowest standarddeviations among all items. For items like ontology met-rics, such as class and property counts, and the editingtool with which the ontology was built, receive low meanratings, and have the highest standard deviations; they arevery important to a handful of people but unimportantto others.Fig. 3Mean rating for each information item. Vertical lines correspond to importance level (optional, should, must)Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 8 of 13Table 1 Descriptive statistics of all information items in MIRO(mean, median, standard deviation)MIRO item Rank Mean Med SDBasics: Ontology URL 1 4.72 5 0.68Basics: Ontology name 2 4.71 5 0.70Basics: Ontology license 4 4.50 5 0.79SRD: Scope and coverage 6 4.15 4 0.84SRD: Development community 25 3.77 4 0.86Basics: Ontology owner 3 4.53 5 0.87Content: Ontology relationships 7 4.13 4 0.88Content: Incorporation of other ontologies 9 4.09 4 0.95Motivation: Target audience 13 3.94 4 0.96Content: Axiom patterns 24 3.80 4 0.96QA: Examples of use 5 4.19 5 0.99KA: Knowledge acquisition methodology 14 3.93 4 0.99Content: Entity metadata policy 16 3.89 4 1.02Content: KR language 8 4.11 4 1.03Content: Upper ontology 17 3.88 4 1.03Change: Versioning policy 23 3.80 4 1.03QA: Testing 18 3.87 4 1.04KA: Content selection 28 3.38 4 1.04Content: Entity naming convention 26 3.74 4 1.04Basics: Ontology repository 10 4.01 4 1.04Change: Entity deprecation strategy 21 3.83 4 1.07Motivation: Competition 12 3.96 4 1.07Motivation: Need 20 3.85 4 1.08Content: Identifier generation policy 19 3.86 4 1.08QA: Evaluation 11 3.99 4 1.08SRD: Communication 22 3.80 4 1.09Change: Sustainability plan 15 3.89 4 1.09KA: Source knowledge location 29 3.36 3 1.09Content: Ontology metrics 27 3.42 3 1.18Content: Development environment 30 2.88 3 1.30Abbreviations: SRD Scope, requirements, development community, QA Qualityassurance, KA Knowledge acquisition, medMedian, sd standard deviationData is sorted by standard deviation (sd) in order to highlight the items that had thelargest disagreementWe have ranked the information items shown in Fig. 3from 1 to 30, with 1 being the most important feature(i.e. the one that received the highest mean rating) and30 being the least important. Apart from the overall rank-ing (see Table 1), we computed the ranking for each userrole separately, to find differences in relative importance.We will report this difference in what follows by the dif-ference in rank compared to the overall rank, mentioningonly those items that deviate by at least 4 positions in theranking. For example, authors of ontology papers are lessinterested in the knowledge acquisition methodology (-7)than the mean. Indeed, if only the scores of respondentsthat are ontology authors are considered, the MIRO itemknowledge acquisition methodology would fall 7 placesin rank order (from position 14 to 21).Depending on the role of the respondents, some rat-ings differed markedly in their overall rank. Apart fromthe above-mentioned uninterest in the knowledge acqui-sition methodology, authors of ontology papers are lessinterested in the identifier generation policy (-6) and thereference to the repository holding the ontology (-4) com-pared to the overall ranking. On the other hand, they aremore interested in upper ontologies (6) and the commu-nity that is being engaged to develop the ontology (6) thanall of the other groups.Developers are less interested in reporting about test-ing (-6), while they care more about the sustainabilityplan (5) and an entity deprecation strategy (6) than themean. That ontology developers rank testing so low (rank24 of 30 items on the developers ranking) is, at leastto us, worrying. Testing should be a critical part of thedevelopment lifecycle, and reporting on the results of thistesting is crucial for increasing confidence in potentialusers. Reviewers, like authors, find the knowledge acqui-sition methodology less important than the mean (-6),while they, perhaps surprisingly given their role, ascribedconsiderably more importance to information on why theontology is needed (5). Ontology users care less than therest about which upper ontologies are used (-4) but rankthe entity deprecation strategy (7) higher than the mean.Lastly, readers of ontology papers are less interested tolearn about the knowledge acquisition methodology (-6)as well as details on the entity metadata policy (-5) com-pared to themean, while caringmore than themean aboutthe sustainability plan (5) and the entity deprecation strat-egy (5) - the latter two items both associated broadly withconsiderations for planning and risk.Analysis of commentsThe MIRO as presented in the survey was restricted tovery short descriptions, often only a simple label, and didnot provide details on the individual information items.In the draft, the authors had provided some operational-isation for the information items. These were omittedfrom the survey to encourage as many comments aboutthe items as possible. As a result of this feedback, weadded four new information items to the original draft(for details, see Ontology development reporting guide-lines section): Methodological framework Dereferenceable IRIs Institutional endorsement Evidence of useMatentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 9 of 13Furthermore, we used the feedback to improve the oper-ationalisation of the MIRO items, as well as perform someminor changes to the item labels.In the following, we present our somewhat ancillaryanalysis of the most frequently discussed topics in thecomment fields (see Materials and methods for details).Note that these categories do not always correspond toMIRO items. The reason for that is that we did not wantto deviate too much from the labelling of the responders.Our goal was to capture areas of personal concern to theresponders and establish a secondary metric of impor-tance that is orthogonal to our main metric (the ratings)and provides further insight.As can be seen in Table 2, the information item thoughtto be most important was that of coverage and scope. Putcrudely, this is what does the ontology claim to cover?and in what detail is it covered by the ontology?. Thisaspect of ontology reporting directly corresponds to theMIRO item SRD: Scope and coverage, which is placedsixth in the main ranking (see Table 1); the highest rank-ing, just after the basics: ontology URL, name, owner andlicense and QA: Example of use. The latter is also theinformation item that correspondsmost closely to the sec-ond most important topic, Use Case, with 18 mentions.Responders were very interested for the report to revealexactly the scenario for which the ontology was designed,to decide whether it is likely to fit their own. This inter-est was further reflected by the topic Evidence of use,which subsequently received its own MIRO item; peopleseem to just want to be reassured that the ontology wasnot merely developed for its own sake (or purely academicreasons) before they give it their trust and apply it to theirown scenario.The active community topic (third most frequentlymentioned topic) corresponds most closely to the MIROTable 2 Analysis of the comments on what is most importantTopic CountScope and coverage 23Use case 18Active community 16Content 11Publishing and life cycle 10Interoperability 9Metadata and documentation 8Representation 8Evidence for use 7Usability 5Other 4All comments were coded and grouped into topics. The counts on the right are thetotal number participants mentioning an item belonging to the groupitem SRD: Development community, which is ranked25th according to importance. The discrepancy here canbe explained by the sentiment of the comments mention-ing the active community: the emphasis of the com-ments was on the community being active, rather than adescription of the creators of the community. The impor-tance of the community being active was perhaps not somuch regarding the communitys role as an informationitem in a report, rather than a key selection criterion forwhether or not to use the ontology reported upon [22].Wefeel that most of this ancillary comment analysis should beviewed in the light of this: often what should be reportedand what should be the case. These are two very differentquestions in practice but are perhaps interpreted as thesame by our survey responders.The remaining, slightly less important topics of inter-est do not exactly correspond to individual MIRO items.Publishing and life cycle for example relates to aspectsof the MIRO Managing change section, as well as otherinformation items such as the URL, the communicationinfrastructure and the repository. However, they all relateto practical considerations: pieces of information that helpusers to decide whether or not to employ the ontology.Is it represented in a format I can use (representation)?How easy is it to use in my scenario (Usability)? Howgood is the ontology document (metadata and documen-tation)? The topic of content, the fourth most frequentlymentioned after active community is very vague, butmost probably expresses the sentiment of the respondersthat it is important to them what is actually in the ontol-ogy; in terms of our issue of what is important to reporton, this means that good descriptions of what is repre-sented in the ontology are critical, andperhapsoftenneglected.Compliance of existing papers with MIRO guidelinesTable 3 shows the 15 papers that were selected for inclu-sion into the review process (for methodological details,see Materials and methods).Compliance is defined as the number of papers thatmen-tion a MIRO item divided by the overall number of papers.We define the following compliance level categories: If thecompliance is <20%, we consider it very low (V), =20% and <50%, we consider it low (L), =50% and <80%, we consider it medium (M), >=80% we consider it high (H).The rating levels optional (O), should (S) and must(M) are defined in Importance of MIRO informationitems section. Table 4 shows, for each of the MIROinformation items the compliance contrasted with the rat-ings from the ontology survey and the compliance-ratingsMatentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 10 of 13Table 3 Reviewed papersTitle Journal YearLOTED2: An Ontology of European PublicProcurement Notices [27]SWJ 2016PPROC, an Ontology for Transparency inPublic Procurement [28]SWJ 2016Overview of the MPEG-21 Media ContractOntology [29]SWJ 2016The Document Components Ontology(DoCO) [30]SWJ 2016The Data Mining OPtimization Ontol-ogy [31]JWS 2015My Corporis Fabrica Embryo: An ontology-based 3D spatio-temporal modeling ofhuman embryo development [32]JBMS 2015Development of an Ontology for Peri-odontitis [33]JBMS 2015Developing VISO: Vaccine InformationStatement Ontology for patient educa-tion [34]JBMS 2015Development and application of an inter-action network ontology for literaturemining of vaccine-associated gene-geneinteractions [35]JBMS 2015The cellular microscopy phenotype ontol-ogy [36]JBMS 2016The Non-Coding RNA Ontology (NCRO):a comprehensive resource for the unifica-tion of non-coding RNA biology [37]JBMS 2016OBIB-a novel ontology for biobanking JBMS 2016VICO: Ontology-based representationand integrative analysis of VaccinationInformed Consent forms [38]JBMS 2016MicrO: an ontology of phenotypic andmetabolic characters, assays, and culturemedia found in prokaryotic taxonomicdescriptions [39]JBMS 2016Representing vision and blindness JBMS 2016Towards exergaming commons: compos-ing the exergame ontology for publishingopen game data [40]JBMS 2016An ontology for major histocompatibilityrestriction [41]JBMS 2016Journals are Semantic Web Journal (SWJ), Journal of Biomedical Semantics (JBMS)and Journal of Web Semantics (JWS)factor (CRF). The compliance-rating factor comprises twoletters, the first of which corresponds to the rating leveland the second to the compliance level. For example, MHstands for a must (M) rating with high (H) compliance.The first observation to be made is that a large proportionof MIRO items fall under the MH category (13 out of 30,43.33%). We need to remember, however, that the surveyonly assessed whether an item was covered at all, so noconclusions can be derived on how well these items wereTable 4 MIRO items ordered by compliance (COM), includingthe rating (RAT) from the ontology surveyMIRO item RAT COM CRFSRD: Scope and coverage 4.15 100.00 MHContent: KR language 4.11 100.00 MHMotivation: Target audience 3.94 100.00 MHMotivation: Need 3.85 100.00 MHContent: Axiom patterns 3.80 100.00 MHBasics: Ontology URL 4.72 93.33 MHContent: Ontology relationships 4.13 93.33 MHSRD: Development community 3.77 93.33 MHBasics: Ontology name 4.71 90.00 MHQA: Examples of usage 4.19 86.67 MHContent: Incorporation of other ontologies 4.09 86.67 MHMotivation: Competition 3.96 80.00 MHKA: Knowledge acqu. methodology 3.93 80.00 MHContent: Ontology metrics 3.42 80.00 SHContent: Development environment 2.88 73.33 OMQA: Evaluation 3.99 66.67 MMContent: Upper ontology 3.88 66.67 MMKA: Content selection 3.38 66.67 SMBasics: Ontology owner 4.53 53.33 MMBasics: Ontology repository 4.01 53.33 MMSRD: Communication 3.80 40.00 MLContent: Entity metadata policy 3.89 33.33 MLBasics: Ontology license 4.50 26.67 MLQA: Testing 3.87 26.67 MLContent: Entity naming conventions 3.74 26.67 MLKA: Source knowledge location 3.36 26.67 SLContent: Identifier generation policy 3.86 6.67 MVChange: Versioning policy 3.80 6.67 MVChange: Sustainability plan 3.89 0.00 MVChange: Entity deprecation strategy 3.83 0.00 MVThe compliance-rating factor (CRF) is described in Compliance of existing paperswith MIRO guidelines sectioncovered by the original papers. The second most impor-tant category is ML (must rating, low coverage) with5 out of 30 items (16.67%), followed by MM (mediumcoverage) and MV (very low coverage) with 4 items(13.33%).We believe the ML and MV categories to be the mostimportant ones to consider, as they represent the high-est discrepancy between what readers wish to see in apaper compared to what they would actually find. Thefour items in the MV category, the identifier generationpolicy, the versioning policy, the sustainability plan andthe entity deprecation strategy all concern aspects of theMatentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 11 of 13ontology lifecycle. It is perhaps less surprising that theidentifier generation policy and the deprecation strategyare rarely mentioned at all: they may either be taken forgranted (perhaps implicitly by referring to the complianceto OBO principles) or simply not be applicable, for exam-ple in cases where the scope of the ontology is small andwell-defined, which would render the use of identifiersunnecessary, as it would a bespoke deprecation strategy.The other two items, however, versioning and sustainabil-ity plan, are applicable to all ontologies, and neglectingto give the reader a sense of them can easily lead to theimpression that the development of the ontology is a one-off, zero maintenance, in some cases even throw-awayprototype case study. In our opinion, this is a wide-spreadproblem even beyond the scope of this review, and findsanother confirmation in the fact that just about half of thereviewed papers explicitly referred to a versioned repos-itory such as GitHub (53%) and less than half mentionsomething like issue tracking or email lists (40%).The items in the ML category are the entity meta-data policy (33.33% coverage), an explicit mention of thelicense under which the ontology may be used (26.67%),means of communication such as email lists and issuetracking (40%), an explicit naming convention for entities(26.67%) and an explicit testing strategy (26.67%). Again,most of these metrics concern the management of theontology life-cycle.Noteworthy is the low compliance on the testing item.Testing differs from an evaluation in that it is not con-cerned with the question of whether the ontology inprinciple does its job (this would be the evaluation, forexample through a use case study), but a systematicattempt to capture non-functional aspects of the ontol-ogy, such as performance (for example classification timewhen reasoning is required) or correctness of the hierar-chy after modelling or mapping with other ontologies, etc.At the very least, we feel, it should be stated whether ornot the ontology is parseable by the usual tools, like theJena API [23] or the OWL API [24], Protégé or OBO-Edit.DiscussionIn this paper, we addressed the problem of what to reportupon in ontology description reports and potentiallyother documentation, including in the ontology itself.There are several actors in this scenario, for example: (1)paper or documentation authors writing up the reportthat need guidance on what aspects of the ontologydevelopment process to cover, (2) ontology users (which,among our survey respondents, frequently coincide withreaders of ontology papers) that need guidance for theontology selection process and (3) ontology developersthat are just about to start development that requirea checklist for recording the forthcoming development.Many of the respondents to our survey adopt a broadrange of ontology-related roles. How ontologies arereported needs to satisfy actors playing all of these roles.We were gratified by the large number of responseswe received to our survey in a relatively small period oftime. Even after the survey was formally closed, we keptreceiving responses, which suggests that the issue of whatshould be reported about an ontology is of significantinterest in the community. With the 110 responses usedin this study, we think the survey is representative of thecommunity; indeed, the number of responses approxi-mates the number of people attending bio-ontology meet-ings such as the Bio-Ontologies COSI at the ISMB and theInternational Conference on Biomedical Ontology.The vast majority of the MIRO guidelines have theimportance designation of must. This may appear oner-ous, but the MIRO guidelines are a minimal list ofthat which should be reported. Being minimal indicatesthat the MIRO information items are intrinsically thosethat are most important. Thus, a claim of compliance withthe MIRO guidelines should mean that the ontology isreported well. Besides, our importance designations aredriven by the data supplied in the survey; irrespective ofany possible response biases [25] and we have trusted thedata.A methodological problem we faced during the papercoding was to judge whether a code was sufficiently cov-ered when it is only implicitly mentioned. For example,items such as scope and need are very hard to not cover atall. That is why the compliance of the MIRO items SRD:Scope and coverage and Motivation: Need was 100%(remember that this does not mean they were coveredwell, only that they were at least mentioned). Coding suchitems was, however, sometimes challenging as they werenot explicitlymentioned, for example by saying The needof this ontology emerged from. . .  or The ontology coversall categories of. . . . As reviewers, we would have liked suchexplicit statements, and it is likely that readers of ontol-ogy papers would also benefit from clarity resulting fromstating information items explicitly.The most important categories were around ontologyscope and coverage, and this is perhaps unsurprising.Apart from this, a category of very high concern tothe community (as reflected for example by Analysis ofcomments section) was the area of publishing and ontol-ogy life-cycle related issues. Such issues touch on someMIRO items such as the sustainability plan, versioningpolicy and repository location. We found that this areais frequently absent in high-end ODR; the three itemswith the lowest compliance all fall under this category.We believe that in some cases, this may point to theintention of the ontology developers to produce a one-off product rather than produce a continuously main-tained knowledge artefact. Our recommendation fromthis work is that authors should make these parts explicitMatentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 12 of 13in the report. If an ODR suggests that it provides animportant service, especially if positioning as a refer-ence to be widely adopted by a community, a descriptionof the sustainability plan must be included. However,it is also possible that ontology developers are simplynot entirely conscious about how important such aspectsare for deciding whether or not to employ an ontology.Most ODR focus on ontology content and knowledgeacquisition rather than aspects of the development life-cycle that become relevant only after the first draft ofthe ontology is published. The MIRO guidelines and theanalysis we presented should help to improve awarenessregarding such information items and their importanceto users.Community feedback was an integral part of the devel-opment process of theMIRO guidelines. Not only were weable to derive categories of importance from the ratings;we were also able to identify four new categories that werenot covered by the firstMIRO draft.We further used com-munity feedback to improve the definitions and labels ofthe information items. For theMIRO guidelines to have animpact on the quality of ontology papers, we believe thatMIRO and projects similar to it should be community-driven to reach the highest degree of consensus possible,in much the same way as the ontology community hasdeveloped some of the most popular ontologies.In the future, it should also be possible to extend MIRObeyond guidelines for reporting in text to a more struc-tured form. Such a form would enable the metadatareportingMIRO to be accessed programmatically inmuchthe same way as approaches such as VoID [7] have suc-cessfully taken. Besides, a W3C working group, similar tothat pursued by the VoID authors, to further the estab-lishment of a structured form of MIRO could help MIRObecome a more widely adopted method used for publish-ing ontologies in literature and on the web. Those involvedin developing and using ontologies should be involvedin such an effort, but an important additional partici-pant would be the maintainers of ontology libraries andrepositories such as the OBO Library, BioPortal and theOntology Lookup Service (OLS). Adoption and publishingof MIRO alongside ontologies in these repositories wouldbe a valuable asset when considering the suitability of anontology for use.As well as structured, computationally amenable report-ing, it should also be possible to derive some aspectsof the MIRO guidelines programmatically. Obvious casesinclude numbers of entities in an ontology, relationshipsused, location, licence and location etc. Programmaticallyextracting axiom patterns is more difficult, but attemptshave been made such as extracting syntactic regulari-ties from ontologies as proxies for axiom patterns [26],which finds syntactic regularities in ontologies. Withsuch computational support, creating sound, up-to-datedescriptions of an ontology in accordance with the MIROguidelines becomes easier.ConclusionsAppropriate reporting of ontologies and ontology devel-opment processes is important for the understanding ofthose ontologies. To this end, we have created a set of min-imum information guidelines for ontology reports uponwhich we have gathered input from the ontology com-munity. The method we have used to develop the MIROguidelines give confidence that they are well supported.We learned which information items are of particularimportance to the community, and we learned where thecurrent reporting is lacking. TheMIRO guidelines need tobe an evolving reporting standard, especially with respectto how each of the reporting items is operationalised; wewelcome continuous input on the MIRO guidelines [20].We recommend the MIRO guidelines to both ontologyusers, authors and reviewers in the ontology communityto improve the presentation of their work.AbbreviationsCRF: Compliance-ratings factor; IRI: International resource identifier; JBMS:Journal of biomedical semantics; KA: Knowledge acquisition; MIRO: Minimuminformation for reporting an ontology; MOD: Metadata for ontologydescription and publication; OWL: Web ontology language; ODP: Ontologydescription paper; OMV: Ontology metadata vocabulary; OBO: Openbiomedical ontologies; QA: Quality assurance; SWJ: Semantic web Journal;SRD: Scope, requirements, development community; TLD: Top level domain;VOID: Vocabulary of interlinked datasets; WSJ: Journal of web semantics; W3C:World Wide web consortiumAcknowledgementsThe authors would like to thank the ontology community for the time theytook to respond to our extensive questionnaire and their valuable feedback.The participation of RS and NM in this work has been funded by the EPSRCproject:WhatIf: Answering What if. . .  questions for Ontology Authoring, EPSRCreference EP/J014176/1.FundingNM was funded by the EPSRC project:WhatIf: Answering What if. . .  Questionsfor Ontology Authoring, reference EP/J014176/1.Availability of data andmaterialThe datasets generated and/or analysed during the current study are availablein the MIRO GitHub repository, https://github.com/owlcs/miro/tree/master/supplementary, as well as on Zenodo, https://doi.org/10.5281/zenodo.398804and http://rpubs.com/matentzn/miro.Authors contributionsJM, CM and RS conceived the study and drafted the guidelines. NM did theanalysis of the data and led the writing supported by the other authors. Allauthors read and approved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Matentzoglu et al. Journal of Biomedical Semantics  (2018) 9:6 Page 13 of 13Author details1School of Computer Science, University of Manchester, Oxford Road,Manchester, UK. 2FactBio, Innovation Centre, Cambridge Science Park, CB4 0EYCambridge, UK. 3Lawrence Berkeley National Laboratory, Berkeley, USA.Received: 25 March 2017 Accepted: 22 December 2017Névéol et al. Journal of Biomedical Semantics  (2018) 9:12 https://doi.org/10.1186/s13326-018-0179-8REVIEW Open AccessClinical Natural Language Processing inlanguages other than English: opportunitiesand challengesAurélie Névéol1 *, Hercules Dalianis2, Sumithra Velupillai3,4, Guergana Savova5 and Pierre Zweigenbaum1AbstractBackground: Natural language processing applied to clinical text or aimed at a clinical outcome has been thriving inrecent years. This paper offers the first broad overview of clinical Natural Language Processing (NLP) for languagesother than English. Recent studies are summarized to offer insights and outline opportunities in this area.Main Body: We envision three groups of intended readers: (1) NLP researchers leveraging experience gained in otherlanguages, (2) NLP researchers faced with establishing clinical text processing in a language other than English, and(3) clinical informatics researchers and practitioners looking for resources in their languages in order to apply NLPtechniques and tools to clinical practice and/or investigation. We review work in clinical NLP in languages other thanEnglish. We classify these studies into three groups: (i) studies describing the development of new NLP systems orcomponents de novo, (ii) studies describing the adaptation of NLP architectures developed for English to anotherlanguage, and (iii) studies focusing on a particular clinical application.Conclusion: We show the advantages and drawbacks of each method, and highlight the appropriate applicationcontext. Finally, we identify major challenges and opportunities that will affect the impact of NLP on clinical practiceand public health studies in a context that encompasses English as well as other languages.Keywords: Natural Language Processing, Clinical Decision-Making, Languages other than EnglishBackgroundClinical research in a global contextHealthcare is a top priority for every country. Thegoal of clinical research is to address diseases withefforts matching the relative burden [1]. Compu-tational methods enable clinical research and haveshown great success in advancing clinical research inareas such as drug repositioning [2]. Much clinical information is currently contained in the free text of sci-entific publications and clinical records. For this reason,Natural Language Processing (NLP) has been increasinglyimpacting biomedical research [35]. Prime clinical appli-cations for NLP include assisting healthcare professionalswith retrospective studies and clinical decision making*Correspondence: aurelie.neveol@limsi.fr1LIMSI, CNRS, Université Paris Saclay, Rue John von Neumann, F-91405 OrsayParis, FranceFull list of author information is available at the end of the article[6, 7]. There have been a number of success stories invarious biomedical NLP applications in English [819].The ability to analyze clinical text in languages other thanEnglish opens access to important medical data concern-ing cohorts of patients who are treated in countries whereEnglish is not the official language, or in generating globalcohorts especially for rare diseases. One such exampleis the Phelan-McDermid Syndrome Foundation (PMSF),which is leading a Patient Powered Research Networkproject (part of the Patient Centered Outcome ResearchInstitute, PCORI [20] on a very rare disease. PMSF par-ents, together with researchers and advisors, launched aninternational patient registry, the PMSIR, that is directed,governed, and implemented by patient families. There area total of 900 cases of this rare disease in the entire world.Each patient contributed their EHR and genomics datato enable phenotype/genotype studies. Recently, Kohaneet al. have shown that methods allowing an aggregated© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Névéol et al. Journal of Biomedical Semantics  (2018) 9:12 Page 2 of 13exploitation of clinical data from multiple healthcare cen-ters could contribute to make headway in the understand-ing of autism spectrum disorders [21]. Cross-lingual textmining of newswires in thirteen languages was shownto be helpful for automated health surveillance of dis-ease outbreaks, and was routinely implemented in theBioCaster portal [22].In this context, data extracted from clinical text and clin-ically relevant texts in languages other than English addsanother dimension to data aggregation. TheWorld HealthOrganization (WHO) is taking advantage of this oppor-tunity with the development of IRIS [23], a free softwaretool for interactively coding causes of death from clini-cal documents in seven languages. The system compriseslanguage-dependent modules for processing death certifi-cates in each of the supported languages. The result oflanguage processing is standardized coding of causes ofdeath in the form of ICD10 codes, independent of thelanguages and countries of origin.Objective and ScopeThis paper follows-up on a panel discussion at the 2014American Medical Informatics Association (AMIA) FallSymposium [24]. Following the definition of the Interna-tional Medical Informatics Association (IMIA) Yearbook[25, 26], clinical NLP is a sub-field of NLP applied to clini-cal texts or aimed at a clinical outcome. This encompassesNLP applied to texts in Electronic Health Records (EHRs),but also extends to the development of resources forclinical NLP systems, and to clinically relevant researchaddressing biomedical information retrieval or the analy-sis of patient-authored text for public health or diagnosticpurposes. We survey studies conducted over the pastdecade and seek to provide insight on the major develop-ments in the clinical NLP field for languages other thanEnglish. We outline efforts describing (i) building newNLP systems or components from scratch, (ii) adaptingNLP architectures developed for English to another lan-guage, and (iii) applying NLP approaches to clinical usecases in a language other than English.Finally, we identify major NLP challenges and opportu-nities with impact on clinical practice and public healthstudies accounting for language diversity.Main TextReviewmethod and selection criteriaConducting a comprehensive survey of clinical NLP workfor languages other than English is not a straightforwardtask because relevant studies are scattered across the lit-erature of multiple fields, including medical informatics,NLP and computer science. In addition, the languageaddressed in these studies is not always listed in the titleor abstract of articles, making it difficult to build searchqueries with high sensitivity and specificity.In order to approximate the publication trends in thefield, we used very broad queries. A Pubmed query forNatural Language Processing returns 4,486 results (as ofJanuary 13, 2017). Table 1 shows an overview of clinicalNLP publications on languages other than English, whichamount to almost 10% of the total.We are showing the results of this query as an imperfectproxy for estimating the scale of the biomedical literaturerelevant to NLP research, as some publications addressingclinical NLP may not appear in PubMed, and some publi-cations referenced in PubMedmay bemissed by the query.As described below, our selection of studies reviewedherein extends to articles not retrieved by the query.Figure 1 shows the evolution of the number of NLPpublications in PubMed for the top five languages otherthan English over the past decade. We can see that Frenchbenefits from a historical but sustained and steady inter-est. Chinese and Spanish have recently attracted sustainedefforts. Japanese and German seem to receive plateauingattention.This work is not a systematic review of the clinical NLPliterature, but rather aims at presenting a selection ofstudies covering a representative (albeit not exhaustive)number of languages, topics and methods. We browsedthe results of broad queries for clinical NLP in MEDLINEand ACL anthology [26], as well as the table of contentsof the recent issues of key journals. We also leveraged ourown knowledge of the literature in clinical NLP in lan-guages other than English. Finally, we solicited additionalRESEARCH Open AccessOntology-based literature mining and classeffect analysis of adverse drug reactionsassociated with neuropathy-inducing drugsJunguk Hur1* , Arzucan Özgür2 and Yongqun He3,4,5,6*AbstractBackground: Adverse drug reactions (ADRs), also called as drug adverse events (AEs), are reported in the FDA druglabels; however, it is a big challenge to properly retrieve and analyze the ADRs and their potential relationshipsfrom textual data. Previously, we identified and ontologically modeled over 240 drugs that can induce peripheralneuropathy through mining public drug-related databases and drug labels. However, the ADR mechanisms of thesedrugs are still unclear. In this study, we aimed to develop an ontology-based literature mining system to identifyADRs from drug labels and to elucidate potential mechanisms of the neuropathy-inducing drugs (NIDs).Results: We developed and applied an ontology-based SciMiner literature mining strategy to mine ADRs from thedrug labels provided in the Text Analysis Conference (TAC) 2017, which included drug labels for 53 neuropathy-inducing drugs (NIDs). We identified an average of 243 ADRs per NID and constructed an ADR-ADR network, whichconsists of 29 ADR nodes and 149 edges, including only those ADR-ADR pairs found in at least 50% of NIDs.Comparison to the ADR-ADR network of non-NIDs revealed that the ADRs such as pruritus, pyrexia,thrombocytopenia, nervousness, asthenia, acute lymphocytic leukaemia were highly enriched in the NID network.Our ChEBI-based ontology analysis identified three benzimidazole NIDs (i.e., lansoprazole, omeprazole, andpantoprazole), which were associated with 43 ADRs. Based on ontology-based drug class effect definition, thebenzimidazole drug group has a drug class effect on all of these 43 ADRs. Many of these 43 ADRs also exist in theenriched NID ADR network. Our Ontology of Adverse Events (OAE) classification further found that these 43benzimidazole-related ADRs were distributed in many systems, primarily in behavioral and neurological, digestive,skin, and immune systems.Conclusions: Our study demonstrates that ontology-based literature mining and network analysis can efficientlyidentify and study specific group of drugs and their associated ADRs. Furthermore, our analysis of drug class effectsidentified 3 benzimidazole drugs sharing 43 ADRs, leading to new hypothesis generation and possible mechanismunderstanding of drug-induced peripheral neuropathy.BackgroundWhile drugs have been widely and successfully used totreat various diseases, most drugs cause different adverseevents (AEs), commonly called adverse drug reactions(ADRs). These ADRs are sometimes severe and signifi-cantly affect public health. Indeed, ADRs are listed asthe fourth killer after heart disease, cancer, and stroke[1]. Therefore, it is critical to carefully study the ADRsand underlying mechanisms.Multiple studies have been conducted to automaticallyidentify ADRs in text using Natural Language Processing(NLP) techniques. Different types of data sources suchas electronic health records [2], scientific publications,and social media data have been used to extract ADRs.A lexicon of ADR-related terms and concepts was com-piled from different sources such as the Unified MedicalLanguage System (UMLS) [3] and the side effect re-source (SIDER) [4] and was used to match the ADR* Correspondence: junguk.hur@med.und.edu; yongqunh@med.umich.edu1Department of Department of Biomedical Sciences, University of NorthDakota School of Medicine and Health Sciences, Grand Forks, ND 58202, USA3Unit for Laboratory Animal Medicine, Department of Microbiology andImmunology, University of Michigan Medical School, Ann Arbor, MI 48109,USAFull list of author information is available at the end of the article© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Hur et al. Journal of Biomedical Semantics  (2018) 9:17 https://doi.org/10.1186/s13326-018-0185-xmentions in user comments retrieved from Daily-Strength (http://www.dailystrength.org) by Leaman et al.[5]. Nikfarjam and Gonzalez used the same user com-ment data set and developed an association rule miningapproach to tag ADR mentions [6]. Similarly to Leamanet al., Gurulingappa et al. [7] also developed alexicon-based matching approach to identify ADRs intext using the lexicon created based on the MedicalDictionary for Regulatory Activities (MedDRA) [8] andDrugBank [9]. However, rather than using user com-ments from social media, Gurulingappa et al. used theabstracts of case reports as their data source. Product la-bels have also been used as data sources to extract ADRsand create knowledge bases of known ADRs [10, 11]. Areview of recent techniques on ADR extraction fromtext from various data sources is available in [12].An important group of ADRs is neuropathy. UsingFDA reported package insert documents and drug safetyrecords, our previous studies identified 242neuropathy-inducing drugs (NIDs) through mining vari-ous public resources and drug labels [13, 14]. We havepreviously developed an Ontology of Drug NeuropathyAdverse Events (ODNAE) that ontologically represents214 NIDs, corresponding chemicals of these drugs,chemical function, adverse events associated with thesedrugs, and various other chemical characteristics [14].Our study also showed that ODNAE provides an idealplatform to systematically represent and analyze AEsassociated with neuropathy-inducing drugs and generatenew scientific insights and hypotheses [14]. One weak-ness of the ODNAE study is that ODNAE only collectsneuropathy-related ADRs commonly found in drugpackage insert documents and misses the collection ofnon-neuropathy ADRs from different sources.In addition to enhanced literature mining, ontologycan also be used for advanced class effect analysis.Specifically, an AE-specific drug class effect is defined toexist when all the drugs in a specific drug class (or druggroup) are associated with an AE. In a recent study oncardiovascular drug-associated AEs, a proportionalclass-level ratio (PCR) value was defined and used toidentify drug class effect on different AEs [15]. Specific-ally, when the PCR value equals to 1, it means that aclass effect of a group of drugs on a specific AE exists.Previous PCR-based heatmap analyses identified manyimportant drug class effects on different AEs [15].In addition to the official FDA drug package insertdocuments, FDA also collects large amounts of spontan-eous ADR case reports. To better understand these casereport data, it is critical to use standardized termin-ologies or ontologies to identify drugs, ADRs, andassociated data from the text reports. Therefore,ontology-based literature mining becomes critical. Previ-ously, we applied the Vaccine Ontology (VO) [16] toenhance our literature mining of interferon-gamma re-lated [17], Brucella-related [18], and fever-related [19]gene interaction networks in the context of vaccines andvaccinations. In these studies, we used and expanded Sci-Miner [20], a literature mining program with a focus onscientific article mining. SciMiner uses both dictionary-and rule-based strategies for literature mining [20].To better study biological interaction networks, wehave also developed a literature mining strategyCONDL, or Centrality and Ontology-based NetworkDiscovery using Literature data [19]. The centrality ana-lysis here refers to the application of different centralitymeasures to calculate the most important genes (i.e.,hub genes) of the resulting gene-gene interaction net-work out of biomedical literature mining. Centralitymeasures, including degree, eigenvector, closeness, andbetweenness, have been studied [19, 21]. The CONDLstrategy was applied to extract and analyze IFN-? andvaccine-related gene interaction network [21] andvaccine and fever-related gene interaction network [19],and our results showed that centrality analyses couldidentify important genes and raise novel hypothesesbased on literature mined gene interaction networks.The main purpose of this study was to develop aCONDL method for literature mining of all ADRs asso-ciated with neuropathy inducing drugs (NIDs) and usedthe mined results for systematic network and class effectanalyses. Using MedDRA [8], ODNAE [14], ChemicalEntities of Biological Interest (ChEBI) [22], and Ontol-ogy of Adverse Events (OAE) [23], we developed anontology-based ADR-SciMiner tool for identifying ADRsfrom drug labels and applied it to NIDs to ontologicallymodel their ADR-associated characteristics. The litera-ture mined results were then used for ontology-basedclass effect analysis, leading to new scientific discoveries.MethodsThe overall workflow of our ontology-based literaturemining approach for the study of neuropathy-inducingdrugs (NIDs) is illustrated in Fig. 1. Briefly, our approachincluded development of ADR-SciMiner platform thatidentifies ADRs from drug labels using the terms inMedDRA and OAE. Various term expansion, namematching, and filtering rules have been implemented.The mining performance was evaluated using manuallycurated drug labels. The final version of ADR-SciMinerwas applied to the NID labels and the results wereexamined using the ADR-ADR interaction network andthe OAE hierarchical structure.NID drug labelsIn the present study, we used a collection ofXML-structured drug labels that are applied for the TextAnalysis Conference (TAC) Adverse Drug ReactionHur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 2 of 10Extraction from Drug Labels track (https://tac.nist.gov/2017/). This data set includes the adverse event sectionsfrom a total of 2308 US FDA drug labels, which weresplit into two sets: Training set and Unannotated set,each containing 101 and 2207 drug labels. The Trainingset contained manually curated ADRs provided by theTAC organizing committee. Among 2207 drug labels inthe Unannotated set, TAC provided 99 labels with manu-ally curated ADRs, which were used for performance evalu-ation of ADR-SciMiner. Figure 2 illustrates an example ofXML-formatted drug-label from the Training set.NIDs were collected from our previous two studies:one examining the systems pharmacological aspects ofNIDs [13] and another focusing on ontology-based col-lection, representation and analysis of drug-associatedneuropathy adverse events [14].SciMiner tagging of ADR and drug termsSciMiner was originally developed as a web-based litera-ture mining platform, designed for identification ofhuman genes and proteins in a context-specific corpus[20]. Later, SciMiner was updated to identify bacterialgenes and various biomedical ontologies such as VaccineOntology (VO) and Interaction Network Ontology(INO), developed by our groups, resulting in specificvariations of SciMiner: INO-SciMiner [24], VO-SciMiner[18], and E-coli-SciMiner [25]. In this study, we devel-oped another version of SciMiner, specializing in theidentification and analysis of ADRs from the US FDAdrug labels.MedDRA, or Medical Dictionary for RegulatoryActivities, is a clinically validated standardized medicalterminology dictionary (and thesaurus), consisting of fivelevels of hierarchy. MedDRA has been widely used forsupporting ADR reporting in clinical trials [8, 26].MedDRA release version 20 (https://www.meddra.org/)and the OAE ontology were used as the source of theADR terms, which have been incorporated into SciMinerdictionary for ADR term identification. Perl packageLingua::EN was used to expand the ADR dictionaryallowing the inclusion of additional plural or singularforms where only one form is included in the dictionary.For example, peripheral neuropathy has been expandedto include peripheral neuropathies. Besides, variousterm variation and filtering rules were implemented toimprove the accuracy of ADR term tagging. For ex-ample, MedDRA terms ID 10003481 has preferred nameof Aspartate aminotransferase increased. ADR-SciMinerwas designed to properly identify variations of this pre-ferred name such as increased AST, AST elevated, andhigh AST. To reduce false positives, any matching ADRterms from section or table headers of drug labels wereexcluded.Performance evaluation of ADR-SciMinerThe TAC dataset included 200 manually curated labels(101 in the Training and 99 in the Unannotated sets)and the details have been recently published [27]. Briefly,four annotators, including two medical doctors, onemedical librarian and one biomedical informaticsFig. 1 Project workflow. This figure illustrates our overall workflow in the present study. US FDA drug labels were analyzed to identify ADRs andnormalized them through MedDRA v20 and OAE using ADR-SciMiner. A network of ADR-ADR based on the ADRs reported to have been causedby NIDs was constructed. The most central ADRs in the network were analyzed. The characteristics of NID-associated ADRs were further exploredusing the ontological structures in OAEHur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 3 of 10researcher, participated in the manual annotation processof these 200 drug labels. These annotators were all trainedbiomedical annotation and the drug labels were annotatedindependently by these annotators. Any disagreementswere reconciled in pairs or collectively resolved by all fourannotators. The mining performance of ADR-SciMinerwas evaluated using the 99 drug labels in the Unannotatedset. The evaluation was done at the level of normalizedMedDRA Preferred Terms (PTs) for each drug. Recall,Precision, and F-Score were calculated.Generation of ADR-ADR network and its analysisNID and non-NID associated ADR-ADR networks wereconstructed in our study. ADRs were represented as thenodes of the network. Two nodes were connected by anedge if they are associated with the same drug. In orderto obtain highly prevalent NID and non-NID specificADRs, an edge weight threshold of 50% was set. In otherwords, two ADRs were connected by an edge if theyco-occur together as ADRs of at least 50% of the NID ornon-NID drugs. Centrality analysis was performed onFig. 2 XML-formatted drug label. This figure illustrates an example of XML-formatted drug labels (adcetris) from the training set. The content hasbeen reduced and simplified to fit into a figure for demonstration purpose. Typical XML-formatted labels from the training set include three mainsections: Text containing the texts from ADR-relevant sections from drug labels; Mentions containing the manually curated ADRs; andReactions containing normalized ADRs in terms of MedDRA termsHur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 4 of 10the ADR-ADR networks using the Cytoscape plug-inCentiScaPe [28] to identify the most salient NID andnon-NID associated ADRs. Degree centrality and eigen-vector centrality were computed. Degree centrality cor-responds to the number of neighbors a node has. Eachneighbor contributes equally to the centrality of thenode. On the other hand, in eigenvector centrality thecontribution of each neighbor is proportional to its owncentrality.ChEBI and OAE-based ontological analyses of threeneuropathy-inducing drugs and associated ADRsThe drugs were mapped to ChEBI [22] terms, which arealso imported and used in the ODNAE. The identifiedADRs were mapped to OAE terms, and the OAE struc-ture was used to classify and analyze the ADR structure.To extract the associated drugs, AEs, and their relatedterms, the Ontofox tool [29] was used. The ProtégéOWL editor [30] was used to visualize the hierarchicalstructure of these extracted terms.Ontology-based analysis of drug class effects on AEsChEBI was used to classify NIDs into differenthigher-level classes or groups. For each high or inter-mediate level class, we calculated the drug class effecton AEs. Specifically, all the identified 53 NIDs were clas-sified into different categories using ChEBI. The AEs as-sociated with each NID were identified in the previousstudies. Based on these results, we were able to identifythe common AEs associated with all NIDs under a spe-cific class (e.g., benzimidazole drugs). Based on the classeffect definition, these results indicate that there exists aclass effect of the specific class on the common AEs (i.e.,the PCR value =1) [15]. All the common AEs were thenclassified based on OAE using the Ontofox tool [29].ResultsNID drug labelsFrom our two published studies on neuropathy-inducingdrugs [13, 14], we collected a total of 242 NIDs. We alsoobtained a collection of XML-structured drug labels thatare used for the 2017 Text Analysis Conference (TAC)Adverse Drug Reaction Extraction from Drug Labelstrack. This data set contains the adverse event sectionsof a total of 2308 US FDA drug labels in two subsets:Training set with 101 labels and Unannotated set with2207 labels, which corresponded to a total of 1883unique drugs. There were 299 unique drug names, eachof which included two or more labels, because a drug inour study refers to a generic drug name or an activedrug ingredient which can have multiple brands withdifferent labels. Among the 2308 labels, there were 69labels corresponding to 53 NIDs, which served as thedataset in the present study.SciMiner tagging of ADR and drug terms andperformance evaluationADR-SciMiner has been developed to include thedictionary of ADRs based on MedDRA release 20 andthe current version of OAE. The ADR term diction-ary is expanded to include variations such as pluralvs singular nouns to increase the coverage. The per-formance of current version of ADR-SciMiner wasevaluated based on the ADRs from 99 labels. Theselabels included 5158 MedDRA PT terms, whileADR-SciMiner reported 5360 PT terms collectively.ADR-SciMiner correctly identified 4198 of these 5158PTs in the TAC data: a recall of 0.81, a precision of0.75, and an F-Score of 0.77 was obtained.MedDRA representation of ADRsTable 1 summarizes the numbers of identified ADRsfrom the 53 NIDs. These NIDs are a subset of the totalNIDs identified in our previous studies [13, 14]. We didnot use all the over 200 NIDs because only these 53NIDs have corresponding ADR text data in the FDATAC 2017 dataset. Briefly, ADR-SciMiner identifiedapproximately an average of 243 ADRs per drug (114unique ADRs per drug). Antidepressant medicine Venla-faxine had the most ADRs of 433, while glucocorticoidtriamcinolone has the least ADRs of 9 (Table 1).Literature mining statistics and ADR-ADR networkFigure 3 is a NID-associated ADR network based on thecutoff of co-occurrence of two ADRs connected in atleast 50% (i.e., 27 out of 53) of the NIDs. The NID spe-cific ADR-ADR network shown in Fig. 3 contains 29nodes and 149 edges. The common ADRs are located atthe center of the network, including terms like headache,vomiting, pyrexia, nausea, dizziness, etc. More specificanalysis of the network is reported below.Centrality analysis of ADR-ADR networkThe eigenvector and degree centrality scores of the 29ADRs found using NIDs are shown in Table 2. The sameapproach was used to construct a non-NID specificADR-ADR network, where two ADRs are connected byan edge if they co-occur in at least 50% of the remaining(i.e., non-NID drugs). This resulted in a networkcontaining only six ADRs, namely headache, vomiting,diarrhoea, rash, nausea, and dizziness. Although these arealso among the most central ADRs in the NID specificnetwork, they are not NID specific, since they are alsoprevalent and commonly occur together in the non-NIDcase. Some notable ADRs central in the NID-specific net-work but not parts of the non-NID specific network in-clude pruritus, pyrexia, thrombocytopenia, nervousness,asthenia, acute lymphocytic leukaemia, decreased appetite,Hur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 5 of 10insomnia, and depression. Degree and eigenvector central-ity produced the same ranking (Table 2).Ontology-based analysis of benzimidazole NID drugs andtheir associated ADR typesOut of the 53 drugs, we used the ChEBI chemicalontology structure to examine the chemical classifica-tion of these 53 drugs and their associatedupper-level hierarchies. One interesting group of che-micals becomes interesting to us, which is the groupof benzimidazole, a colorless heterocyclic aromaticorganic compound that consists of the fusion of ben-zene and imidazole [31]. Benzimidazole drugs arestructural isosteres of naturally-occurring nucleotides,allowing them to interact with the biopolymers of liv-ing systems and become an important group of drugswith antimicrobial, anti-inflammatory, and anticanceractivities. The three benzimidazole NIDs identified inour study include lansoprazole, omeprazole, and pan-toprazole (Fig. 4), which are all proton-pump inhibi-tors that inhibit gastric acid secretion [32]. Thesethree drugs can all be used for relief of symptoms ofgastroesophageal reflux disease, gastric and duodenalulcer disease, and eradication of Helicobacter pyloriinfection [32]. Their shared and different ADR pro-files have not been studied.In our study, lansoprazole, omeprazole, and panto-prazole are associated with 389 (273 are unique), 298(165 are unique), and 166 (74) ADRs, respectively.We identified 43 ADRs associated with all threedrugs. Based on our drug class effect definition [15],these 43 ADRs are all categorized as AEs out of theclass effect of the benzimidazole drug class. Further-more, we applied the OAE to generate a subset viewof these ADRs in the OAE framework (Fig. 5). Asshown in this figure, these 43 ADRs are focused onbehavioral and neurological ADRs, digestive ADRs,and skin ADRs. There are also many ADRs in thehematopoietic system, homeostasis system, immunesystem, and muscular system.DiscussionThe contributions of this study are multiple fold.First, we developed and applied an ontology-basedSciMiner literature mining approach, which was thenused to mine the FDA TAC 2017 dataset. It is a hugechallenge to identify all ADRs using textual descrip-tion of ADR case reports. Our MedDRA/OAE-basedSciMiner literature mining approach was successfullyused to mine the FDA TAC 2017 dataset with a spe-cial focus on 53 neuropathy-inducing drugs (NIDs).Our study demonstrates the important role of theMedDRA controlled terminology and ontologies (e.g.,ChEBI, OAE, and ODNAE) in the literature miningTable 1 Identified ADRs from 53 NIDs drug labelsColor highlight was used to visualize difference among the number of ADRsacross NIDsHur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 6 of 10and further ADR analysis. Second, we constructed anADR-ADR network and applied centrality analysis toidentify the hub ADRs in the network. Third, amongthe 53 NIDs, our ChEBI-based analysis found threebenzimidazole drugs, which formed a drug class effecton 43 ADRs. An OAE analysis of these ADRs furtheridentified many enriched ADR categories. Based on theresults, we can hypothesize that the proton-pump inhib-ition role, common to all the three benzimidazole drugs,might participate in different pathways leading to theseADRs. To our knowledge, our study represents the first ofsuch literature mining-derived ontology-based drug classeffect analysis.The present study is based on a subset of US FDAdrug labels, which was included in the 2017 TextAnalysis Conference (TAC) Adverse Drug ReactionExtraction from Drug Labels track. We used this dataset as a proof of concept as well as to develop a proto-type version of ADR-SciMiner. We assumed that if anADR is mentioned in the file of a drug, it is associatedwith the drug. However, it is likely that the ADR occurswithin a negation or speculation statement such asdepression was not observed as an ADR of the drug ordepression might be an ADR of the drug. Therefore,more semantic oriented NLP analysis techniques may bedeveloped to identify whether an ADR is really associ-ated with a drug or not.To identify the most salient ADRs associated withNIDs, we created ADR-ADR networks both specificto NIDs and non-NIDs using a threshold of 50% forassociation. In other words, two ADRs wereconnected by an edge, if they co-occur in at least50% of the NIDs or non-NIDs. Six of the centralADRs in the NID specific network were also in-cluded in the non-NID specific network, showingthat these are prevalent and commonly occur to-gether both in NID and non-NID cases. The otherADRs in Table 2 are central only in the NID associ-ated network, which might reveal that they are moreNID specific. As future work, we plan to extend thenetwork analysis by including the specific drugs tothe network as well and creating bipartite drug-ADRnetworks. The types of relations between drugs andADRs can be identified by using the Interaction Net-work Ontology (INO) [24].Our study identified three benzimidazole drugs (i.e.lansoprazole, pantoprazole, and omeprazole) thatinduce similar profiles of ADRs. Overall these threedrugs have been found safe in terms of their associ-ated ADR reports [3335]. For example, a previousstudy with 10,008 users of lansoprazole in daily prac-tice indicated that the most frequently reported lanso-prazole ADRs were diarrhoea, headache, nausea, skindisorders, dizziness, and generalized abdominal pain/cramps, but no evidence of rare ADRs were found[33]. Current study found many ADRs associated witheach of these three drugs, and all these three drugsare associated with 43 ADRs, commonly behavioraland neurological, digestive, muscular, and skin ADRs.A common reason for stopping pantoprazole usagewas found to be the diarrhea ADR [34], which is alsolisted as one of the 43 ADRs.Fig. 3 NID associated ADR network. Two ADRs are connected by an edge if they co-occur in over 50% of the NIDs. Node sizes are proportionalto the degrees of the nodes. Edge thickness corresponds to the number of drugs having two ADRsHur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 7 of 10A previous study suggested that these three drugs havesimilar profiles to interact with other drugs (most com-monly vitamin K antagonist), suggesting a class effect[36]. According to the ODNAE records [14], lansopra-zole, omeprazole, and pantoprazole are all associatedwith neuropathy adverse events. Our study found 43AEs commonly shared with these three benzimidazoledrugs. Interestingly, many of these AEs are also found tobe the hubs of the highly enriched NID network fromour literature mining data centrality analysis. It is likelythat these three benzimidazole drugs, which function asproton-pump inhibitors, use the same or similar path-ways to induce neuropathy adverse events.It is noted that the ontology-based drug class ef-fect study is novel in many aspects compared to itsoriginal report [15]. First, compared to the previousreport using the drug package insert information,our study uses the data generated from literaturemining of FDA provided case report data. Second,given the large size of AE data for each vaccine, wewere able to identify many AEs commonly used by aclass of drugs, in our case, 43 AEs associated withthe three benzimidazole drugs. Our OAE-based ana-lysis was able to further identify the common pat-terns among these AEs. Such a high throughputstudy was not reported in the previous package in-sert document-based studies.The ADR identification performance is not yet op-timal and there is still much room for improvement.The majority of falsely identified ADR terms bySciMiner could be grouped into three types: (1)incorrect mapping of acronyms to ADRs (e.g., all, asin all patients, mapped to acute lymphocytic leukae-mia); (2) ADR that may not be caused by thecurrent drug (e.g., caution is needed in patients withdiabetes); and (3) ADRs that occur as discontinuousentities in text (e.g., corneal ulceration is an ADR,but does not occur as a continuous text fragment incorneal exposure and ulceration). Integration ofother dictionaries such as SNOMED CT [37] intoADR-SciMiner will be explored to possibly expandthe ADR dictionary thus to improve the recall. Iden-tifying whether a term is an acronym for an ADR ornot, determining whether an ADR that occurs in adrug label is really caused by that drug, and detect-ing ADRs that occur as discontinuous text fragmentsTable 2 The centrality scores of the ADRs in the NID specificADR-ADR networkADR Degree Eigenvectornausea 27 0.311headache 26 0.310vomiting 23 0.301diarrhoea 23 0.301pruritus 19 0.270dizziness 16 0.245pyrexia 14 0.231rash 14 0.231thrombocytopenia 14 0.228nervousness 13 0.222asthenia 13 0.214acute lymphocytic leukaemia 10 0.187decreased appetite 10 0.177insomnia 8 0.149depression 8 0.148urticaria 7 0.139hypersensitivity 7 0.138leukopenia 7 0.137abdominal pain 6 0.122dyspepsia 6 0.118constipation 6 0.114neuropathy peripheral 5 0.102seizure 4 0.086somnolence 4 0.086paraesthesia 2 0.044myalgia 2 0.044arthralgia 2 0.041alopecia 1 0.022hyperhidrosis 1 0.022Two centrality measures (degree and eigenvector) were calculated usingCytoscape app CentiScaPeFig. 4 Identification of three benzimidazole drugs associated withneuropathy adverse events. The three drugs were grouped by ChEBIunder the benzimidazoles chemical group. The hierarchical structureof the benzimidazoles chemical group is also laid outHur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 8 of 10in text require deeper semantic understanding of thesentences by considering the context information(i.e., the surrounding words) of an ADR in text. Ourcurrent method is a dictionary and rule-basedmethod, which does not consider the context of anADR occurrence in text. These challenges can betackled by using machine learning methods with fea-tures that capture context information and utilizethe syntactic analysis of the sentences such as theirdependency parses.As future work, we plan to develop machine learningbased methods to improve the accuracy of ADR taggingas well as the detection of the associations betweenADRs and drugs. We will also extend our approach toinclude all available structured drug labels in theDailyMed database, maintained by National Institute ofHealth. DailyMed currently contains listings of 95,513drugs submitted to the US FDA, about 28,000 of whichare prescription drugs for human. Our ontological studyof NIDs will be extended using this larger drug labeldataset.ConclusionsIn this study we developed an MedDRA andontology-based SciMiner literature mining pipeline, ap-plied the pipeline to mine a FDA text set for ADRs asso-ciated with neuropathy-inducing drugs, performedcentrality network analysis, and drug class effect studies.Our approach identified scientific insights regardingthese drug-specific ADRs. Our study demonstrates thefeasibility of using ontology-based literature mining,network analysis, and drug class effect classification toefficiently identify and study specific drugs and theirassociated ADRs.AbbreviationsADR: Adverse Drug Reaction; ChEBI: Chemical Entities of Biological Interest;CONDL: Centrality and Ontology-based Network Discovery using Literaturedata; INO: Interaction Network Ontology; MedDRA: Medical Dictionary forRegulatory Activities; NID: Neuropathy Inducing Drug; NLP: Natural LanguageProcessing; OAE: Ontology of Adverse Events; ODNAE: Ontology of DrugNeuropathy Adverse Events; PCR: Proportional Class level Ratio; TAC: TextAnalysis Conference; VO: Vaccine OntologyAcknowledgementsThe authors thank the participants of the 6th International Workshop onVaccine and Drug Ontology Studies (VDOS) 2017 for their valuable feedback.FundingThis work was partially supported by the BAGEP Award of the ScienceAcademy (to AO).Availability of data and materialsAll data generated or analyzed during this study are included in thispublished article.Authors contributionsJH developed the ontology-based literature mining pipeline and applied thepipeline to generate literature mining results. AO performed the centrality-based network analysis. YH performed ontology-based result analyses. JH,AO, and YH all participated in the project design, result interpretation, andmanuscript writing. All authors read and approved the final manuscript.Ethics approval and consent to participateNot applicable.Competing interestsThe authors declare that they have no competing interests.Fig. 5 Hierarchical display of 43 ADRs associated with three benzimidazoles drugs. The OAE IDs corresponding to the 43 ADRs were identified,and Ontofox was used to these terms and their associated hierarchical terms using the IncludeComputedIntermediate conditionHur et al. Journal of Biomedical Semantics  (2018) 9:17 Page 9 of 10Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Department of Department of Biomedical Sciences, University of NorthDakota School of Medicine and Health Sciences, Grand Forks, ND 58202,USA. 2Department of Computer Engineering, Bogazici University, 34342Istanbul, Turkey. 3Unit for Laboratory Animal Medicine, Department ofMicrobiology and Immunology, University of Michigan Medical School, AnnArbor, MI 48109, USA. 4Department of Microbiology and Immunology,University of Michigan Medical School, Ann Arbor, MI 48109, USA. 5Center forComputational Medicine and Bioinformatics, University of Michigan MedicalSchool, Ann Arbor, MI 48109, USA. 6Comprehensive Cancer Center, Universityof Michigan Medical School, Ann Arbor, MI 48109, USA.Received: 6 February 2018 Accepted: 18 May 2018Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 https://doi.org/10.1186/s13326-018-0178-9RESEARCH Open AccessMatching biomedical ontologies basedon formal concept analysisMengyi Zhao1,2, Songmao Zhang1*, Weizhuo Li1,2 and Guowei Chen1,2AbstractBackground: The goal of ontology matching is to identify correspondences between entities from different yetoverlapping ontologies so as to facilitate semantic integration, reuse and interoperability. As a well developedmathematical model for analyzing individuals and structuring concepts, Formal Concept Analysis (FCA) has beenapplied to ontology matching (OM) tasks since the beginning of OM research, whereas ontological knowledgeexploited in FCA-based methods is limited. This motivates the study in this paper, i.e., to empower FCA with as muchas ontological knowledge as possible for identifying mappings across ontologies.Methods: We propose a method based on Formal Concept Analysis to identify and validate mappings acrossontologies, including one-to-one mappings, complex mappings and correspondences between object properties.Our method, called FCA-Map, incrementally generates a total of five types of formal contexts and extracts mappingsfrom the lattices derived. First, the token-based formal context describes how class names, labels and synonyms sharelexical tokens, leading to lexical mappings (anchors) across ontologies. Second, the relation-based formal contextdescribes how classes are in taxonomic, partonomic and disjoint relationships with the anchors, leading to positiveand negative structural evidence for validating the lexical matching. Third, the positive relation-based context can beused to discover structural mappings. Afterwards, the property-based formal context describes how object propertiesare used in axioms to connect anchor classes across ontologies, leading to property mappings. Last, therestriction-based formal context describes co-occurrence of classes across ontologies in anonymous ancestors ofanchors, from which extended structural mappings and complex mappings can be identified.Results: Evaluation on the Anatomy, the Large Biomedical Ontologies, and the Disease and Phenotype track of the2016 Ontology Alignment Evaluation Initiative campaign demonstrates the effectiveness of FCA-Map and itscompetitiveness with the top-ranked systems. FCA-Map can achieve a better balance between precision and recall forlarge-scale domain ontologies through constructing multiple FCA structures, whereas it performs unsatisfactorily forsmaller-sized ontologies with less lexical and semantic expressions.Conclusions: Compared with other FCA-based OM systems, the study in this paper is more comprehensive as anattempt to push the envelope of the Formal Concept Analysis formalism in ontology matching tasks. Five types offormal contexts are constructed incrementally, and their derived concept lattices are used to cluster thecommonalities among classes at lexical and structural level, respectively. Experiments on large, real-world domainontologies show promising results and reveal the power of FCA.Keywords: Ontology matching, Formal concept analysis, Concept lattice*Correspondence: smzhang@math.ac.cn1MADIS, Academy of Mathematics and Systems Science, Chinese Academy ofSciences, Beijing, Peoples Republic of ChinaFull list of author information is available at the end of the article© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 2 of 27BackgroundOntologies aim to model domain conceptualizations sothat applications built upon them can interoperate witheach other by sharing the same meanings. Such knowl-edge sharing and reuse can be severely hindered by thefact that ontologies for the same domain are often devel-oped for various purposes, differing in coverage, granular-ity, naming, structure and many other aspects. Ontologymatching (OM) techniques aim to alleviate the hetero-geneity by identifying correspondences across ontologies.Ontology matching can be performed at the element leveland the structure level [1]. The former considers ontologyclasses and their instances independently, such as string-based and language-based techniques, whereas the latterexploits relations among entities, including graph-basedand taxonomy-based techniques. Most ontology match-ing systems [28] adopt both element and structure leveltechniques to achieve better performance.Life sciences is one of the most successful applica-tion areas of the Semantic Web technology, and manybiomedical ontologies have been developed and utilizedin real-world applications. These ontologies cover dif-ferent yet overlapping domains and are often of largescale, including, for example, the Foundational Model ofAnatomy (FMA) [9] and Adult Mouse Anatomy (MA)[10] for anatomy, National Cancer Institute Thesaurus(NCI) [11] for disease, and Systematized Nomenclature ofMedicine-Clinical Terms (SNOMED-CT) [12] for clinicalmedicine. Moreover, efforts such as the Unified MedicalLanguage System (UMLS) [13] integrate various biomed-ical systems so as to enhance their reuse and interoper-ability. For such biomedical domain ontologies, the annualOntology Evaluation Alignment Initiative (OAEI) [14] setsthree competition tracks, the Anatomy, the Large Biomed-ical Ontologies, and the Disease and Phenotype, whichhave attracted many state-of-the-art ontology matchingsystems [24, 7, 8] to challenge.Among the first batch of OM algorithms and tools pro-posed in the early 2000s, FCA-Merge [15] distinguishedin using the Formal Concept Analysis (FCA) formalismto derive mappings from classes sharing textual docu-ments as their individuals. Proposed by Rudolf Wille [16],FCA is a well developed mathematical model for ana-lyzing individuals and structuring concepts. FCA startswith a formal context consisting of a set of objects, aset of attributes, and their binary relations. Concept lat-tice, or Galois lattice, can be computed based on formalcontext, where each node represents a formal conceptcomposed of a subset of objects (extent) with their com-mon attributes (intent). The extent and the intent of aformal concept uniquely determine each other in the lat-tice. Moreover, the lattice represents a concept hierarchywhere one formal concept becomes sub-concept of theother if its objects are contained in the latter.Both ontologies and FCA aim at modeling conceptsin hierarchical structures. The purpose of an ontology isto represent a shared understanding of the domain ofinterest [17] that can be queried and reasoned upon in anautomated way. On the other hand, FCA is a conceptualclustering technique with solidmathematical foundations,allowing to derive concept hierarchies from datasets.Ontologies and FCA can complement each other, as ana-lyzed in [18] from an application point of view. FCA cannaturally be applied to constructing ontologies in ontol-ogy engineering [1921], and is also widely used in dataanalysis, information retrieval, and knowledge discovery.Following the steps of FCA-Merge, several OM sys-tems continued to use FCA as well as its alternative for-malisms, exploiting different entities as the sets of objectsand attributes for constructing formal contexts [2226].FCA-OntMerge [23], for example, utilizes the classes ofontologies and their attributes to form its formal con-text, whereas in [22] the formal context is composed ofontology classes as objects and terms of a domain-specificthesaurus as attributes. Different types of formal contextsdecide the information used for ontology matching, andwe observed that some intrinsic and essential knowledgeof ontology has not been involved yet, including both tex-tual information within classes (e.g., class labels and syn-onyms) and relationships among classes (e.g., ISA, sibling,disjointedness relations, and properties and axioms).This motivated the study in this paper, i.e., empow-ering FCA with as much as ontological information aspossible for identifying and validating mappings acrossontologies. Our method, called FCA-Map, incrementallygenerates a total of five types of formal contexts andextracts mappings from the lattices derived. First, thetoken-based formal context describes how class names,labels and synonyms share lexical tokens, leading to lex-ical mappings (anchors) across ontologies. Second, therelation-based formal context describes how classes arein taxonomic, partonomic and disjoint relationships withthe anchors, leading to positive and negative structuralevidence for validating the lexical matching. Third, afterconflict repairing, the positive relation-based context canbe used to discover structural mappings. Afterwards,the property-based formal context describes how objectproperties are used in axioms to connect anchor classesacross ontologies, leading to property mappings. Last, therestriction-based formal context describes co-occurrenceof classes across ontologies in anonymous ancestorsof anchors, from which extended structural one-to-onemappings and complex mappings can be identified.We participated in the three OAEI 2016 tracks relatedto the biomedical domain, and the results demonstratethe effectiveness of FCA-Map and its competitivenesswith the OAEI top-ranked OM systems. FCA-Map isone of the three winners of the Disease and PhenotypeZhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 3 of 27track of the OAEI 2016 campaign. Our method is suit-able for aligning large-scale domain ontologies with richlexical and structural knowledge, due to a comprehen-sive construction of multiple FCA structures using names,hierarchies, properties, and axioms. This requires thatontologies provide meaningful lexical symbols and termsfor classes, deep taxonomic hierarchies, and a large num-ber of classes and expressive logical axioms specifyingrestrictions on properties linking classes. Such conditionscan be satisfied by many ontologies in the biomedi-cal domain, for which FCA-Map is effective and suc-ceeds in discovering mappings that are missed by otherOM systems.The rest of the paper is organized as follows. We firstintroduce the basic definitions and characteristics of FCA.An overview of the FCA-Map method is presented, fol-lowed by five sections describing the five types of formalcontexts and the derivation of mappings in detail. Theevaluation section presents a comprehensive group ofexperiments, including the respective empirical results ofthe five steps as well as step-wise comparisons with coun-terparts. The evaluation also includes comparisons withOAEI 2016 top-ranked systems and previous FCA-basedOM systems. Finally, we analyze in-depth the advantagesand limitations of FCA-Map in contrast with other OMsystems and FCA-based systems, and discuss the futurework, followed by a conclusion.PreliminariesFormal Concept Analysis (FCA) is a mathematical theoryof data analysis based on applied lattice and order the-ory. FCA constructs formal contexts for objects and theirattributes, and then derives concept hierarchical struc-tures which constitute lattices. Formal context is definedas a triple K := (G,M, I), where G is a set of objects, Ma set of attributes, and I a binary relation between G andM in which gIm holds, i.e., (g,m) ? I, reads: object g hasattribute m [27]. Formal contexts are often illustrated inbinary tables, as exemplified by Table 1, where rows cor-respond to objects, columns to attributes, and a cell ismarked with × if the object in its row has the attributein its column. In Table 1, the marked cell represents thatthe animal listed in the row possesses the correspondingfeature in the column.Table 1 An example formal contextKeVertebrate Mammal Flying Aquatic CarnivorousElephant × ×Dolphin × × × ×Porpoise × × × ×Hawk × × ×Octopus × ×Definition 1 [27] For subsets of objects and attributesA ? G and B ? M, derivation operators are defined asfollows:A? = {m ? M | gIm for all g ? A}B? = {g ? G | gIm for all m ? B}A? denotes the set of attributes common to the objectsin A; B? denotes the set of objects which have all theattributes in B.A formal concept of contextK is a pair (A,B) consistingof extent A ? G and intent B ? M such that A = B? andB = A?. B(K) denotes the set of all formal concepts ofcontextK. The partial order relation, namely subconcept-superconcept-relation, is defined as:(A1,B1) ? (A2,B2) :? A1 ? A2(? B1 ? B2)Relation ? is called a hierarchical order of formal con-cepts. B(K) ordered in this way is exactly a completelattice, called the concept lattice and denoted byB(K).For an object g ? G, its object concept ? g := ({g}??, {g}?)is the smallest concept in B(K) whose extent containsg. In other words, object g can generate formal concept? g. Symmetrically, for an attribute m ? M, its attributeconcept ?m := ({m}?, {m}??) is the greatest concept inB(K) whose intent contains m. In other words, attributem can generate formal concept ?m. For a formal concept(A,B), its simplified extent (simplified intent), denoted byKex (Kin), is a minimal description of the concept. Eachobject (attribute) in Kex (Kin) can generate the formal con-cept (A,B). As a matter of fact, Kex dose not occur in anydescendant of (A,B) in B(K) and Kin dose not occur inany ancestor of (A,B) inB(K). Figure 1 shows the conceptlattice of context Ke in Table 1. In the concept lattice dia-grams in this paper, each node represents a formal conceptlabeled by its simplified intent and simplified extent, thelatter being given in italics. A line connecting two nodesrepresents that the lower formal concept is a subconceptof the upper concept. The node at the top representssuprema whose extent is the set of all objects, whereas thenode at the bottom is infima whose intent is the set of allattributes.MethodsGiven two ontologies, FCA-Map builds formal contextsand uses the derived concept lattices to cluster the com-monalities among ontology entities including classes andobject properties, at lexical level and structural level,respectively. Concretely, FCA-Map performs step-by-stepas follows, where a total of five types of contexts areconstructed.Step 1 Acquiring anchors lexically. Based on classnames, labels and synonyms, the token-basedformal context is constructed, and from itsZhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 4 of 27Fig. 1 Concept latticeB(Ke) with simplified labeling for the example formal context in Table 1derived concept lattice, a group of lexicalmappings between classes across ontologies canbe extracted, called lexical anchorsA0class.Step 2 Validating anchors structurally. Based onA0class, ISA and PART-OF hierarchies anddisjointness axioms, the relation-based formalcontext is constructed, and from its derivedconcept lattice, positive and negative structuralevidence of anchors can be extracted. Moreover,an enhanced alignmentA1class without anyconflicts among anchors is obtained.Step 3 Discovering structural matches. Based onA1classand ISA and PART-OF hierarchies, the positiverelation-based formal context is constructed, andfrom its derived concept lattice, structuralmatches among classes across ontologies can beidentified, augmentingA1class to alignmentA2class.Step 4 Acquiring property mappings. Based onA2classand axioms specifying that object properties holdbetween instances of class mappings, theproperty-based formal context is constructed, andfrom its derived concept lattice, a group ofmappings among properties across ontologiesAproperty can be extracted.Step 5 Identifying extended and complex mappings.Based onAproperty,A2class and axioms specifyingrestrictions on how to use properties with respectto classes, the restriction-based formal context isconstructed, and from its derived concept lattice,extended structural mappings among classesacross ontologiesA3class can be extracted,including one-to-one mappings and complexmappings where a class is identified to correspondto a semantic expression composed of classes andproperties in another ontology.To illustrate every step of FCA-Map, we use parts offourmatching tasks from the Anatomy track and the LargeBiomedical Ontologies track of OAEI 2016, shown inTable 2, as running examples in the subsequent sections.MA, NCI, FMA, and SNOMED-CT are all real-world,biomedical ontologies and the versions used are the OWLfiles provided by OAEI. These matching tasks use smallfragments of the corresponding ontologies, whose pro-portions are listed in Table 2.Constructing the token-based formal context toacquire lexical anchorsMost OM systems rely on lexical matching as initiationdue to the fact that classes sharing names across ontolo-gies quite likely represent the same entity in the domain ofinterest. FCA-Map, rather than using lexical and linguis-tic analyzing techniques, generates a formal context at thelexical level and obtains mappings from the lattice derivedfrom the context. Concretely, names of ontology classesas well as their labels and synonyms, when available,are exploited after normalization that includes inflection,tokenization, stop word elimination1, and punctuationelimination. The token-based formal context for ontologymatching is defined as follows.Definition 2 The token-based formal context for ontol-ogy matching is a triple Klex := (Glex,Mlex, Ilex), whereobjects Glex is the set of strings each corresponding to aname, label, or synonym of classes in two source ontolo-gies, attributes Mlex is the set of tokens in these strings, andbinary relation (g,m) ? Ilex holds when string g containstoken m, or a synonym or lexical variation of m.Table 2 Matching tasks of fragment ontologies of the OAEI 2016Anatomy track and the Large Biomedical Ontologies trackMatching task Number of classes in O1 Number of classes in O2MA-NCI 2744 (100% of MA) 3304 (5% of NCI)FMA-NCI 3696 (5% of FMA) 6488 (10% of NCI)FMA-SNOMED 10157 (13% of FMA) 13412 (5% of SNOMED)SNOMED-NCI 51128 (17% of SNOMED) 23958 (36% of NCI)Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 5 of 27We use the UMLS Sub-Term Mapping Tools [28] toaccess synonyms and the UMLS SPECIALIST Lexicon[29] to access lexical variations of biomedical terms.Table 3 shows Klex of a small part of MA and NCI,and its derived concept lattice is displayed in Fig. 2. Foreach formal concept derived, in addition to the stringsin its extent, we are also interested in the classes thatthese strings come from, and we call them class-originextent. For example, in Fig. 2, the class-origin extentof formal concept by node 8 is {MA:mammary glandfluid/secretion, NCI:Breast Fluid or Secretion} since inNCI, Mammary Gland Fluids and Secretions is a syn-onym of class NCI:Breast Fluid or Secretion.An essential feature of FCA is the duality between aset of objects and their attributes. The more attributesdemanded, the fewer objects can meet the requirements.In the case of the token-based formal concept, the morecommon tokens occurring in its intent, the fewer stringsthe extent contains, and the more possibly for the classesin class-origin extent to bematched. This is to say that car-dinality of the extent can reflect how similar the stringsare, thus classes from different source ontologies in asmaller-sized class-origin extent can be considered as amapping with higher confidence. Practically, we restrictour attention to formal concepts whose simplified extentor class-origin extent contains exactly two strings orclasses across ontologies, and extract two types of lexi-cal anchors, namely Type I anchor for the exact match,and Type II anchor for the partial match, respectively. Onthe other hand, note that cardinality of the intent cannotbe used to measure the similarity of strings. For example,MA:nerve and NCI:Nerve, which is a match, share onlyone token, whereas MA:left lung respiratory bronchioleand NCI:Right Lung Respiratory Bronchiole, not a match,share three tokens.Type I anchor. Simplified extent Kex of the for-mal concept contains exactly two strings from classesacross ontologies. This indicates that the two stringsare composed of the same or synonymous tokens, thusthe corresponding classes are extracted to be a match,as exemplified by ?MA:mammary gland fluid/secretion,NCI:Breast Fluid or Secretion? through formal concept ofnode 8 in Fig. 2 whose Kex has two strings, one from MAand the other NCI.Type II anchor. The class-origin extent of the formalconcept contains exactly two classes across ontologiesand simplified extent Kex contains strings from at mostone source ontology. Here the strings share tokens inthe intent rather than composed of the same or syn-onymous tokens. For example, ?MA:adrenal gland zonafasciculata, NCI:Fasciculata Zone? is extracted from node3 in Fig. 2, due to the common token fasciculata whichexists solely in these two classes. And ?MA:palatine gland,NCI:Palatine Salivary Gland? is identified as an anchorfrom node 7, due to the common tokens palatine andgland which co-exist solely in these two classes.Constructing the relation-based formal context tovalidate lexical anchorsStructural relationships of ontologies are exploited to val-idate the matches obtained at the lexical level. One of ourprevious studies [30] proposed using positive and nega-tive structural evidence among anchors for the purposeof validation. More precisely, classes of one anchor shar-ing relationships to classes in another anchor can be seenas their respective positive evidence. On the other hand,negative structural evidence refers to the conflict based onthe disjointedness relationships between classes. In FCA-Map, we build the relation-based formal context, definedas follows, to obtain both positive and negative structuralevidence for lexical anchors. Specifically, we exploit thetaxonomic, partonomic and disjoint relationships whichare common in biomedical ontologies. Both explicitly rep-resented and inferred semantic relations are used in ourmethod.Definition 3 The relation-based formal context forontology matching is a tripleKrel := (Grel,Mrel, Irel), whereobjects Grel is the set of all classes in two source ontologies,and attributes Mlex is the lexical anchors prefixed withTable 3 Token-based formal contextKlex of a small part of MA and NCIGland Adrenal Zona Zone Fasciculata Reticularis Salivary Palatine Mammary Secretion FluidMA:palatine gland × ×MA:adrenal gland zona fasciculata × × × ×MA:adrenal gland zona reticularis × × × ×MA:mammary gland fluid/secretion × × × ×NCI:Palatine Salivary Gland × × ×NCI:Fasciculata Zone × ×NCI:Reticularis Zone × ×NCI:Mammary Gland Fluids and Secretions × × × ×Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 6 of 27Fig. 2 Concept lattice with simplified labeling derived fromKlex in Table 3four kinds of relationships, i.e., ISA, SIBLING-WITH,PART-OF, and DISJOINT-WITH, labeled by (ISA),(SIB), (PAT), and (I-D) (or (D-I)), respectively.Binary relation (g,m) ? Irel holds if g in its ontologyhas the relationship ISA, SIBLING-WITH, PART-OF, orDISJOINT-WITH (as in the prefix of m) with the class inanchor m.The relation-based formal context Krel of a small partof MA and NCI is displayed in Table 4. For instance,MA:periodontal ligament and NCI:Periodontium are sub-classes of MA:ligament and NCI:Ligament, respectively,thus (MA:periodontal ligament, (ISA)?MA:ligament,NCI:Ligament?) ? Irel and (NCI:Periodontium, (ISA)?MA: ligament, NCI:Ligament?) ? Irel hold. Moreover,MA:adipose tissue is a subclass of MA:organ systemwhereas NCI:Adipose Tissue is disjoint with NCI:OrganSystem, thus (MA:adipose tissue, (I-D)?MA:organ system,NCI:Organ system?) ? Irel and (NCI:Adipose Tissue,(I-D)?MA:organ system, NCI:Organ system?) ? Irel hold.The derived concept lattice Krel of a small part of MAand NCI is illustrated in Fig. 3. Formal concepts whoseextents include both classes in an anchor indicate struc-tural evidence, defined as follows.Table 4 Relation-based formal contextKrel of a small part of MA and NCI(ISA)?MA:ligament,NCI:Ligament?(I-D)?MA:organ system,NCI:Organ System?(SIB)?MA:adipose tissue,NCI:Adipose Tissue?(SIB)?MA:larynx ligament,NCI:Laryngeal Ligament?(PAT)?MA:larynx,NCI:Larynx?MA:ligament × ×MA:periodontal ligament × × ×MA:auricular ligament × × ×MA:adipose tissue ×MA:larynx ligament × × ×NCI:Ligament ×NCI:Periodontium × × ×NCI:Broad Ligament × × ×NCI:Adipose Tissue ×NCI:Laryngeal Ligament × × ×Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 7 of 27Fig. 3 Concept lattice ofKrel with simplified labelingDefinition 4 In the derived concept lattice of therelation-based formal context Krel, if a formal concept(A,B) satisfies that its extent A includes both classes in thesame anchor a, then for anchors in its intent B with label(ISA), (SIB) or (PAT) , a is a positive evidence; and foranchors in its intent B with label (I-D) or (D-I), a is anegative evidence.For example, in the extent of node 3 in Fig. 3,?MA:periodontal ligament, NCI:Periodontium? and?MA:larynx ligament, NCI:Laryngeal Ligament?, twoanchors acquired lexically, are positive evidences toanchor ?MA:ligament, NCI:Ligament? with label (ISA)in the intent, and negative evidences to anchor ?MA:organsystem, NCI:Organ System? with label (I-D). We useP(a) and N(a) to denote the sets of positive and negativestructural evidence of anchor a, respectively, whose cardi-nalities are called the support degree and conflict degree ofanchor a. FCA-Map utilizes all the positive evidence setsP and negative evidence sets N to eliminate incorrectlexical anchors and retain the correct ones, as follows.Conflict repairing. The negative evidence leads to con-flicts among anchors, for which FCA-Map repairs in agreedy way, i.e., eliminating the conflict-causing anchorsiteratively until N becomes empty. At each iteration,anchor a having the least negative evidence set, i.e., thesmallest conflict degree, is selected. For every anchor b inN(a), if conflict degree of b is greater than a, eliminateb; otherwise, compare the support degree of a and b, andeliminate the one with smaller support degree.Anchor screening. Anchors having no positive structuralevidence according to the updated P are either caused bythe structural isolation of classes, or simply mismatches.FCA-Map screens anchors based on both lexical andstructural evidence, and Type II anchors without positiveevidence are eliminated.Constructing the positive relation-based formalcontext to discover structural matchesAfter conflict repairing and screening, anchors retainedare those supported both lexically and structurally. Basedon the enhanced alignment, FCA-Map goes further tobuild the positive relation-based formal context aim-ing to identify new, structural mappings. The way posi-tive relation-based formal context KposRel constructed issimilar to Krel, i,e., using classes in two source ontolo-gies as object set and anchors prefixed with relationshiplabels as attribute set. Concretely, five kinds of relation-ships are considered, ISA, SUPERCLASS-OF, SIBLING-WITH, PART-OF, and HAS-PART, where disjointednessrelationship is no longer necessary. For the derived for-mal concepts, we restrict our attention to those withclasses across ontologies in the simplified extent, andboth one-to-one mappings and complex mappings canbe identified.One-to-one structural mappings are extracted fromthe formal concepts whose simplified extent exactly con-tains two classes across ontologies. Although most of themappings extracted this way have already been identi-fied at the lexical level, new additional matches emerge,as exemplified by ?MA:hindlimb bone, NCI:Bone of theLower Extremity?.Complex mappings are traced from the formal con-cepts whose simplified extents contain more than twoclasses from different source ontologies. It means thatthese classes share the same structural relationships toanchors in the intent. Such classes may compose a com-plex mapping, as elaborated in the following.Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 8 of 271 One-to-group mappings. The simplified extentcontains only one class from one source ontologyand multiple classes from the other source ontology.For example, MA:inferior suprarenal vein can bemapped to the group of concepts {NCI:LeftSuprarenal Vein, NCI:Right Suprarenal Vein} as thethree concepts are contained within one simplifiedextent that has no more classes. This one-to-groupmapping comes from the difference in granularitybetween MA and NCI.2 Group-to-group mappings. The simplified extentcontains multiple classes from different sourceontologies, respectively. For example, two groups ofconcepts {MA: sacral vertebra 1, MA:sacral vertebra2, MA:sacral vertebra 3, MA:sacral vertebra 4} and{NCI:S1 Vertebra, NCI:S2 Vertebra, NCI:S3 Vertebra,NCI:S4 Vertebra, NCI:S5 Vertebra} can be mapped asthese classes are contained in one simplified extentthat has no more classes. This group-to-groupmapping represents the difference between mouseand human anatomy.In all the four matching tasks of Table 2, such complexmappings can be identified, as shown in Table 5, wherethe classes within one mapping are of the same type, thusthe logical constructor used in the semantic expressions isdisjunction. Note that no extra operations are needed inFCA-Map for identifying such complex mappings as theyand the one-to-one mappings are implied similarly in theformal concepts derived from the positive relation-basedformal context.Constructing the property-based formal context toacquire property mappingsProperties across ontologies tend to differ greatly innames, even for ontologies of the same domain [30].Thus, we utilize the structural rather than lexical infor-mation to obtain property mappings. Axioms specifyingwhat properties are used to link the individuals of anchorsin respective ontologies are the core for identifying thecommonalities among properties.Definition 5 The property-based formal context forontology matching is a triple Kpro := (Gpro,Mpro, Ipro),where objects Gpro is the set of all object properties in twosource ontologies, and attributes Mpro is the pairs of one-to-one class mappings. Binary relation (g,m) ? Ipro holdswhere m =< (CAi,CBi), (CAj,CBj) >, i = j, if axiomCAi  ?g.CAj or CAi  ?g.CAj (CBi  ?g.CBj or CBi ?g.CBj) is asserted or can be inferred within one sourceontology.The property-based formal context Kpro of a small partof SNOMED and NCI is displayed in Table 6. Take thesecond column of Table 6 for example. The two cellsare marked because axioms Benign neoplasm of buccalmucosa  ?Finding site.Buccal mucosa and Benign Buc-cal Mucosa Neoplasm  ?Disease Has Primary AnatomicTable 5 Some one-to-group and group-to-group mappings discovered by the positive relation-based formal contextsClasses Semantic expressionsMA Inferior suprarenal vein Inferior suprarenal veinNCI Left Suprarenal Vein, Right Suprarenal Vein (Left Suprarenal Vein unionsq Right Suprarenal Vein)FMA T helper cell type 1, T helper cell type 2 (T helper cell type 1 unionsq T helper cell type 2)SNOMED T helper subset 1 cell, T helper subset 2 cell (T helper subset 1 cell unionsq T helper subset 2 cell)FMA First sacral spinal ganglion, (First sacral spinal ganglionSecond sacral spinal ganglion, unionsq Second sacral spinal ganglionThird sacral spinal ganglion unionsq Third sacral spinal ganglion)SNOMED S1 spinal ganglion, (S1 spinal ganglionS2 spinal ganglion, unionsq S2 spinal ganglionS3 spinal ganglion unionsq S3 spinal ganglion)SNOMED Simian foamy virus, (Simian foamy virusChimpanzee foamy virus, unionsq Chimpanzee foamy virusChimpanzee foamy virus human isolate unionsq Chimpanzee foamy virus human isolate)NCI Foamy Retrovirus Foamy RetrovirusSNOMED Malignant teratoma of undescended testis Malignant teratoma of undescended testisNCI Stage I Immature Testicular Te ratoma, (Stage I Immature Testicular Te ratomaStage II Immature Testicular Teratoma unionsq Stage II Immature Testicular TeratomaStage III Immature Testicular Teratoma, unionsq Stage III Immature Testicular Teratoma)Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 9 of 27Table6Property-basedformalcontextKproofasmallpartofSNOMEDandNCI<?SNOMED:Benignneoplasmofbuccalmucosa,NCI:BenignBuccalMucosaNeoplasm?,?SNOMED:Buccalmucosa,NCI:BuccalMucosa?><?SNOMED:Synoviomabenign,NCI:BenignSynovialNeoplasm?,?SNOMED:Softtissues,NCI:SoftTissue?><?SNOMED:PhocomeliaofupperlimbNOS,NCI:PhocomeliaoftheUpperLimb?,?SNOMED:Upperextremitypart,NCI:UpperExtremityPart?><?SNOMED:Bowenoidpapu-losis,NCI:BowenoidPapulosis?,?SNOMED:Humanpapillomavirusinfection,NCI:HumanPapillomaVirusInfection?><?SNOMED:Insulincoma,NCI:InsulinComa?,?SNOMED:Hypoglycemia,NCI:Hypoglycemia?><?SNOMED:Laparoscopy,NCI:Laparoscopy?,?SNOMED: Endoscopedevice,NCI:Endoscope?>SNOMED:Findingsite×××SNOMED:Dueto××SNOMED:Usingdevice×NCI:DiseaseHasPrimaryAnatomicSite××NCI:DiseaseMayHaveAssociatedDisease×Zhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 10 of 27Site.BuccalMucosa can be inferred in SNOMED andNCI,respectively.The derived concept lattice of Kpro of a small part ofSNOMED and NCI is illustrated in Fig. 4. We can extractproperty mappings from the formal concepts whoseextents contain exactly two properties across ontologies.This means that they are used to connect the samepairs of mappings. For example, ?SNOMED:Finding site,NCI:Disease Has Primary Anatomic Site? is extracted fromnode 4 in Fig. 4.Constructing the restriction-based formal contextto acquire extended and complexmappingsWith the availability of property mappings, we can startexploiting anonymous classes in ontologies, i.e., restric-tions on how to use properties with respect to classes.An axiom with a named class at the left-hand side anda restriction at the right-hand actually defines a neces-sary condition for the class, and the condition becomesnecessary and sufficient in equivalent axioms. When twoclasses in an anchor have necessary conditions (restric-tions) described by the same property, the two classesspecified in the restrictions, i.e., fillers of the property,could possibly be a match across ontologies. We illus-trate this by a validated anchor ?SNOMED:Hemangiomaof liver, NCI:Hepatic Hemangioma?. All the anonymousancestors of these two classes in SNOMED and NCI,respectively, are listed in Table 7. They are either assertedor inferred, as shown in Fig. 5. Since ?SNOMED:Findingsite, NCI:Disease Has Associated Anatomic Site? is aproperty mapping, one can see that the fillers of the prop-erties imply some correspondences across two ontolo-gies. We pair fillers in anonymous ancestors of thetwo classes in anchor, denoted as FP . In the case ofanchor ?SNOMED:Hemangioma of liver, NCI:HepaticHemangioma?, 16 such pairs can be generated. We utilizethese potential matches to construct a FCA formal contextso as to confirm the correct mappings.Definition 6 The restriction-based formal context forontology matching is a triple Kres := (Gres,Mres, Ires),where objects Gres is the set of all classes in one sourceontology, and attributes Mres is the set of all classes in theother source ontology. Binary relation (g,m) ? Ires holdsif (g,m) ? FP , where FP denotes the set of pairs (D,E)from axiom CA  ?g.D (or CA  ?g.D) in one ontologyand axiom CB  ?h.E (or CA  ?h.E) in the other ontol-ogy where ?CA,CB? is a class mapping and ?g, h? a propertymapping.Table 8 showsKres of a small part of SNOMED andNCI,where the gray area corresponds to Table 7. The derivedconcept lattice of Kres of a small part of SNOMED andNCI is illustrated in Fig. 6. Mappings can be extractedfrom the formal concepts according to the simplifiedextent Kex and simplified intent Kin.For a formal concept (A,B) with non-empty simpli-fied intent and simplified extent, Kin represents theattributes uniquely introduced by (A,B) compared withall its ancestors in the lattice. Similarly, Kex is the set ofobjects uniquely introduced by (A,B) compared with allits descendants. Hence, Kin and Kex are introduced byformal concept (A,B) at the same time, in other words,the objects in Kex specifically embody the attributes inKin; and the attributes in Kin describe the most particularcharacteristics of the objects in Kex. In the case of therestriction-based concept lattice, if both Kex and Kinof a formal concept contain exactly one class, then itmeans that these two classes always occur at the sametime as fillers of the same properties in anonymousancestors of anchors. They are more likely a match thanother filler pairs in FP that are also present acrossthe intent and extent of the same formal concept. Forexample, node 7 in Fig. 6 represents a formal conceptwith intent {NCI:Cardiovascular System, NCI:Heart,NCI:Epicardium} and extent {SNOMED:Structureof visceral pericardium, SNOMED:Heart structure,Fig. 4 Concept lattice ofKpro with simplified labelingZhao et al. Journal of Biomedical Semantics  (2018) 9:11 Page 11 of 27Table 7 Anonymous ancestors of SNOMED:Hemangioma of liverand NCI:Hepatic HemangiomaClasses in an anchor Anonymous ancestorsSNOMED:Hemangioma of liver ? Finding site.Structure of cardiovascularsystem? Finding site.Blood vessel structure? Finding site.Vascular structure of liver? Finding site.Liver structureNCI:Hepatic Hemangioma ? Disease has associated anatomicsite.Cardiovascular system? Disease has associated anatomicsite.Vascular system? Disease has associated anatomicSite.Blood vessel? Disease has associated anatomicSite.LiverSNOMED:Structure of cardiovascular system}. Its simpli-fied intent is {NCI:Epicardium} and its simplified extent{SNOMED:Structure of visceral pericardium}, indicatingthat these two classes are always used as fillers at the sametime, i.e., in the restrictions about the same properties forthe same anchor classes across ontologies. For other pairsof classes across the intent and extent of node 7 in Fig. 6,their two classes may occur as fillers at the same time butnot always. Thus ?SNOMED:Structure of visceral peri-cardiumis,NCI:Epicardium? is extracted to be a match.Similarly, node 6 in Fig. 6 yields match ?SNOMED:Atrialstructure,NCI:Cardiac Atrium?.There are formal concepts in the restriction-based lat-tice that have an empty simplified intent (extent) and anon-empty simplified extent (intent), indicating the dif-ference in class hierarchies and expressions of axiomsacross two ontologies. Rather than one-to-one mappings,complex mappings might be implied in such cases. Forexample, node 8 in Fig. 6 has an empty Kin whereas itsKex is {SNOMED:Vascular structure of liver}. Instead ofone class, there may be a complex combination of NCIclasses in the complete intent of node 8 that correspondsto {SNOMED:Vascular structure of liver}. Under a manualreview, a complex mapping is determined, as illustratedin Fig. 7.ResultsTo evaluate the effectiveness of FCA-Map, we conductexperiments on three OAEI 2016 biomedical tracks,the Anatomy, the Large Biomedical Ontologies, and theDisease and Phenotype. Additionally, we run FCA-Mapon the Conference track to test its performance on a rel-atively general-purpose domain. The versions used arethe OWL files of the ontologies provided by OAEI 2016,and the precision, recall and F-measure values listed inthe subsequent subsections are computed based on thereference alignments provided by OAEI. In the LargeOliveira and Pesquita Journal of Biomedical Semantics  (2018) 9:1 DOI 10.1186/s13326-017-0171-8RESEARCH Open AccessImproving the interoperability ofbiomedical ontologies with compoundalignmentsDaniela Oliveira1,2* and Catia Pesquita2AbstractBackground: Ontologies are commonly used to annotate and help process life sciences data. Although their originalgoal is to facilitate integration and interoperability among heterogeneous data sources, when these sources are annotatedwith distinct ontologies, bridging this gap can be challenging. In the last decade, ontology matching systems havebeen evolving and are now capable of producing high-quality mappings for life sciences ontologies, usually limited tothe equivalence between two ontologies. However, life sciences research is becoming increasingly transdisciplinaryand integrative, fostering the need to develop matching strategies that are able to handle multiple ontologies andmore complex relations between their concepts.Results: Wehavedevelopedontologymatching algorithms that are able to find compoundmappings betweenmultiplebiomedical ontologies, in the form of ternary mappings, finding for instance that aortic valve stenosis(HP:0001650) isequivalent to the intersection between aortic valve(FMA:7236) and constricted (PATO:0001847). The algorithmstake advantage of search space filtering based on partial mappings between ontology pairs, to be able to handle theincreased computational demands. The evaluation of the algorithms has shown that they are able to produce meaningfulresults, with precision in the range of 60-92% for new mappings. The algorithms were also applied to the potentialextension of logical definitions of the OBO and the matching of several plant-related ontologies.Conclusions: This work is a first step towards finding more complex relations between multiple ontologies. Theevaluation shows that the results produced are significant and that the algorithms could satisfy specific integrationneeds.Keywords: Biomedical ontologies, Ontology alignment, AlgorithmsBackgroundLife sciences research is becoming increasingly integra-tive, with research areas such as Systems Biology andTranslational Medicine bridging distinct domains to pro-vide novel insights. The need for data integration acrossdomains coupled with the massive amounts of data beingproduced both by biological and clinical domains posesnew challenges. A common strategy to deal with thisdata deluge involves linking the information to ontolo-gies, making it easier to search through databases andto develop algorithms to process information. Ontologies*Correspondence: daniela.oliveira@insight-centre.org1Insight Centre for Data Analytics, NUI Galway, Galway Business Park, Dangan,Galway, H91 AEX4, IrelandFull list of author information is available at the end of the articlehave been remarkably successful in the life sciences, espe-cially in the biomedical domain, where the Gene Ontology[1] is the most notable success case. BioPortal1, a por-tal for life sciences ontologies, lists over 400 ontologiesdedicated to diverse domains ranging from molecules tophenotypes.However, when data is annotated with different ontolo-gies, to allow data interoperability the ontologies them-selves need to become interoperable. This can be achievedthrough a process called ontology matching [2], wherebymeaningful links are established between semanticallyrelated concepts. The matching of biomedical ontolo-gies poses specific computational challenges due to theirlarge size and vocabulary complexity [3], and also by theirincreasing semantic richness in the form of new kinds© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Oliveira and Pesquita Journal of Biomedical Semantics  (2018) 9:1 Page 2 of 13of relations between classes and complex axioms. Theseopen challenges have attracted the interest of the com-munity and spurred the inclusion of specific tracks dedi-cated to biomedical ontologies in the Ontology AlignmentEvaluation Initiative [4].Currently, ontology matching techniques and systemsare mostly devoted to finding links between two equiva-lent entities from two distinct ontologies, but when datacrosses domains, the need arises for matching techniquesthat go beyond this and allow linking more than twoontologies through more complex relations. Compoundontology matching [5] allows the matching of severalontologies with distinct but related domains through theestablishment of compoundmappings that involve severalentities. A specific case is the ternary compound mappingwhereby two classes are related to form a class expressionthat is then mapped to a third class. For instance, the classHP:0000337 labelled broad forehead is equivalent toan axiom obtained by relating the classes PATO:0000600(increased width) and FMA:63864 (forehead) via anintersection. Such mappings allow a fuller semantic inte-gration of multidimensional semantic spaces, supportingmore complex data analysis and knowledge discoverytasks.In this paper, we present a set of new algorithmswhich are able to create ternary compound alignments forlarge biomedical ontologies. The algorithms were evalu-ated against reference ontology alignments and appliedto potentially extend ontology logical definitions and tomatch plant ontologies.Related workOntology matching can be defined as a function f thatreturns an alignment between the classes of a pair ofontologies O and O? [2]. An alignment consists of aset of correspondences (mappings) between semanti-cally related entities of different ontologies. This pro-cess can be extended by using other parameters andresources, e.g., weights, thresholds, and even externalknowledge. Most ontology matching systems usuallyinclude three main types of components: (1) loadingand pre-processing, where ontology files are loaded andother procedures are employed such as normalizationof labels; (2) matching, where pairs of mapped ontol-ogy entities are given a score reflecting their close-ness; (3) refinement, where the list of mappings is fil-tered to adhere to quality, cardinality and consistencyrequirements among others. Typically, ontology match-ing corresponds to binary mappings between classes,properties or instances. However, more complex kinds ofontology matching that extend the definition have beenproposed.One of the first steps in this direction was the defini-tion of complex ontology matching, which is commonlydescribed as a correspondence between two classes fromtwo different ontologies, where one of them is a com-plex concept or property description. It involves onlytwo ontologies, but each mapping relates to more thantwo entities in those ontologies. An example of a com-plex mapping could be the alignment of the conceptAcceptedPaper in one ontology, to the entity Paperin a second ontology, which has the associated propertyAccepted [6]. Ritze et al. [7] developed a pattern-basedapproach to finding these mappings, where they presentcorrespondence patterns and define matching conditionsfor each of them.The CGLUE [8] and the iMAP systems [9] were bothdeveloped to find complex matches. CGLUE applies a rulelearning process and iMAP uses several searchers, eachconsidering a meaningful subset of the space, to find com-plex mappings. They both apply a beam search to controlthe search through the space of candidate matches, givenits large size.Partial matching has also been investigated. Dhom-bres and Bodenreider [10] employed lexical and logicalapproaches to derive partial mappings for theHP ontologyand SNOMED CT.A related, but more complex approach, is compoundmatching [5] which is the process of identifying com-pound mappings, i.e. matches between class or prop-erty expressions involving more than two ontologies.This means that a ternary compound mapping is atuple <X,Y,Z,R,M>, where X, Y and Z are classes fromthree distinct ontologies, R is a relationship establishedbetween Y and Z to generate a class expression thatis mapped to X via a mapping relation M. The ontol-ogy to which X belongs is considered to be the sourceontology, and the ontologies that define Y and Z are con-sidered as the target ontologies. In this particular case,the relation R is always an intersection (regardless ofany qualifier) and the mapping M an equivalence. Theconcept of compound alignment is defined as a set ofmappings between classes from a source ontology Osand class expressions obtained by combining two otherclasses, each belonging to a different target ontologyOt1 and Ot2.To the best of our knowledge, there are currently noontology matching systems capable of generating suchmappings. However, a preliminary approach was testedby [5] that first matched the source ontology to eachof the target ontologies individually, using an anchor-based word matching algorithm, and then matched allpairs of target classes that map individually to the samesource class. Despite the reduced search space, theycould not test their algorithm in larger sets of ontolo-gies and evaluated only the MP-PATO-CL and MP-PATO-NBO alignments, obtaining recall values of 30and 11% respectively, but precision values below 1%.Oliveira and Pesquita Journal of Biomedical Semantics  (2018) 9:1 Page 3 of 13This work was the starting point for the develop-ment of our novel approach (with preliminary resultspresented in [11]).MethodsThe design, development and implementation of com-pound matching algorithms involved three stages: (i) anexploratory stage, which consisted in a pattern anal-ysis of a representative set of biomedical ontologiesto devise strategies and explore the challenges of thedevelopment of compound matching algorithms; (ii) theadaptation and extension of existing classical match-ing algorithms to compound ontology matching, whichwere (iii) implemented in a state-of-the-art ontologymatching system.Pattern analysisThe first stage of this work had an exploratory natureand aimed to understand the mappings between sourceand target ontologies and to seek new strategies to applyto ternary compound matching. We used the ontologiesfor which we were able to create a reference alignmentfor compound matching from logical definitions of OBOontologies (see Reference alignments section).Table 1 presents these biomedical ontologies with thenumber of different classes and names (labels and syn-onyms) that each one had at the time of the download.Using a source ontology and a single target ontology asinput, several binary alignments were created by apply-ing AMLs Word Matcher and String Matcher (see theAgreementMakerLight section). The mappings of thosealignments were manually analysed to uncover the fol-lowing patterns: (1) addition, where the source or targetclass label had one or more extra words; (2) variation,which had labels with the same number of words, butone word did not match; (3) combination with map-pings that combined the previous patterns; and, (4) fullmatch, which had terms that match completely, but cansometimes have words in a different order. The referencealignments were also split into pairs to form binary align-ments and a manual search for the previously definedpatterns was performed. This search led to the discov-ery of a new pattern which is the occurrence of synonymsbetween the two classes that are being matched. Table 2shows one example mapping for each of the situationsdescribed.The analysis of all the alignments led to the conclusionthat the majority of the mappings fit in at least one of thepattern categories. Most, however, are a combination ofthe first two patterns, with the addition pattern being themore prevalent one (see Table 3).The mappings that were classified with the additionpattern are mostly partial mappings, i.e., only somewords matched between the labels of the classes mapped.Dhombres and Bodenreider [10] worked on a methodto identify partial lexical matches between HP andSNOMED CT. The authors used existing matching tech-niques and extended them to find partial mappings.Their approach identified 7358 partial lexical matchesand 82% of them had an inferred logical mapping. Com-paring with the 14,535 mappings analysed approximately82% fit the addition pattern and can be considered apartial match.These findings served as the conceptual foundationfor the development of the compound matching algo-rithms. For instance, the prevalence of theaddition pat-tern indicated that a bag-of-words approach could bean efficient solution. The existence of mapped classeswith different word order, however, can change themeaning of a concept in a class and this situationwould be overlooked by the bag-of-words approach.This approach would also not directly handle the syn-onym pattern. Finally, the variation pattern led to theuse of a popular word stemmer, the Snowball stemmer2,which was applied to the words in each label of allthe classes.Table 1 Biomedical ontologies downloaded from the OBO Foundry in May 2015 (http://obo.sourceforge.net)Ontology Acronym Classes Names ReferenceCell type CL 4775 4375 [29]Foundational model of anatomy FMA 78977 126190 [30]Gene ontology (biological process domain) GO 43048 276577 [1]Human phenotype HP 28621 18431 [31]Mammalian phenotype MP 28643 29592 [32]Neuro behaviour ontology NBO 116710 1168 [33]Phenotypic quality PATO 2497 3378 [34]Uber anatomy ontology UBERON 18322 50713 [35]Caenorhabditis elegans phenotype WBP 2290 2739 [36]Oliveira and Pesquita Journal of Biomedical Semantics  (2018) 9:1 Page 4 of 13Table 2 Examples of the patterns found in a manual analysis of binary alignmentsPattern Source URI and label Target URI and labelAddition WBP:0001911 axon regeneration defective GO:0031103 axon regenerationVariation MP:0002269 muscular atrophy GO:0014889 muscle atrophyCombination MP:0013527 absent conjunctiva goblet cells CL:2000084 conjunctiva goblet cellFull match MP:0002119 dipsosis NBO:0000541 dipsosisSynonym HP:0010108 aplasia of the hallux FMA:25047 big toeNone MP:0002229 neurodegeneration GO:0070657 neuromast regenerationAlgorithm implementationThe CompoundMatching algorithm has three main steps:Step 1 - First-pass recall selection.The algorithm performs a pairwise mapping of thelabels of the source ontology with the labels of the targetontology to match first (target 1). Each word is weightedby its Evidence Content (EC) [12]. The EC is the inverselogarithm of the frequency of a word and reflects the usageof that word within the ontology. The similarity is thencalculated by finding the ratio between the sum of the ECTable 3 Distributions of mappings fitting lexical patterns 1 or 2Matcher Ontology Addition Variation SizeString MatcherMP-CL 26 7 34MP-GO 287 210 501MP-NBO 354 205 594MP-UBERON 58 11 71WBP-GO 182 137 322HP-FMA 272 23 304MP-PATO 18 1 29WBP-PATO 28 2 41HP-PATO 12 1 25Word MatcherMP-CL 4 1 5MP-GO 32 25 65MP-NBO 118 44 219MP-UBERON 42 5 50WBP-GO 183 33 219HP-FMA 158 44 252MP-PATO 33 21 59WBP-PATO 19 1 25HP-PATO 6 0 12ReferenceMP-CL 439 12 474MP-GO 805 83 944MP-NBO 177 24 219MP-UBERON 1693 126 1999WBP-GO 256 39 325HP-FMA 1691 66 1893MP-PATO 3096 35 3636HP-PATO 1710 8 1893WBP-PATO 302 4 325Total 12001 1168 14535Oliveira and Pesquita Journal of Biomedical Semantics  (2018) 9:1 Page 5 of 13of the words shared by the source label (ls) and the target1 label (lt1), and the sum of the EC of the words in lt1.sim1 (ls, lt1) =?EC (word ? (ls ? lt1))?EC (word ? lt1) (1)Step 2 - Search space reduction.The algorithm filters out all mappings with similar-ity below a given threshold and removes all the sourceclasses which were not mapped to any target 1 classes. Italso reduces the number of words of the source labels byremoving from themapped classes all the words that had amatch with a word from a target 1 class. Taking the exam-ple of Aortic valve stenosis (HP:0001650), after matchingHP with FMA, which would capture the mapping for aor-tic valve (FMA:7236), HPs class label would be reducedto stenosis.Step 3 - Longest match precision selection.For each of the remaining mappings, the algorithm per-forms a pairwise mapping of the reduced source labelsagainst the labels of the last target (target 2). In this step,however, the denominator corresponds to the sum of ECof the words in the longer label, to ensure a completematch.sim2 (ls, lt2) =?EC (word ? (ls? ? lt2))?EC(word ? longest (ls, lt2)) (2)The final similarity between the matched labels is com-puted as the average between the similarities computed insteps 1 and 2. Mappings with an average below the secondthreshold are filtered out.The resulting alignment is a list of all mappings abovethe selected threshold, without any consideration forcardinality. To ensure proper cardinality, refinement (orselection) strategies need to be employed. The referencealignments have a cardinality of 1, meaning that for eachsource class there is a single compound mapping. How-ever, given the potential for conflicts, it was also desirableto investigate the option of allowing two mappings for thesame source class. To this end, both a top-one and top-tworanked selectors were implemented.Both are greedy algorithms that select mappings basedon their similarity. They start by sorting the mappingsin the compound alignment in descending order of theirsimilarity values. When there are competing mappingswith equal similarity values, the top-one selector choosesa single mapping taking the one that was sorted as first,whereas the top-two selector, chooses the two first sortedmappings.AgreementMakerLightThe AgreementMakerLight (AML) ontology matchingsystem [13] focuses on the efficient matching of very largeontologies and is one of the most successful systems foraligning ontologies [14]. AML has three main modules:(1) ontology loading, (2) ontology matching and (3) align-ment selection and repair. When an ontology is loadedinto AML, a Lexicon is built with all class labels and syn-onyms. AML has several matchers that explore lexical andstructural information. The selection and repair moduleensures that the final alignment has the desired cardinalityand removes mappings causing logical inconsistencies.In this work, we adapted the loading module to han-dle three ontologies. The implementation of our matchingalgorithm takes advantage of the data structures AMLbuilds for its Word Matcher. The Word Matcher uses abag-of-words approach and creates a new Lexicon withevery word frequency and EC. The similarity betweenclasses of different ontologies is then based on a weightedJaccard index. We also made use of AMLs selectionstrategies, which were adapted to work over compoundmappings.EvaluationThe compound alignments were evaluated with classifi-cation metrics automatically against reference alignmentsand also manually, to better understand the results andpoint towards possible improvements.Reference alignmentsThe technique for the construction of the compound ref-erence alignments used in the evaluation originated fromthe work of [5], where ternary compound mappings werederived from logical definitions of OBO ontologies to beused as a gold-standard.Logical definitions are applied to classes and use genus-differentia constructs of the form X is a G that D, whereX is the defined class, G is the genus and D the differentia.The genus is a more general class than X and D discrim-inates instances of X from other instances of G [15]. Thefollowing text shows an example of a logical definition:id: MP:0000216 ! absent erythroidprogenitor cellintersection_of: PATO:0002000 ! lacksall parts of typeintersection_of: inheres_in CL:0000038! erythroid progenitor cellOBO ontologies with over 100 logical definitions thathad a class expression intersected by two classes from twoother ontologies were selected (see the example above).Following these rules, we created six reference align-ments, which determined the sets of biomedical ontolo-gies used throughout this work.Precision, recall and F-measureThe automatic evaluation of the algorithms was per-formed based on the classification metrics precision,Oliveira and Pesquita Journal of Biomedical Semantics  (2018) 9:1 Page 6 of 13recall and F-measure, which are defined in this context asfollows.Pr(A) = | {m|m ? A ? m ? R} ||A|Rec(A) = | {m|m ? A ? m ? R} ||R| (3)F-measure(A) = 2 · Pr(A) · Rec(A)Pr(A) + Rec(A) (4)where A is an alignment resulting from the algorithmsdeveloped, Pr(A) and Rec(A) are the precision and recall,respectively, of the alignment. m is a mapping in analignment. R is the reference alignment to which A iscompared.ThresholdsThe thresholds used in the evaluation process weredefined through a series of tests aimed to find consistentvalues across all six sets of ontologies, which returned thebest metrics.The evaluation process involved testing the algorithmswith different thresholds for the first and third algorithmsteps and checking which were the two optimal valuesto use throughout the evaluation process. These valueshad to return good precision or recall but also needed tohave a reasonable runtime with a considerable amount ofmappings found3.The first-pass recall selection needs to return a highrecall so that the search space is not too narrowed forthe subsequent steps of the algorithm while still providinggood filtering of irrelevant mappings. For this to happenthe first threshold (T1) needs to be low to return a highrecall, at expense of the precision. To determine the bestT1 the compound reference were reduced to binary ref-erences by removing the second target from each class.Barros et al. Journal of Biomedical Semantics  (2018) 9:18 https://doi.org/10.1186/s13326-018-0186-9RESEARCH Open AccessDisease mentions in airport and hospitalgeolocations expose dominance of newsevents for disease concernsJoana M. Barros1* , Jim Duggan2 and Dietrich Rebholz-Schuhmann1,3AbstractBackground: In recent years, Twitter has been applied to monitor diseases through its facility to monitor userscomments and concerns in real-time. The analysis of tweets for disease mentions should reflect not only user specificconcerns but also disease outbreaks. This requires the use of standard terminological resources and can be focusedon selected geographic locations. In our study, we differentiate between hospital and airport locations to betterdistinguish disease outbreaks from background mentions of disease concerns.Results: Our analysis covers all geolocated tweets over a 6 months time period, uses SNOMED-CT as a standardmedical terminology, and explores language patterns (as well as MetaMap) to identify mentions of diseases inreference to the geolocation of tweets. Contrary to our expectation, hospital and airport geolocations are not suitableto collect significant portions of tweets concerned with disease outcomes. Overall, geolocated tweets exposed a largenumber of messages commenting on disease-related news articles. Furthermore, the geolocated messages exposedan over-representation of non-communicable diseases in contrast to infectious diseases.Conclusions: Our findings suggest that disease mentions on Twitter not only serve the purpose to share personalstatements but also to share concerns about news articles. In particular, our assumption about the relevance ofhospital and airport geolocations for an increased frequency of diseases mentions has not been met. To furtheraddress the linguistic cues, we propose the study of health forums to understand how a change in medium affects thelanguage applied by the users. Finally, our research on the language use may provide essential clues to distinguishcomplementary trends in the use of language in Twitter when analysing health-related topics.Keywords: Social media, Disease surveillance, SNOMED-CT, MetaMap, Part-of-speech tagging, GeolocationBackgroundThe increase in life expectancy through better health ofthe world population has mainly been achieved throughadvancements in the fields of medicine, biology andmicrobiology [1]. However, it becomes increasingly cru-cial to public health research to detect, monitor, treat andavoid threats to population health [2]. Thus, public healthhas benefited from the use of surveillance [3] which hasbeen crucial for the detection of disease outbreaks and itscounter-actions in our modern information society. This*Correspondence: joana.barros@insight-centre.org1Insight Centre for Data Analytics, Data Science Institute, NUI Galway, LowerDangan, Galway, IrelandFull list of author information is available at the end of the articlehas become a key issue for public health and has led to theapplication of new sources of valuable health information.Modern sources of data such as search engine queries[4] and online news [5, 6] can provide near real-time,government independent information through differentchannels, and have been harnessed in the health domain.In recent years, social media networks have moved intothe focus of the research; this medium fosters the shar-ing of health-related content (e.g. personal experiencesand opinions), thus, being the preferred platform for shar-ing information [7]. One of such platforms is Twitter[8]. This resource is being used by over 310 millionusers worldwide [9] who publish their messages to thepublic (i.e. tweets) possibly in combination with the© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Barros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 2 of 11location of the individual; thus, it supplies a continu-ous stream of data useful to monitor public health con-cerns such as disease spread. Twitter has been exploitedto monitor disease awareness and surveillance [1013],suggesting its usefulness for evaluating the health state ofa population. The available location information helpedto identify global movement patterns [14] and hasbeen integrated into specific applications in the healthdomain [1517].Given the richness of this source, we take advantage ofthe rapid availability of data, textual features and geoloca-tion provided from Twitter. We focus on the full range ofillnesses, including infectious and non-communicable dis-eases, to determine the scope of disease mentions in socialmedia. The origin of the tweets is given special attentionto contrast hospital geolocated tweets with those fromairports. Furthermore, we address the linguistic cues,provided by the users, when health is discussed. By com-paring both infectious and non-infectious diseases, wehope to discover if language and/or locations features canbe used to uniquely characterise these categories. Thisresearch is based on the hypothesis that large-scale socialmedia data can provide new insights about the health stateof the population through the analysis of language andwith a focus on location.Our research is based on the following assumptions:1 Twitter is a prime news medium where a wide rangeof illnesses are discussed. This enables the detectionof different patterns in the discussion of selecteddiseases, and as a consequence allows linking ofworldwide events with such disease mentions.2 Considering the location plays an important role indetermining the relevant health mentions and inmonitoring specific areas for their distribution ofhealth mentions. As a primary assumption, weexpected that a hospital location would inflate thenumber of disease mentions, given the purpose of thelocation.3 Different language styles could be predominant whencommunicating different illnesses; knowing thelanguage patterns could help to identify non-explicitmentions of a given disease. Furthermore, differentlanguage patterns may be attributed to differentlocations.Related workIn the health domain, there is an increased interest inthe use of social media analytics. The first exploita-tion of Twitter in this regard was performed by [18]to improve market predictions based on external infor-mation, in this case, using the public belief concerningthe likelihood that H1N1 (i.e. swine flu) would turn intoa pandemic. For the case of specific diseases and withthe focus on the health state of the population, Twit-ter was initially tested for the case of influenza (i.e. flu)in the areas of surveillance and prediction [12, 1921].This illness was comprehensively researched due to theavailability of well documented and historic gold stan-dard data, its seasonality, and its ease in infectingothers [22]. In this case study, more attention was initiallygiven to specific words (i.e. keywords) or individual words(i.e. unigrams) present in a tweet to select potentiallyrelevant messages [12, 1921]. However, further devel-opments led to machine learning approaches which takeadvantage of additional features such as n-grams [11, 23],regular expressions [13], user behaviour [24], and partof speech [25] to further filter relevant messages. Withthe results achieved for influenza, other diseases such asEbola [26], food-borne illnesses [27], respiratory illnesses[28], and mental health diseases [29], were researchedfollowing comparable methodologies. Geolocation hasbeen harnessed to focus on specific cities, regions, andcountries [12, 17, 30, 31] and to study disease diffusionnetworks [15, 16, 23, 27].Given this, there is still research to be conducted regard-ing how the proximity to disease-prone locations influ-ences Twitter users, especially at a language level. Inaddition, these findings could elucidate on what and howinformation is shared. There is also a lack of researchregarding the identification of multiple diseases fromtweets, although, this is partially addressed by topic mod-elling approaches [32, 33].Following the same principles described above, theanalysis of language could be used to distinguish non-communicable disease mentions in tweets from infectiousdisease mentions. For example, it is expected that usersapply a different language when being concerned aboutcancer and food poisoning. So far, Twitter has been usedto analyse specific disease outbreaks, which required tocapture specific mentions of disease. By contrast, weobserve in this study the full spectrum of disease termsto better analyse the language use of disease mentions onTwitter for the outbreak or trend development of differentdisease types.Furthermore, we use the geolocation to differentiateTwitter use for hospital visitors in contrast to airports. Inour primary assumption, we expected that Twitter use inhospitals is focused on specific disease mentions, whereasthe use of Twitter at airports could form an indicatorfor the early detection of communicable diseases. Cer-tainly, the geolocation restricts the amount of Twitter dataand the use of medical terminology for the identificationof disease mentions may not necessarily reflect the men-tion of diseases in the daily common language in the useof Twitter. However, monitoring the full amount of datapublished through Twitter should give sufficient input toanalyse the questions addressed above.Barros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 3 of 11MethodsDataThe collected Twitter data amounts to 58751297 tweetsgathered between the 26th of October 2016 and the 27thof March 2017. This was performed using the Twitterapplication program interface (API) [34] by collecting onlytweets containing latitude and longitude coordinates (i.e.geolocated tweets) and written in English, using the APIlanguage filter. These messages were stored using Mon-goDB [35] due to its document-oriented construction,efficiency in querying large quantities of documents andscalability [36]. To improve the signal-to-noise ratio, weapplied regular expressions to remove job advertisements(e.g.Were #hiring! Read about our latest #job openinghere: St. Louis Trauma Hospital Seeking Multiple Special-ties) and predefined location sharing messages (e.g.Imat Terrabela Zona Sul in Porto Alegre, RS). Subsequently,the data set was filtered according to the proximity oftweets to airports (airport collection) and hospitals (hos-pital collection). We chose an area within a 3 km distancesurrounding the airports, motivated by the large size of anairport, and a 0.2 km radius surrounding hospitals. Theremaining set of messages constitutes the geolocated col-lection. The airport coordinates were retrieved from theOurAirports [37] database; after considering only largeairports the result consisted of 575 locations. Regardingthe hospitals, to gather a large sample we utilise OpenStreet Map [38] to automatically collect 77989 locationsworldwide.With this partition of the data, we can focus on identi-fying the differences that location poses on the frequencyand language for distinct illnesses. In particular, the tar-geted locations that constitute disease hot spots due totheir nature. Increases in international travel are raisingconcerns regarding travel-associated illnesses [39], andhospitals are inherently prone to have a high frequency ofsick people.Disease termsClinical terms have been collected from the SystematizedNomenclature of Medicine  Clinical Terms [40], the ref-erence source for a comprehensive and precise coverageof clinical terms. These terms, referred to as diseaseterms in this manuscript, have been selected from theDisease class of the terminology, as depicted in Fig. 1. Theretrieval was performed through the Bioportals API [41]and, to achieve a broader scope, for each class the corre-sponding sub-classes and synonyms have been collected.The synonyms enable the normalisation of the diseasesnames, i.e.laymans terms are considered as well as propermedical terminology. Due to the limits in Twitters mes-sage length (in characters) and due to the complexityof specific disease terms, we decided to remove nameswhich are composed of more than three terms withoutFig. 1 Disease terms retrieval. The disease terms were collected fromthe Disease class and its sub-classes. In the example acute idiopathicthrombocytopenic purpura is a sub-class of idiopathic thrombocytopenicpurpura, and this disease is a sub-class of idiopathic diseaseconsidering determiners, conjunctions and prepositions(i.e. of, from, the, a, and, to), which were fre-quently observed in the list of disease terms. The remain-ing disease terms and synonyms were utilised for theselection of relevant tweets. The search for the retrievedterms and synonyms was performed using the completeset of terms (e.g. muscle atrophy was searched as a strictterm in contrast to combinatorial variants of muscle andatrophy), followed by a search using the synonym list.Part-of-speech taggingTweets contain a variety of special characters, therefore,we applied five modifications to increase the perfor-mance of the pattern identification: (1) the disease term, ifpresent, was normalised into DISEASE; (2) usernames,defined by words immediately preceded by the symbol @,were replaced by @username; (3) URLs were replacedby URL; (4) the @ symbol was replaced by at when notsucceeded or preceded by a word; and (5) the remainingpunctuation symbols have been removed. Subsequently,each tweet was tokenized and a Part-of-Speech (POS)tag was assigned to each token. These steps were per-formed using the TweetTokenizer and the POS taggerfrom Pythons Natural Language Tool Kit (NLTK) pack-age [42], as well as the Penn Treebank tag set for thePOS tagger. It was decided to focus on POS due to theirability to provide a general grammatical tag based on aword definition and its context. NLTK was chosen givenits widespread use and good performance. To producethe POS patterns, a rule-based approach, exemplified inFig. 2, was followed. This approach permitted to focus onPOS tags surrounding the disease term which we hypoth-esise being related to the semantics of the disease term inthe tweet.Barros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 4 of 11Fig. 2 POS patterns example. Patterns are created around the diseaseterm. When it is not possible to have an equal number of POS tags oneach side, the pattern stops. For this example, four patterns werecreatedNamed entity recognitionThe disease terminology collected for this researchincludes polysemous terms mainly due to the presenceof synonyms for some of the illnesses. Although the con-sideration of synonyms allows for a more appropriaterepresentation of the layman language used on Twitter,it can also lead to the dubious semantics of the terms,rendering the tweet unusable for monitoring the healthstatus of the population sample. To address this issue, weapply Named Entity Recognition (NER) techniques. Withthis, we try to classify entities (i.e. disease names is thiscase) contained in each tweet. Given that disease termsare not common entities in NER tools provided by popu-lar services such as Stanford NER [43] and GATEs TwitIE[44], we decided to use MetaMap [45]. MetaMap utilisesthe unified medical language system (UMLS) metathe-saurus to identify concepts referenced in the presentedtext, the relevance is given through theMetaMap Indexing(MMI) score which has a maximum score of 1000 cor-responding to highest relevance. Although specialised tobiomedical text, we chose this tool given its suitability forthe purposed task.Results and discussionData explorationThe retrieval and filtering of the clinical terms amountedto 21080 disease names and 19813 synonyms. Bothnumbers differ due to the lack of synonyms for somedisease terms.The full analysis of all 58751297 with regards to dis-ease mentions and geolocation produced the followingresult: 242 messages are within the 3 km radius of air-ports, 132 occur near hospitals, and the remaining 10242are assigned to the geolocated collection. From the 132messages within the 0.2 radius from hospitals, 3 are simul-taneous within 3 km of airports. In total, 10613 poten-tially relevant tweets have been identified, i.e. containinga disease mention that could be normalised to the diseaseterms.Given the contrast between the number of hospitallocations and airport locations, the smaller number ofretrieved messages occurring near hospitals suggest that:(1) the proximity to hospitals does not induce a signifi-cant number tweets covering disease mentions; and (2)there seems to be only a small number of users (withgeolocation activated) tweeting near hospitals.Data statisticsFurther analysing the complete set of 10613 tweets, 493disease terms and synonyms have been identified in thedata set, with 302 present in more than 1 message. As afirst step, we utilised tweets in each collection to deter-mine the distribution of the disease terms. For the hos-pital and airport tweets, results are shown in Figs. 3and 4 where terms with a frequency lower than 1 and 2,respectively, were omitted. The term distribution for thegeolocated collection is present in Fig. 5. In this case, itwas decided to only show the terms with a frequencyhigher or above 30. The decision to omit certain termsis due to simplicity and improved readability of thepresented figures.The results show that 21 disease terms are commonto all collections, which includeviral hepatitis, alwayssleepy, heart failure, kidney stone, brain damage,mosquito bite, among others. Knocked out, Lassafever, heart disease, substance abuse, mental disor-der, and culture shock are present in high frequencyin both collections. The term human immunodeficiencyvirus (HIV), the most frequent in the airport collec-tion, is absent in the geolocated and hospital collections.Upon further inspection, it was found that these messageswere created by an organisation, which raises awarenessof stigma, to share information about HIV related news.Additionally, we compare the least frequent terms in allcollections. For the geolocated sample, 383 terms appearin less than 10 messages. In the airport collection, 50terms have a frequency below 10. The hospital collectionhas 55 terms with a frequency lower than 5. Consideringthe three samples, multiple myeloma is the only termcommon to all. Upon further inspection of the low fre-quency tweets, it was clear that personal statements werevastly more common than the presence of news titles,especially in the geolocated collection. For the airport andhospital collections this occurs to a lesser extent.These results suggest that less mentioned terminology,in our dataset, is correlated with an increase in personalmessages, which are useful to monitor the populationshealth, and more frequent term occurrences are inflateddue to the repetitiveness of news article titles. However,term specificity is another important factor. Disease termssuch as acute laryngopharyngitis, metabolic acidosis,Barros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 5 of 11Fig. 3 Terms distribution for the hospital collection. A total of 55 messages containing terms with a frequency of 1 were omitted for simplicity. Intotal, 61 disease terms are present in this collectionsegmental vitiligo, and plantar fasciitis use specialisedterminology of which the vast majority of the popula-tion is unaware. This may suggest the frequency of 1for these terms. On contrary, terms such as bee stingand mosquito bite, although appearing in 5 messages,use informal terminology more characteristic of Twittersusers.For the second step, we consider all collections as a sin-gle dataset. For the remaining of the paper, we will focuson terms with an occurrence frequency above 199; thissubset corresponds to? 50% of the Twitter data collectionhence we believe it to be appropriate for further analysisand it provides a reasonable amount of messages for eachdisease mention. Additionally, we utilised this thresholdgiven the high volume our data and to guarantee readabil-ity throughout the paper. All the terms found correspondto the diseases clinical terminology, i.e. the terms are notsynonyms. The filtering of terms with more than threewords reduced the complexity hence their presence in thedata set, given that we expect simple English to be pref-erentially used in Twitter. The disease terms range fromclear health-related terms (e.g. heart disease, mentaldisorder, brain damage) to clinically less relevant terms(e.g. knocked out). All messages with a disease termFig. 4 Terms distribution for the airport collection. Terms with a frequency of 1 (n = 26) were omitted for simplicity. In total, 29 disease terms arepresentBarros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 6 of 11Fig. 5 Terms distribution for the geolocated collection. Due to their high frequency and for simplicity, terms appearing in less than 30 messageswere omitted (n = 432). The total number of disease terms present is 56have been further analysed to determine the distributionof terms and the use of language in the tweets. The resultsof this analysis are partly presented in Table 1.Considering the disease terms knocked out and cul-ture shock, the messages may use the terms in a non-clinical sense. The first term can be interpreted and oftenbe used in the sense of elimination (e.g. Novak Djokovicknocked out of Australian Open by 117th-ranked DenisIstomin to hand Andy Murray a huge boost URL.), andculture shock is mostly used as a company name (e.g.Cool new socks! Made in Chicago USA! at Culture Shock- Clothing and Records URL). Similarly, cat ear clini-cally corresponds to a malformation of the inner ear [46];however, in the tweets, the term is used to express a cloth-ing item (e.g. I dont like Halloween I just like being ableto wear cat ears again URL). Similar results, although toa lesser extent, can be identified with brain damage andcardiac arrest which are used in song names (e.g. BrainDamage by Pink Floyd is #nowplaying in Veras On TheDrive, Vancouver., #PalmillaBeach pool is #nowplayingCardiac Arrest by #BadSuns #cubevenue).Our analysis and findings reveal that there is a strongpresence and influence of news articles and their distri-bution on the use of medical language when properlyanalysing Twitter feeds. The majority of disease termsoccur frequently in tweets with reference to specific newsarticles or explicitly repeat the article title. As a conclu-sion, newsmedia are the source explicit disease termmen-tions and their frequency form a systematic bias to diseasementions and have to be excluded when analysing Twitterfeeds for surveillance. This also shows that Twitter usersgive high relevance to news media; this phenomenoncould receive particular relevance when determining theimpact of campaigns (exposed as specific news events)Table 1 Disease terms message analysisMessage contentLassa fever The term is only applied in the context ofnews-related messages.Heart disease The term is used to express health concerns innews reports, raise awareness and forpersonal statements.Substance abuse The messages contain news article titles,personal tweets and awareness tweets. Thesemessages also include job advertisementswhich were not filtered by the previous steps.Mental disorder The term is used in personal messages and intweets related to awareness.Eating disorder The messages are related to news stories andpersonal opinions.Kidney disease The term is mostly used in news stories, to alesser extent it is applied in personal tweets.Female genital mutilation All messages correspond to news articles.Heart failure The majority of the messages correspondto news articles and job advertisementsunfiltered by the previous steps. Theremaining messages are personal statementsfrom the users.Brain damage The messages include news stories andpersonal tweets in which the disease term isapplied in a clinical sense. The remaining mes-sages use a term with a non-clinical meaning.Spinal cord injury The term is mostly applied in the contextof news-related messages and jobadvertisements. The remaining tweetscorrespond to personal statements.Content analysis for knocked out and culture shock is present in textBarros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 7 of 11that target given diseases (e.g. awareness campaigns). Fur-thermore, a short-term peak in the frequency of a diseaseterm aligned with the increase in related news articles canbe exploited as an indicator for changes in public con-cerns, perceptions, and opinions for an illness, or could beremoved as an obvious distraction from the surveillanceanalysis.Geographic distributionFor all 10613 messages (see Fig. 6), we show the world-wide geographic distribution of the tweets assigned toeach collection. The presence of English written tweetsoccurring in countries where English is not the nativelanguage occurs due to the high presence of news arti-cle titles written in English. With regards to the subsetselected above, we present in Fig. 7 the geographic dis-tribution of each disease term given the location of themessages. Overall, a large portion of tweets originates inthe United States of America, since this country has thehighest number of Twitter users [47].As an exception, messages about Lassa fever origi-nate from Nigeria and nearby regions, and are entirelyconstituted by news related tweets. We use this caseto further explore the correlation between our Twitterdataset and other sources of health information, in an out-break context. For this, we gathered past information fromthe World Health Organisation (WHO) website [48] andGoogle News [49] (using Lassa fever as a search term).Given the high presence of URLs in our Twitter datasetwe hypothesise that news articles released at a giventime-stamp could be the source of the high frequency oftweets. We then compared outbreak reports or news withspikes in the frequency of tweets mentioning Lassa fever.WHO provided scarce information, including the absenceof reports from October 2016 to January 2017. In addi-tion, we found no clear association between Google Newsand the news mentioned in the tweets. This can be dueto the lack of sources from the major affected countries,in Google News. Lassa fever is considered endemic inregions of sub-developed countries where internet accessis not widely available. However, this data can be leveragedto extract outbreak information from local news sources.These findings suggest that attributing a location mayreduce the capability of identifying disease outbreaksmainly relating to the high volume of news media content.In addition, the presence of news media hinders the con-tent analysis of Twitter with the goal of monitoring thehealth state of a population.Part of speechDisease term part of speechOur POS analysis with regards to the disease mentionsshows that the most frequent POS patterns are singu-lar nouns (NN, 4390 terms), and singular proper nouns(NNP, 4201 terms). To a lesser degree, we identified adjec-tives (JJ, 350 terms), verbs in base form (VB, 339 terms),and prepositions or subordinating conjunctions (IN, 289terms). For themajority of the disease terms, the preferen-tial POS is an NNP or NN, a few exceptions are knockedout which is assigned an IN, as the second most frequentterm, and kidney disease which is attributed a VB alsoas the second most frequent term. Due to the nature ofthe disease term, it was expected a high presence of nounsor proper nouns for selected diseases (e.g. Lassa fever),hence, the results suggest that the term could be used withits clinical meaning. Furthermore, in previous sections, weab cFig. 6 Geographic distribution for the dataset collections. a Hospital, b Airport, c GeolocatedBarros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 8 of 11a b cd e fg h ij k lFig. 7 Disease terms geographic distribution. a Knocked out, b Lassa fever, c Heart Disease, d Substance abuse, eMental Disorder, f Culture shock,g Eating disorder, h Kidney disease, i Female genital mutilation, j Heart failure, k Brain damage, l Spinal cord injuryaddressed the high presence of news articles which is alikely indicator that the term is used in a clinical context.However, it is applied to give reports regarding the diseaseand not to express personal statements from the users.Part of speech patternsConsidering the POS patterns generated through the pre-viously mentioned method, overall, the results suggest astrong presence of NNP. This predominance of nouns hasalready been addressed in the literature [50] and it waslinked to the distinct vocabulary used on Twitter. Thesefindings also suggest a relaxed use of regular grammarconstructs such as the random capitalisation of words.In Tables 2, 3 and 4 we present a selection of the mostfrequent patterns for the hospital, airport and geolocatedcollections. In addition to the presence of NNP, the col-lections also contain IN and coordinating conjunctions(CC). Focusing on the patterns assigned to each disease,the strong presence of NNP is again present.The frequency of each pattern is correlated with thenumber of messages related to news articles. These tweetscontain the same news title, thus, the high frequency ofgiven patterns in disease terms such as Lassa fever andfemale genital mutilation. A similar behaviour can beseen with culture shock, albeit, in this case, it is relatedto the standard pattern used for advertising the prod-uct of a company. In contrast, terms such as substanceabuse, brain damage and spinal cord injury are presentin patterns with similar frequencies. These are similarlylinked to news articles, however, the titles and news storiescontain more diversity and are also repetitive, therefore,contributing to the frequency of the patterns. In addition,the results also suggest that similar patterns, within eachdisease term, tend to occur with similar frequencies.Named entity recognitionMetaMap was able to identify disease terms in 3489tweets from the initial 10618 messages containing the ter-minology. In the hospital collection, Metamap reached ascore of 4.04 and identified terms in 44 messages fromTable 2 POS patterns for the hospital collectionFrequencyDISEASE,NNP 14IN,DISEASE,NNP 6JJ,DISEASE,IN 6The table presents the top 3 patterns with a frequency higher than 5Barros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 9 of 11Table 3 POS patterns for the airport collectionFrequencyNNP,DISEASE,NNP 36NNP,NNP,DISEASE,NNP 16NNP,DISEASE,NNP,NNP 11IN,DISEASE,CC 10The table presents the top 4 patterns with a frequency higher than 9the total of 132 messages. A total of 45 terms such aseating disorder, brain damage, and substance abusewere not identified. Considering the airport collection,MetaMap achieved an average relevance score of 25.49and was able to identify the terms contained in 183 mes-sages out of 242 messages. A total of 23 terms were notidentified, these include terms such as bee sting, shellshock, Lassa fever and bipolar disorder. In the geolo-cated collection, the average relevance score was 3.36 andMetaMap was able to identify the terms in 3272 messagesof a total of 10242. Similarly to the airport and hospi-tal collections, issues regarding the identification of termsin the tweets were verified. For example, despite repre-senting the most frequent term knocked out was alsothe term most difficult to identify to MetaMap. The sameoccurs with Lassa fever and culture shock. Consider-ing the agreement betweenMetaMaps term identificationand the actual terminology, the score is of 0.62 (the termscontained in 2044 tweets are representative of the termi-nology present in the 3272 messages in which MetaMapcan identify a UMLS concept). As an example, we detailthe specific case of the term knocked out. This term ispresent in 103 of the 3272 tweets identified by MetaMap,however, it is never identified by MetaMap as knockedout; the algorithm identifies other disease terms in these103 messages.The results suggest a trade-off between the use of com-plex and colloquial terminology. Although more frequentin Twitter, health-related layman terms can pose signif-icant challenges and require the application of domain-specific semantic disambiguation tools. In addition, wesuspect the characteristics of the messages (e.g. shortlength and possible disregard for grammar rules) mayhave increased the difficulty for MetaMaps algorithm.Table 4 POS patterns for the geolocated collectionFrequencyIN,DISEASE 592DISEASE,NNP 589IN,DISEASE,NNP 418NNP,DISEASE,NNP 401In this table it is represented the top 4 patterns with a frequency higher than 300This is further supported by cases where the same ter-minology was found in some tweets and not in others,despite being referenced in both.ConclusionIn this paper, we tested an approach to determine thepresence of linguistic patterns associated with diseasesand we explored the representation value of a geolocatedTwitter sample. We analysed the full body of Twitterfeeds with geolocation over a period of 5 months. UsingSNOMED clinical terms, we verified a higher presence ofnon-communicable diseases compared to infectious dis-eases. The division of the data showed that hospital andairport locations do not contribute to the increase in thenumber of disease mentions, contrary to our expectation.We also identified questionable interpretations forselected disease terms, exposing non-medical interpre-tations of the medical term (e.g. knocked out, cultureshock). The findings originating from our data explo-ration suggest a high presence of news article titles ormentions in the messages which indicate that currentevents have a strong influence on the diseases frequency.An example is Lassa fever, the majority of the mes-sages correspond to news stories originating from regionsaffected by an outbreak in late 2016/early 2017. Thisfinding provides an alternative way to explore the newscontent provided by Twitter, mainly through the deter-mination of the degree of concern of users and throughthe gathering of outbreak information from local newsproviders, thus, suggesting its utility for disease monitor-ing. However, our findings also suggest that Twitter is notonly used as a medium to share personal statements butalso to disseminate news articles. Furthermore, it suggeststhat users give high relevance and interest to news media.Regarding the POS tagging, the majority tweets exposenoun or proper noun use of the disease term which cor-responds to our expectations and previous findings in thescientific literature. For the POS patterns, the high pres-ence of news articles may have hindered the identificationof linguistic patterns as the ones identified may solelyrelate to the news articles and not individual statementsregarding health conditions. To address this, an additionalstep to remove tweets related to news articles could beimplemented to exclusively analyse personal tweets. Theresults from the NER suggest that despite being usefulto identify tweets containing the correct clinical termi-nology and providing semantic disambiguation, furtherdevelopments are needed to better handle the uniquestyle of the Twitter messages. To further address the lin-guistic cues, we propose the study of health forums tounderstand how a change in medium affects the languageapplied by the users. Furthermore, these insights can pro-vide new information on the complexity of language whendiscussing health.Barros et al. Journal of Biomedical Semantics  (2018) 9:18 Page 10 of 11Using a collection of more than 58 million tweets, weused and determined language patterns, and contrastedthe use of Twitter between airport locations (with a largernumber of feeds) against the hospital location (with asmall number of feeds). As an outcome, we determinedthat these locations are not suitable for the collectionof significant portions of tweets concerned with diseaseoutcomes. Additionally, we verified a high presence of dis-cussion regarding non-communicable diseases.This studyis based on the premise that users utilise Twitter as amedium to share concerns regarding illnesses and thathospital and airport locations would be preferential for thediscussion of certain diseases or diseases in general. Thiswas not verified, in contrast, we found the predominanceand influence of news articles. To closely monitor dis-ease outbreaks, personal statements mentioning illnessesor symptoms are desired. However, our findings can alsobe applied to measure the degree of concern expressed bythe users, although not strictly indicative of an outbreakit can be used use to determine if additional public healthmeasures should be implemented.FundingThis research has been funded by Science Foundation Ireland (SFI) underGrant Number SFI/12/RC/2289.Availability of data andmaterialsPlease contact author for data requests.Authors contributionsJMB designed the study, developed the methodology, collected the data,performed the analysis, and wrote the manuscript. JD provided generalguidance. DRS contributed for the study design, provided general guidanceand revised the manuscript. All authors read and approved the finalmanuscript.Ethics approval and consent to participateNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Insight Centre for Data Analytics, Data Science Institute, NUI Galway, LowerDangan, Galway, Ireland. 2School of Computer Science, NUI Galway, UniversityRoad, Galway, Ireland. 3ZB MED, University Cologne, Gleueler Str. 60, 50931Cologne, Germany.Received: 17 August 2017 Accepted: 25 May 2018Kropf et al. Journal of Biomedical Semantics  (2018) 9:16 https://doi.org/10.1186/s13326-018-0180-2RESEARCH Open AccessQuerying archetype-based EHRs bysearch ontology-based XPath engineeringStefan Kropf1* , Alexandr Uciteli1, Katrin Schierle2, Peter Krücken2, Kerstin Denecke3 and Heinrich Herre1AbstractBackground: Legacy data and new structured data can be stored in a standardized format as XML-based EHRs onXML databases. Querying documents on these databases is crucial for answering research questions. Instead of usingfree text searches, that lead to false positive results, the precision can be increased by constraining the search tocertain parts of documents.Methods: A search ontology-based specification of queries on XML documents defines search concepts and relatesthem to parts in the XML document structure. Such query specification method is practically introduced and evaluatedby applying concrete research questions formulated in natural language on a data collection for information retrievalpurposes. The search is performed by search ontology-based XPath engineering that reuses ontologies and XML-relatedW3C standards.Results: The key result is that the specification of research questions can be supported by the usage of searchontology-based XPath engineering. A deeper recognition of entities and a semantic understanding of the content isnecessary for a further improvement of precision and recall. Key limitation is that the application of the introducedprocess requires skills in ontology and software development. In future, the time consuming ontology developmentcould be overcome by implementing a new clinical role: the clinical ontologist.Conclusion: The introduced Search Ontology XML extension connects Search Terms to certain parts in XMLdocuments and enables an ontology-based definition of queries. Search ontology-based XPath engineering cansupport research question answering by the specification of complex XPath expressions without deep syntaxknowledge about XPaths.Keywords: Electronic health records, Medical informatics applications, Search ontology, Information retrieval,EHR query, Pathology electronic health records, Query engineeringBackgroundPrecise questions on semi-structured medical recordsSince clinicians prefer narratives and dictated speech overrigid entry forms [1], Electronic Health Records (EHRs)are often stored as free text. This information type isreferred to by the term semi-structured, preassumed thedocuments are structured by headers and keywords man-ually assigned by the physicians. This structure is usuallynot technically implemented. Queries on such data cannot be very precise because there is no semantic informa-tion explicitly available as markup in the free text.*Correspondence: stefan.kropf@imise.uni-leipzig.de1Institute for Medical Informatics, Statistics and Epidemiology (IMISE), LeipzigUniversity, Härtelstraße 16-18, 04107 Leipzig, GermanyFull list of author information is available at the end of the articleIn order to specify precise queries on semi-structuredhealth records, a transformation of semi-structured healthrecords into Structured EHRs is required as well as meth-ods for Querying on Structured EHRs.Structured EHRsA well written patient history may be a narrative orstructured document.[...] There is a drive to structureand/or code all clinically relevant information in EHRs tobenefit from computability of information [2]. Not onlymachines, also physicians are benefiting by structureddocuments, because it seems that having an expectationof what to find under a certain heading makes for afaster interpretation of the text [3]. Anyway, there are© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Kropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 2 of 14narrative as well as structured EHRs; and when the physi-cians structure their information using certain keywordsand headers in the narratives, it is possible to transferfree text based medical records into standardized andsection-structured XML EHRs [4]. Querying EHRs bykeywords in certain sections requires that the sectionsare recognized by Section Boundary Detection (SBD) andstored in an appropriate format. In previous work [4], weshowed, that such a transfer is possible: A set of pathol-ogy reports has been automatically transformed intoarchetype-based Pathology Electronic Health Records(PEHRs). The standard openEHR was exploited for thistransformation.Querying structured EHRsAfter the transformation process, queries can be appliedto specific sections instead of the entire document. Thiscan reduce false positive results. There is a need for anontology-based way for the generation of XPath expres-sions. This method, referred to as search ontology-basedXPath engineering, will be introduced in this work. Morespecifically, the suggested approach [5] will be provenin a real world scenario by real Research Questions(RQs) on a real data set. One hypothesis of this paperis: when the PEHRs are structured into sections by SBDand stored in an XML database, the sections can be usedfor Research Question Answering (RQA).Related workRelated work can be distinguished in EHR Query Lan-guages on Data Marts, and Ontology-based Queries.EHR Query Languages on Data Marts Particularly inhealth care, secondary use and mining on EHRs is stillchallenging [6]. There are already well defined query lan-guages for archetype based EHRs [7, 8]. These querylanguages define an abstract language, which borrowskeywords from StructuredQuery Language (SQL) [9], andcombines them with archetype path expressions, whichare similar to XPaths [10]. Another prominent SQL basedapproach is the usage of the i2b2 [11] data mart for query-ing EHRs. Precondition for that is an Extract TransformLoad (ETL) transformation process into the i2b2 StarSchema [12].Ontology-based queries When the data is stored ona structured relational database, semantic searches canbe applied for answering different kinds of RQs [13].The PONTE platform [14] enables querying on a globalEHR ontology using SPARQL statements [15]. A simi-lar approach uses ontology-based mediation and ObjectQuery Language (OQL) for query formulation [16].The XOntoRank system [17] enables semantic searchby inferring semantic relationships between the querykeywords and the terms in the documents (based ondomain ontologies like Systematized Nomenclature ofMedicine (SNOMED)). A promising approach is theSPARQL2XQuery framework [18], which enables both,transformation between XML and ontologies, and thequery translation of SPARQL to XQuery [19].Reducing ETL processes All in all, for answering RQsby structured query languages like SQL or SPARQLtime consuming ETL processes are necessary. In essence,EHRs have to be transformed into data marts like i2b2or an ontology for enabling SPARQL. Moreover, thetransformation into data marts or ontologies requiresstructured data, but again, many EHRs consist of freetext. We can skip these time consuming processeswhen queries are directly applied to PEHRs (using SBDand XPaths).Demarcation to Question Answering (QA) systemsResearching QA systems was an early explored researchfield in computer science [20]. Nowadays the topic ofsemantic QA systems is a comprehensive and activeresearch field with many different approaches [21].Nevertheless the approach of this paper can sup-port experts during RQA by ontology-based queryformulation and query generation, we distance thisapproach from general QA systems, because QA sys-tems directly return answers, rather than documentscontaining answers, in response to a natural languagequestion [22].Other limitations The category Ontology-based Queriesis promising a higher precision than queries by keywordsin certain sections, because SPARQL queries on OWLbased patient data would be more powerful than XPathexpressions on XML; but a comprehensive and long termpersistence storage of pathology data within semantic webtechnologies is only partially solved. A deep semanticunderstanding of free text based EHRs is an open researchtopic, but in the near future especially the time consumingmanual review process could be supported by methods ofNamed Entity Recognition (NER) and ontology extraction(? Discussion section).Generally speaking, the approach of this paper is inher-ent independent from the underlying XML structureand belongs to the category of Ontology-based Queries.We suggest the usage of an ontology, which is stronglybound to the used XML structure for the generation ofXPath expressions. This strong binding on a structure isonly meaningful when standardized XML-based EHRsare used.Kropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 3 of 14Fig. 1 Use case overview: search ontology-based XPath generationApproach and paper overviewWe consider in this work RQs from the pathology domainas a concrete example (? M1. Questions by a domainexpert section) which have to be answered by a setof PEHRs. These PEHRs are stored after applying SBDto the (free) text on an XML database (? M2. structured PEHRs section). After that, XPath expressionscan address certain parts of the XML documents (?Querying PEHRs using XPaths section). The devel-opment of such XPaths is time consuming for domainexperts, but also for computer scientists. We suggest touse ontologies to support experts for answering RQs bysearch ontology-based XPath engineering (? I. SO-basedXPath engineering section) using the Search OntologyXML extension (SOX). For answering clinical RQs orfor searching similar cases, XPaths can be generatedautomatically out of this ontology (? II. AutomaticXPath Generation section), which in turn can be appliedto document corpora on XML database systems.Figure 1 gives an overview of the idea of this paper. Inthe middle of the search process is a domain expert. Onthe left hand side of Fig. 1 it is illustrated, that the agentuses Protégé, the ontology editor of the Stanford Univer-sity [23] for modeling the query using the SO (? Searchontology section) and SOX (? Search ontology XMLextension section). On the right hand side of Fig. 1 theagent interacts with the XML database; by using XPaths(? Querying PEHRs using XPaths section) the agentcan retrieve relevant XML documents. In summary, focusof this work is the evaluation of the SOX-approach by try-ing to support RQA. Themain contribution is a tool whichis able to generate XPaths expressions out of the SOX(? Search Ontology XML Extenstion XPath Generator(SOXPathGen) section). The tool is tested on samplePEHRs files (? Simple Test Files (Pathology ElectronicHealth Records section) by applying five real-world RQs(? Table 1).MaterialM1. Questions by a domain expertTable 1 lists the questions in Natural Language (NL), thatare asked by a pathologist, which we will try to solve byapplying SOX. In this paper, the Question 1 (Q1) will bepicked as continuous example, which will be referenced inthe following sections. In Q1 the pathologist is interestedin the average flake weight, that occurs when prostatecancer is diagnosed. More precisely:(1) Query for answering Q1 in NL (formulated by acomputer scientist)We search for all PEHRs, where inthe Macroscopy section occurs a prostate flakeweight, intersected with all PEHRs where a prostatecancer diagnosis occurs. These are PEHRs, whichcontain certain terms in the OverallInterpretation section, or they have certainclassification strings in the Typification andLocalisation section. For a better precision,PEHRs which have blister related terms in Materialhave to be excluded.Q1 is in principle a simple question, but it shows thatprocessing NL questions is difficult to understand forhumans as well as for machines. Because of that we areconvinced: there is a demand of an ontological-basedquery formulation.M2. structured PEHRsIn this article, we will concentrate on the special domainof pathology, where a lot of semi-structured informationTable 1 NL description of the queries (? SearchOntology-based Pathology Questions (OWL) section)Q QuestionQ0a PEHRs which contains T2 as primary tumor classificationand defined phrases of excised skinmaterialQ1 Prostatic carcinomas are found starting from how manygrams of flake tissue?Q2 Prostatic carcinomas are found starting from how manycapsules? What influence has the processing method(with/without remainder)?Q3 How large are the leiomyomas of the uterus in the entrymaterial?Q4 How many lymph node metastasis occur at colon cancerin stage pT2?Q5 In how many esophageal biopsies is a barret mucosa found?Exclude a certain negation expressionb (cave).aQ0 is only for proofing the concept [5]bohne Nachweis einer Barrett-Schleimhaut (en: without evidence of barrettmucosa)Kropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 4 of 14occurs in terms of pathology reports. In fact, pathol-ogy reports are based on certain section patterns andsection-introducing keywords, like material, macroscopyor microscopy. We verified manually, that keywords likeMaterial, Makroskopie or Mikroskopiewere con-stantly used for section tagging of pathology reportsof the Institute of Pathology of Leipzig. Therefore, thereports can be section-structured very precisely into anarchetype-based Pathology Patient Information Model(PPIM) by the application of methods like SBD andopenEHR [4]. As a result of this previous work, 68,583openEHR-based PEHRs are stored on an XML database,ready for answering RQs. For a better understanding,we publish herewith some test files (? Simple TestFiles (Pathology Electronic Health Records) section).The corresponding XML of one sample PEHR is listedin Fig. 2.MethodsQuerying PEHRs using XPathsWhen EHRs are stored in XML, another query languageis more suitable than classical free text retrieval meth-ods such as Lucene [24]. XPath expressions are followingthe structure of the EHRs and are a W3C standardizedmethod for addressing parts in XML documents [10]. Anexample XPath expression regarding Q1 is shown in Fig. 3.XPath functions are used for matching the German wordstems. E.g. when florid(\w)* is used as matchingpattern, we will also find any variation like floride orflorides. Of course, irregular words needs to be treatedbymultiple disjunct specifications. For the combination ofwords, the expression ([\w]*\s){0,2} can be useful,which implies that a maximum of two words is allowed tomatch the pattern, which is similar to Lucene ProximitySearches [24].OntologiesTop level ontology General Formal Ontology (GFO)The GFO introduces a top level ontology [25], useful forconceptual modeling. The GFO classes Concept andSymbolic_structure and the property has_parthave been reused during the introduction of the SO andSOX classes and properties (summarized in Fig. 4).Search ontology The development, management andreuse of search concepts is a complex task, that canbe supported by the SO [26]. The SO has been devel-oped to support full text search on documents; it canbe used for Information Retrieval (IR) in any domainby extending it by the corresponding domain ontol-ogy. The representation of the knowledge in the SO issimilar to knowledge-based IR, where Hierarchical Con-cept Graphs (HCGs) constitute hierarchical thesauri asan useful knowledge representation [27]. In the SO wedistinguish Search_Concepts from Search_Terms,disaggregating the latter into Simple_Terms andComposite_Terms. Composite_Terms are made upof Simple_Terms, related by the Object PropertyFig. 2 Simplified XML-based pathology EHR snippet, containing a specimen, an overall interpretation and a macroscopic findings partKropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 5 of 14Fig. 3 One simple XPath examplehas_part, and Composite_Terms are constrainedby the additional data property max_distance, whichdefines the word distance between Simple_Terms,where max_distance=0 represents, that one wordimmediately follows another word. Writing variations,synonyms, abbreviations as well as term phrases can behandled by the assignment of multiple labels to the con-crete individuals of a Simple_Term. The SO is illus-trated and described in detail in Fig. 5.Search ontology XML extension We extended alreadythe SO in a way that allows querying structured datastored as XML documents [5]. By extending the SO,XPaths are automatically producible out of the ontology,which can be executed on XML documents by integrat-ing them into XSLT or XQueries. The extension of the SOis summarized in Figs. 4 and 6. On the top level of theontology the class XML_Structure was added, whichsubclass structure represents the XML structure. Figure 6shows that Search_Concepts are described_bySearch_Terms. Search_Terms belong to certainparts in the XML_Structure, linked by the added inrelation. Namespaces and tag names of the XML docu-ment are defined within the class IRI. For a combinationof multiple Search_Concepts, we enhance the SO bya new class, the Search_Query (? I.5 CombiningSearch_Concepts to Search_Queries section). Further, anadditional annotation property xpath is adhered duringthe XPath generation process (? II. Automatic XPathGeneration section).Fig. 4 SO ? SOXEngineering and generation process overviewFigure 7 is important for understanding the overall pro-cess, in which the ontologymethods are used. Prerequisitefor the query engineering is a concrete RQ (M1) andstructured PEHRs (M2), which are stored on an XMLdatabase. The process illustrated in Fig. 7 is described inthe following subsections (I.-IV.).I. SO-based XPath engineeringThe modelling of the queries has to be done manually andconsists of the following sub-steps:I.1 Defining the XML_StructureI.2 Understanding and Formalization of the QuestionsI.3 Preparing the Search_TermsI.4 Describing the Search_Concept and linking themto the XML_StructureI.5 Combining Search_Concepts toSearch_QueriesThe process order is not strict. In practice, it is alsouseful to describe the Search_Concept (I.4) before thedefinition of the Search_Terms (I.3). Practical queryengineering is a cyclic process (? Refinement circlessection), which will be explained in the following by apractical example.I.1 Defining the XML_Structure The definition of theXML_Structure in a HCG is conditional, becauseSearch_Termshave to be bound to the XML_Structurein a later sub-step. Namespace declarations are directlyFig. 5 Overview search ontologyKropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 6 of 14Fig. 6 Search ontology XML extensionused in the IRI. Figure 8 illustrates the XML_Structure,which is based on the PEHRs and required for answeringthe questions of Table 1.I.2 Understanding and formalization of the questionsIn this preparation step, all questions of Table 1 can beformalized like suggested in Table 2. Another approachwould be the usage of NL, as long as it is clear andcomplete.I.3 Preparing the Search_Terms Based on the lattersub-step (Table 2) the Search_Term classes, moreprecisely Simple_Terms and Composite_Terms,were defined. Firstly Simple_Terms classes andinstances were defined; multiple labels can be created,which can contain regular expressions. Figure 9 illus-trates the defined Search_Term classes and labelsregarding Q1. After defining the Simple_Terms,Composite_Terms can be constructed by linking themto the Simple_Terms by the has_part relation.I.4 Describing the Search_Concept Search_Conceptsare primitive classes, which are described by the followingsomeValueFrom restriction:described_by some (Search_Term and (insome XML_Structure))For instance (Q1), to refine a Search_Conceptto a class which represents, that certain adeno-carcinoma Search_Terms are expected in anOverall_interpratation section, the followingclass description is used.Adenocarcinoma_in_Interpretation:described_by some (Adenocarcinomaand (in some pim:Overall_interpretation/pim:value/oe:value))I.5 Combining Search_Concepts to Search_QueriesIt became clear during the engineering process ofthis practical use case, that an additional concept isFig. 7 Overall process overviewKropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 7 of 14Fig. 8 XML_Structure treeneeded for connecting multiple Search_Conceptstogether by Boolean expressions. The following classdescription represents the combination of multipleSearch_Concepts regarding Q1.(2) Q1 class description (Boolean connected)G_Unit_in_Makro and (Adenocarcinoma_in_Interpretation or(ICD-O-C-61_in_Localisation andICD-O-M-8140/3_in_Typification))and No_Blister_in_MaterialThere is an improved readability when we compare (1)Query for answering Q1 in NL with the latter (2) Q1 classdescription.II. Automatic XPath generationThe latter ontological query engineering yields anOWL file, that holds all necessary data for the auto-matic generation of the XPath expressions. Duringthat generation, each Search_Query concept gets anXPath annotation. These annotations are generated bya program fetch, that interprets the class descriptionsand labels by the usage of the Jena API [28]. Thealgorithm dissolves each Search_Concept containedin the Boolean expression of each Search_Query.When the Search_Concept is described_by aSimple_Term, a disjunction is generated, that containsfor every instance label of the Simple_Term an XPathexpression; the generation is based on the labels ofthe Simple_Term instances and is based on the pathof the referenced XML_Structure node. Otherwise,when the Search_Concept is described_by aComposite_Term, a disjunction of a constructed crossproduct of the referenced Simple_Terms is generated.III. Fetching EHR snippetsThe generated XPath expressions are integrated inXQueries, which are applied on an XML database forTable 2 DL-based-description of the queriesQ QuestionQ0a (HE_Shapes in Macroscopy ANDT2_Term in Overall_staging) [5]Q1 G_Unit inMacroscopy AND ¬ Blisterin Interpretation AND(Adenocarcinoma in Interpretation OR(ICD-O-C-61 in Localisation ANDICD-O-M-8140/3 in Typification))Q2 (without residual) K_No_Rest inMacroscopy AND¬ Blister in Interpretation AND (Adenocarcinomain Interpretation OR (ICD-O-C-61 in Localisation ANDICD-O-M-8140/3 in Typification))AND (ProstateFlake inMacroskopy)OR ProstateFlake in Interpretation)Q2 (with residual) K_Rest inMacroscopy AND ¬ Blisterin Interpretation AND(Adenocarcinoma in InterpretationOR (ICD-O-C-61 in Localisation ANDICD-O-M-8140/3 in Typification)) AND(ProstateFlake inMacroskopy ORProstateFlake in Interpretation)Q3 CM_Unit in Interpretation AND Leiomyom inInterpretation AND Uterus inMaterialQ4 (C18 in Localisation or Colon inMaterial) AND T2 inOverall_staging AND TNM_Sub_pN in stagingQ5 (numerator) BarrettsMucosa in Overall_interpratation ANDNO_Exclusion_Cave in InterpretationQ5 (denominator) EsohagusBiopsy inMaterialaQ0 is only for proofing the concept [5]The in relation was introduced in SOX. X in Y means that at least one instance of theSearch_Term class X (bold) should occur in the section representing class YKropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 8 of 14Fig. 9 Class Quest1_ProstateCancerGramCorrelationKropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 9 of 14retrieving relevant XML snippets. After that, the relevantPEHR snippets are stored on the local file system, readyfor the manual review.IV. Manual reviewDuring the manual review process, the retrieved PEHRssnippets have to be evaluated and interpreted. Ideally afterthat step, the initial RQ can be answered. In practice cir-cles occur, which means that the question has often to berefined during the manual review.ResultsThe main contribution of this work, the introducedmethod SO-based XPath engineering, has been evaluatedby the application of the described process by an ontol-ogist, where five RQs have been processed. Each processyields interim results, that will be presented in the follow-ing. Based on these interim results, which are OWLs andPEHR snippets, a short interpretation of the RQA indi-cates the practical usefulness of the presented approach.I. SO-based XPath engineering and automatic XPathgenerationThe OWL class descriptions (which relate to Q1) areverbosely listed in Fig. 10. For a better understanding, wepublished the resulting OWL files containing the generated XPath expressions for the five RQs (?Search Ontology-based Pathology Questions(OWL) section), as well as the binary of the XPath generation tool (?Search Ontology XML Extenstion XPath Generator(SOXPathGen) section).II. Fetched PEHR snippets andmanual reviewThe XPaths have been applied within XQueries for fetch-ing the relevant PEHR snippets. The second columnof the Table 3 shows the amount of retrieved XMLsnippets for each of the five questions. These PEHRsnippets are used for RQA during the manual review,where each PEHR snippet has to be evaluated to pre-vent false positives in the query result. After removingthe false positives, the PEHR snippets are ready for theinterpretation.III. InterpretationTable 3 summarizes the amount of retrieved PEHRs andindicates the counts of cases of enumerated content. Inthe result set, about ? 64% of the PEHRs contained enu-meration lists. Moreover, all RQs of Table 1 could beenanswered in Table 4. In particular, the amount of resultsretrieved for Q1, Q3, and Q5 are useful for answering thecorresponding RQs:Q1 The average weight of flakes ? 18.26 g seems to bereasonable.Q3 Especially the relatively high amount of 93 casesindicates, that the average maximum diameter ofleiomyomas of ? 2.76 cm could be a plausible answer.Q5 The high amount of cases indicates, that in about 8 of10 cases a barret mucosa has been found during anesophageal biopsy. This value is a characteristicquality factor, usable for a comparison of clinicians aswell as institutes.All questions could be better evaluated by a biggeramount of PEHRs in the database.Fig. 10 OWL Class Quest1_ProstateCancerGramCorrelationKropf et al. Journal of Biomedical Semantics  (2018) 9:16 Page 10 of 14Table 3 Overview on the evaluation resultsQuestion |PEHR| |PEHR| |PEHR| |PEHR|(partly) enumerated ECRI false PQCRI falsepositives positivesQ0a 12 9 n/ab n/acQ1 36 5 1 0Q2 (without residual) 18 6 0 0Q2 (with residual) 9 2 0 0Q3 153 67 1 60Q4 4 4 n/ab n/acQ5 (denominator) 902 632 n/ad n/acQ5 (numerator) 756e skipf n/ad n/acSumg 1134 725 2 60aQ0 is only for proofing the concept [5]bnot structured by an enumeration list, TNM classification codes are usedcPQCRI can not occur because no units are used in this querydECRI can not occur because in this type of PEHR the specimen tissue section wasnot structured by an enumeration listenot part of the column sum because Q5 (denominator) contains the Q5(numerator) recordsfevaluation can be skipped because Q5 (denominator) contains already the Q5(numerator) recordsgwithout Q5 (numerator)In the second column is the amount of the retrieved PEHRs, in the third column isthe amount of numbered content, in the fourth column is the amount of falsepositives which occur because of the ECRI, and in the fifth column is the amount offalse positives which occur because of the PQCRIDiscussionWe introduced an extension of the Search Ontology tosupport querying XML documents. The SOX approachcan simplify the generation of a big pool of XPath expres-sions. During the practical evaluation of the approach,Table 4 Answers of the NL Questions based on the dataset of68,583 PEHRs, interpretated by the ontologistQ AnswerQ1 The least weight was 3 g, the maximum weightwas 38 g, were prostate carcinomas have beenfound. The average weight was? 18.26 g , ? ? 10.18 g.Q2 (without residual) At least 2, at most 26 capsules were took withoutrest. In average 9.28 capsules were took,? ? 4.78 capsules.Q2 (with residual) At least 6, at most 10 capsules were took with rest.In average ? 9.55 capsules were took,? ? 0.15 capsules.Q3 ? 2.76 cm is the maximum diameter of leiomyomasin average, ? ? 1.42 cm.Q4 In four found casesa 0.5metastasis occur at coloncancer in stage pT2 in average.Q5 In 83.81% of the esophageal biopsies a barretmucosa has been found.a(1/1), (1/1), (0/41), (0/19)difficulties regarding NL arose, which will be discussed inthe following.Uncertainty of NLsUncertainty of NL questions Q1 can be interpreted indifferent ways: (1) The pathologist wants to know theminimum known flake weight, were prostate carcinomacould be diagnosed. (2) The pathologist wants to know anavarage value. (3) The pathologist wants to know a valuerange. We solved this uncertainty by offering answers ofall of these variations in Table 4.Uncertainty in the material During the manual reviewprocess, we recognized a frequent occurrence of certaintypes of false positives in the result set: (1) Enumera-tion Coreference Resolution Issue (ECRI) and (2) PhysicalQuantity Coreference Resolution Issue (PQCRI).(1) ECRI In essence, an enumerated PEHR consists usu-ally of different material items: mat1, . . . , mati, matn;and then, the macroscopy section could also have an enu-meration list mac1, . . . , macj, macn. Imagine we found aPEHR, where matx contains one related search term (e.g.adenocarcinoma), andmacy contains e.g. the weight con-cept. Everything is fine when x = y, which e.g. meansthat the weight concept belongs to the adenocarcinomamaterial. But when x = y we found a false positive,RESEARCH Open AccessUsing OWL reasoning to support thegeneration of novel gene sets forenrichment analysisDavid J. Osumi-Sutherland1*, Enrico Ponta2, Melanie Courtot1, Helen Parkinson1 and Laura Badi2AbstractBackground: The Gene Ontology (GO) consists of over 40,000 terms for biological processes, cell components andgene product activities linked into a graph structure by over 90,000 relationships. It has been used to annotate thefunctions and cellular locations of several million gene products. The graph structure is used by a variety of tools togroup annotated genes into sets whose products share function or location. These gene sets are widely used tointerpret the results of genomics experiments by assessing which sets are significantly over- or under-represented inresults lists. F Hoffmann-La Roche Ltd. has developed a bespoke, manually maintained controlled vocabulary (RCV) foruse in over-representation analysis. Many terms in this vocabulary group GO terms in novel ways that cannot easily bederived using the graph structure of the GO. For example, some RCV terms group GO terms by the cell, chemical ortissue type they refer to. Recent improvements in the content and formal structure of the GO make it possible to uselogical queries in Web Ontology Language (OWL) to automatically map these cross-cutting classifications to sets ofGO terms. We used this approach to automate mapping between RCV and GO, largely replacing the increasinglyunsustainable manual mapping process. We then tested the utility of the resulting groupings for over-representationanalysis.Results: We successfully mapped 85% of RCV terms to logical OWL definitions and showed that these could be usedto recapitulate and extend manual mappings between RCV terms and the sets of GO terms subsumed by them. Wealso show that gene sets derived from the resulting GO terms sets can be used to detect the signatures of cell andtissue types in whole genome expression data.Conclusions: The rich formal structure of the GO makes it possible to use reasoning to dynamically generate novel,biologically relevant groupings of GO terms. GO term groupings generated with this approach can be used in.over-representation analysis to detect cell and tissue type signatures in whole genome expression data.Keywords: OWL, EL, gene ontology, GO, gene set enrichment analysis, enrichment, over-representation analysis,ontology mappingBackgroundThe Gene Ontology (GO) consists of almost 40,000 termsand has been used to annotate millions of gene productsto record their subcellular location (e.g., lysosome), theirmolecular function (e.g., kinase activity) and their widerrole in cellular, developmental and physiological processes(e.g., signal transduction) [1]. In its original form, the GOwas conceived of as a directed acyclic graph in whichterms referring to classes are nodes and edges record rela-tionships between classes including classification (is a)and partonomy (part of). This graph structure is com-monly used to group genes annotated with related termsin user-facing tools such as QuickGO [2] and AmiGO [3]and to generate gene sets for enrichment (over- represen-tation) analyses [4] to interpret the results of genomicsexperiments. For example, an experiment to assess thehow treatment of liver cells with a particular drug effectsthe transcriptome (genome-wide gene expression profile)in liver cells may result in a list of all genes whose expres-sion is increased by the drug treatment. The GO graph* Correspondence: davidos@ebi.ac.uk1European Bioinformatics Institute (EMBL-EBI), Wellcome Trust GenomeCampus, Cambridge CB10 1SD, UKFull list of author information is available at the end of the article© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Osumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 DOI 10.1186/s13326-018-0175-zstructure can be used to group all genes in the relevantgenome into sets sharing function or location. One canthen ask which gene sets are statistically over or under-represented in the gene list compared to the expectednumber of of genes from that set in an equivalent lengthlist generated by random sampling from the set of allgenes in the genome.In recent years, the GO has developed into a richlyaxiomatised formal ontology specified using Web Ontol-ogy Language (OWL) [5, 6] and defining GO terms withreference to terms from other ontologies. For example,the GO now records the chemical participants in over12,000 processes and functions via axioms referencingchemical entities defined by the Chemical Entities ofBiological Interest (ChEBI) ontology [7, 8]. Over 8000GO classes have some direct or indirect logical link to aterm from the Cell Ontology (CL) [9] or the Uber anat-omy ontology (Uberon) [10]. These record, for example,the location of cellular components (the acrosome andits parts are present only in sperm), cell types that arethe sole location of some process (natural killer cell de-granulation only occurs in natural killer cells), and theproducts of developmental processes (bone is a productof bone morphogen- esis). There are also over 2500logical axioms recording the functions of cellularcomponents via links to molecular function and bio-logical process terms.When combined with standard OWL reasoning tech-nologies, this improved axiomatisation opens up newpossibilities for grouping terms and their xannotationsin biologically meaningful and semantically precise waysthat are potentially useful in enrichment analyses. Forexample, we can use OWL reasoning queries to groupprocesses occurring in T-cells or in the pancreas, orprocesses involving nitric oxide or collagen fibers.The results of enrichment analyses using gene sets forall GO terms can be difficult and slow to interpret dueto high levels of overlap between gene sets. There are anumber of sources of overlap: grouping via class andpart hierarchies means that gene sets derived from anno-tation to a class subsumes the gene sets of its subclassesand subparts; one GO class can sit in multiple branchesof the hierarchy; a single gene product may be annotatedto terms in multiple branches. For this reason, many en-richment analyses rely on a more limited number ofgene sets, corresponding to grouping under a limitednumber of high or intermediate level GO termscommonly referred to as a slim.Rather than use a slim of GO terms, F. Hoffmann-LaRoche Ltd. (Roche), maintains an internal controlledvocabulary (referred to hereafter as the RCV) for use inenrichment analyses. The RCV consists of around 360terms, each of which is mapped to a set of terms fromthe GO. It is tailored to the research interests of Roche,and its terms were chosen with the aim of achievinggene set composition descriptive and broad enough toallow robust and statistically significant results thoughnot so broad and redundant in composition that it pre-vents easy interpretation of results. Detecting enrich-ment to gene products involved in anatomy, organ orcell-specific processes or components can be critical forpharmacological research, especially when working withcomplex tissues where there is a need to tease apartevents occurring in specific tissue compartments or celltypes. To support this, many RCV terms group GOterms in ways that are out of scope for classes in theGO, including groupings of GO terms related to specificcell, tissue or molecule types.Here we describe the development and testing of adynamic, computable mapping between RCV terms andthe GO that makes use of OWL reasoning. We showthat RCV groupings of GO terms related to specific cell,and molecule types can be used to identify the transcrip-tomes of those cell types via enrichment analyses.MethodsAs the RCV is a flat list and includes classifications thatare orthogonal to the classification schemes used by theGO, it is not amenable to mapping via ontology align-ment techniques that use ontology structure [11]. Giventhe small size of the RCV, it is viable to manually mapeach RCV term to an OWL class expression (DL query),which can then be used in conjunction with an OWLreasoner to generate lists of GO terms. The RCV doesnot include textual definitions to clarify meaning, so foreach RCV term we attempted to find a class expression(a mapping query) that reflected the intended meaningof the RCV term, as judged by the RCV term name,manual mappings and discussion with RCV developers.Query strategyWe manually mapped each RCV term to an OWL classexpression (a mapping query) and used a standard OWLreasoner to generate a combined list of classes equiva-lent to and classes subsumed by the class expression.We tested classification and query answering using theGO with imports of CHEBI, CL and Uberon on a2.9 GHz Intel Core i7 Mac laptop, assigning 10Gb ofRAM to the JVM. Classification with the OWL 2 EL rea-soner ELK [12] com- pleted in under 6 s and used lessthan 4Gb of RAM. Subsequent queries of the classifiedontology took 10-100 ms milliseconds. In contrast, clas-sification using the HermiT reasoner [13], whichsupports OWL 2 DL took ~70 min and used 7.5Gb ofRAM. Subsequent query answering was very slow, withsome test queries timing out. To ensure speed and scal-ability, we therefore chose to restrict mapping queries tothe EL profile of OWL 2 and use the ELK reasoner. TheOsumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 Page 2 of 10expressiveness of the GO and of imported ontologies isalmost entirely within the OWL 2 EL profile (the onlyexception is a handful of inverse property assertions), sowhile some incompleteness in query answering ispossible, we dont expect it to be common.In order to keep the mapping process simple, only asingle GO, CL, Uberon or ChEBI mapping class wasspecified for each mapping query.To compensate partially for the lack of disjunction(OR) in OWL-EL, we developed a hierarchy of high levelobject properties for use in queries. For example, we de-fine occurs in OR has participant as a grouping relationallowing queries for processes that occur in a specifiedcell, or have that cell as a participant. Many RCV termsgroup processes in which a specified chemical or cellparticipates with processes regulating those in which itparticipates (see Table 1 for example). To support suchgroupings, we used an OWL property chain axiom [5] todefine a relation, regulates o has participant, which canbe used to query for processes that regulate a process inwhich some specified entity is a participant. We then de-fined a super-property, participant OR reg participant,for this new relation and has participant:regulates o has participant. subPropertyOf participant OR reg participant. . subPropertyOf regulates o has participant. . subPropertyOf has participantThese new, high-level object properties are difficult toname in a way that communicates the meanings of map-ping queries clearly. In order to compensate for this, weused scripting to generate human readable descriptionsfor each mapping query. Compare, for example, themapping query for the RCV term cannabinoid with itsdescription:Mapping query: participant OR reg participantsome cannabinoid.Description: A process in which a cannabinoidparticipates, or that regulates a process in which acannabinoid participates.Ontologies usedWe used a fully expressive release version of the GO [14],release version 201501-30, supplemented with the bespokerelations described above (21 relations). This resultingontology includes over 40,000 GO classes and over 13,000imported classes from the Cell Ontology, ChEBI, Uberon,the Sequence Ontology the Protein ontology and over 130object properties imported from the OBO RelationsOntology [15]. The DL expressiveness is SRI. For a sum-mary of owl entity and axioms counts please see Table 2.PipelineMapping queries were run using the ELK OWL 2 rea-soner [12] via calls to the OWL- API [16]. The queryand results processing pipeline was written in Jython[17]. All code, mapping tables and results were main-tained in a GitHub repository [18]. The mapping wasspecified using a single tab separated values (TSV) file inwhich each line maps an RCV term to an OWL-ELmapping query that includes a term from GO, ChEBI,CL, Uberon or NCBI taxonomy [19]. Query results wereused to generate a TSV file, allowing direct comparisonof manual and automated mappings (see Table 1 for anexample). We used the GitHub API to generate ticketsfor each mapping, linked to the relevant TSV results file,which GitHub renders as a table. This allowed easymanual review and editing by RCV curators at Rochewho used the linked tickets to discuss mapping issuesand record the approval status of all mappings.Mapping queries were selected, tested and theresults reviewed against manual mappings to decidewhich patterns were most appropriate. Once a map-ping query was chosen, corrections and/or additionsto the GO were made where results were wrong orincomplete. At this point, any clear errors in themanual mapping we blacklisted. Review of automatedmappings was then passed to Roche who approved orblacklisted individual classes (see Table 1 for anexample). When satisfied with the results, the corre-sponding GitHub ticket was closed, thereby indicatingthe mapping as approved. Results approved by RocheTable 1 Results table for RCV cannabinoidGO name GO ID manual auto checked black listed is obsoleteregulation of endocannabinoid signaling pathway GO 2000124 1 1 1 0 0cannabinoid signaling pathway GO 0038171 1 1 1 0 0endocannabinoid signaling pathway GO 0071926 1 0 1 0 0cannabinoid receptor activity GO 0004949 0 1 1 0 0cannabinoid biosynthetic process GO 1901696 0 1 1 0 0The table shows the mapping of an RCV termcannabinoid to a set of GO terms, comparing manual mapping (manual column) with automated mapping (auto column).The automated mapping results from an OWL query for processes in which a cannabinoid participates, or that regulates a process in which a cannabinoid participates. Theautomated mapping found three additional GO terms compared to the manual mapping. In this case, no manually mapped terms were obsolete in GO and all automatedmappings were approvedOsumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 Page 3 of 10were combined to produce a new RCV mapping table(available from [20]).Over-representation analysisFor each RCV term we generated a gene set consistingof all human genes directly annotated to each mappedGO term (retrieved from NCBI/entrez [21]). These arereferred to in the following text and figures using theRCV term name name suffixed with rcv.We additionally generated 155 gene sets that areenriched in specific tissue types. These were generatedfrom three datasets: the Neurocrine Biosciences (NB)CNS dataset [22], the GNF Gene Expression Atlas [23](both based on the Affymetrix microarray technology),and sequencing-based RNASeq Atlas [24]). The Giniindex was used to identify tissue-enriched genes in eachdataset [25] by selecting genes with Gini coefficient > 0.7for the specified tissue. The list of tissue specific genesfrom each dataset was combined in a non-redundanttissue signature list. These are referred to in the follow-ing text and figures by the tissue name suffixed with ts.Using tissue type expression data from the Genotype-Tissue Expression project (GTEx) [26], we calculated theaverage level of expression (mean reads per kilobase oftranscript per million mapped reads (RPKM) signal)across all samples available for a given tissue type andused this to construct a gene vector (a rank order list ofgenes by expression level) for each tissue type. We usedthe same approach to process a publicly available set ofimmune-cell type expression data [27, 28] for enrich-ment analysis.For each geneset and each tissue-type or cell-type genevector we calculated an enrichment score, defined as the-log10 (p-Value) of the Wilcoxon test [29] applied usingthe entire geneset collection as universe. We present en-richment by using the resulting Z-scores to generateheat maps showing over- or under-representation ofeach gene set in each tissue, using euclidean distanceclustering to cluster similar results on both the X axis(GTEx tissue type) and Y axis (gene set). Heat maps area standard way to represent this type of analysis, in partbecause they make clustering of similar results easilyvisible as blocks of similar patterns.ResultsMapping resultsWe developed successful mapping queries (owl class ex-pressions) for 308 out of 364 RCV terms and used OWLreasoning to find all classes equivalent to or subsumedby the mapping query for each RCV term (only 72 RCVclasses had equivalent classes as well as subsumed clas-ses in GO).Over a third (104) of the mapping queries were sufficientto recapitulate all manual mappings. A further 40% of themappings (148) had 10 or fewer additional manual map-pings (Fig. 1a) and most of these (114) had fewer than 5.Mapping queries identified many GO terms that werenot in the manual mapping (Fig. 1b). In some cases (e.g.,leukocyte activation), over 1000 new mappings werefound. On manual review, only 8 out of several thousandautomated mappings were flagged as unsuitable by Roche,56 terms were not mapped. Some were judged to be se-mantically equivalent to other RCV terms. The rest wererejected as currently not mappable due to the lack of suit-able terms or axiomatisation within the GO. For example,RCV has terms for aerobic and anaerobic metabolic pro-cesses, but the GO currently has no terms for these pro-cesses, and no axiomatization that allows them to bequeried for. Further axiomatization of the GO is likely toimprove the number of RCV terms that can be mapped.Improvements to the GOWhile the GO has extensive axiomatization linking pro-cesses to cells, anatomical structures and chemicals, thisis not always complete. In mapping from the RCV to theGO we found and corrected over 200 omissions in theaxiomatization including links from processes to partici-pant cell types, anatomical structures and chemicals. Ex-amples include linking GO process terms referring tothe aggregation of immune cell types such as lympho-cytes and thymocytes to the relevant cell type terms inthe cell ontology.We also found and corrected a number of errors, in-cluding errors in axiomatization of developmental pro-cesses that led to incorrect inferences for RCV anatomyterms. For example, we uncovered and fixed errors inaxiomatisation of epidermis development that tangledtogether classification of terms referring to animal epi-dermal structures (e.g. skin and its parts) with thosefrom plants (such as stomatal guard cells).Table 2 Ontology metrics: Counts of OWL entity and axiomstypes in the ontology used for mappingentity/axiom type CountLogical axioms 142,894Classes 53,799Object properties 153SubClassOf axioms 113,104EquivalentClasses 29,386DisjointClasses 148GCI 6910SubObjectPropertyOf 164InverseObjectProperties 28TransitiveObjectProperty 16ReflexiveObjectProperty 1SubPropertyChainOf 46Osumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 Page 4 of 10Assessing the utility of RCV for over-representationanalysis.We assessed the ability of gene sets derived from RCVterms to identify tissue and cell types in enrichmentstudies. The RCV contains a number of terms for immunecell types defined using a standard pattern that groupsterms related to immune cells ina variety of ways. e.g.:T_cellsSome part of a T cell, or some process in which: a T cellparticipates or that occur in a T cell or which results in thedevelopmental progression of a cell that will form a Tcell.Using immune cell-type expression profiles from [27, 28]we calculated how over-or under-represented each RCVimmune cell type gene set was in each cell-type expressionprofile. The results are displayed in Fig. 2, with cell-types(X-axis) clustered according to similarity of enrichmentprofile across gene sets. All RCV immune cell gene setsare highly enriched in whole blood transcriptomes. Theover- representation profiles for RCV immune cell-typegene sets matched immune cell types. NK cells rcv and Bcells rcv and monocytes rcv were all enriched only inmatching cell-type transcriptomes. CD8 is expressed in Tcells and a subset of Natural Killer cells [30, 31]. Consist-ent with this, enrichment to the T cells rcv and NK cellsrcv gene sets is seen in CD8 expressing cells. CD4 isexpressed in subset of T-cells [30]. Consistent with this, alow level of enrichment is seen for the RCV T-cell genesets. To test the ability of the RCV to match tissue typesmore broadly, we used tissue-type expression profilesfrom GTEx [26], a publicly available data set with expres-sion profiles of 47 different tissues. We used this to com-pare enrichment to RCV terms to enrichment to a set oftissue-derived gene sets. The complete results are availablein Additional file 1 as a heat map showing over and underrepresentation of each gene set (Y-axis) for each GTExtissue-type (X-axis), with both axes clustered for similarity.Distinct enrichment clusters generated by this analysis in-clude clusters identifying tissues rich in immune cells(Fig. 3), clusters indentifying brain tissue (Fig. 4 and clus-ters identifying skin (Additional file 1).Figure 3 shows enrichment analysis for genesets re-lated to immune cells. RCV immune cell genesets forma co-cluster with immune cell enriched genesets - show-ing over-representation in tissues rich in immune cellssuch as blood, lungs and gut mucosa as well as in trans-formed lymphocytes.Comparable ts and rcv gene sets have very little over-lap: all comparisons between equivalent rcv and tsgenesets have a Jaccard index of less than 0.1 (Table 3).They therefore provide complementary sets of signaturesfor detecting the presence of immune cells in samplesfor which transcriptomic data is available.Also included in the cluster are immune-systemrelated groupings such as immune response rcv, inflam-mation rcv and leukocyte activation rcv, showing thatthe RCV provides a semantically richer picture than sim-ply detecting cell-types.Figure 4 shows a similar cluster of enrichment forbrain tissue samples. Gene sets derived from annotationto processes involving glial cells (glial rcv) and morespecifically astrocytes (astrocyte rcv) are sufficient todistinguish brain tissue types and nerves from othertissue types in GTEx. The cluster also contains RCVgene sets for novel grouping terms defined with re-spect to molecules (dopamine, cAMP, neurotransmit-ter) and cell components (synapse) with definitionsfollowing the patterns:Fig. 1 Summary of mapping results a. Distribution of manualmappings not found by automated mapping X axis = number ofmanual-only mappings. Y axis = Number of RCV terms. Over 80% ofmappings are completely automated or require less than 10 manualmappings. b. Distribution of automated mappings not present in theoriginal manual mapping. X axis = number of auto-only mappings.Y axis = Number of RCV terms. Many new mappings were uncoveredby automationOsumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 Page 5 of 10Fig. 2 Use of RCV-derived gene sets to identify immune cell types. RCV-derived gene sets (Y-axis); immune cell type transcriptomes (X-axis); over-representation is indicated in red; under-representation in blue. Cell-type transcriptomes are clustered based on similarity of enrichment profileacross gene setsFig. 3 Comparison of RCV derived gene sets and tissue derived gene sets for identification of immune-cell rich tissues Over-representation ofRCV-derived gene sets (Y-axis) in tissue-type transcriptomes (X-axis) is indicated in red, under-representation in blue. Tissue-type transcriptomesare clustered based on similarity of enrichment profile across gene sets (X-axis) and gene sets are clustered by similarity of enrichment profileacross tissues (Y-axis). Only the immune-rich tissue cluster of gene sets is shown in this figure. For the fully enrichment analysis please seeAdditional file 1Osumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 Page 6 of 10NeurotransmitterA process in which some substance with neurotransmit-ter biological role participates, or that regulates aprocess in which substance with neurotransmitter bio-logical role participates.SynapseA synapse OR part of a synapse OR a process that re-sults in organisation of a synapse OR that has a synapseas a participant.These enrichments make sense given what is knownabout the biology of neural tissue, as do a set of RCVterms that map to conventional GO terms: ion transport;cell cell signaling, glutamate metabolism and nervoussystem development terms.DiscussionMapping of RCV terms to set of GO terms is now fullydynamic, allowing RCV to be automatically kept up todate as the GO. Where new terms follow mapping querypatterns that are already used, they can be added simplyby specifying an additional line in the mapping file.48% of mapped RCV terms have 10 or fewer manualmappings. We are reviewing all of these cases to decidewhether to drop manual mappings or whether completeautomation might be achieved by a different query strat-egy. In some cases, a more complete mapping could beachieved by a disjunctive query. For example, all RCVterms referring to metabolism of some specifiedFig. 4 Comparison of RCV derived gene sets and tissue derived gene sets for identification of brain derived tissues. Over-representation ofRCV-derived gene sets (Y-axis) in tissue-type transcriptomes (X-axis) is indicated in red, under-representation in blue. Tissue-type transcriptomesare clustered based on similarity of enrichment profile across gene sets (X-axis) and gene sets are clustered by similarity of enrichment profileacross tissues (Y-axis). Only the brain tissue cluster gene sets is shown in this figure. For the full enrichment analysis please see Additional file 1Table 3 Overlap between cell-specific gene sets derived fromRCV and cell expression data is lowGene sets Jaccard IndexB cells rcv vs Lymphocyte B FOLL ts 0.064NK cells rcv vs Lymphocytes NK ts 0.000T cells rcv vs Lymphocytes T various tsa 0.025aT helper rcv vs Lymphocytes T H ts 0.032dendritic cell rcv vs Dendritic cells ts 0.000granulocyte rcv vs Granulocyte INFL ts 0.082lymphocyte rcv vs Lymphocytes NK ts 0.071macrophage rcv vs Macrophage PB ts 0.033mast cell rcv vs Mast cell PB ts 0.045Column one lists the two gene sets compared. Column 2 lists the Jaccard similaritycoefficient comparing the two gene sets (0 = no overlap, 1 = full overlap.) aIn thecase of T cells the average of a range of T-cell expression datasets is shownOsumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 Page 7 of 10chemical are mapped to GO terms referring to meta-bolic and transport processes in which the specifiedchemical is a participant. (This is consistent with somemedical use of the term metabolism.) A more completemapping could be achieved using a disjunctive querywith an OWL2 DL reasoner such as HermiT. Thisapproach was found to be prohibitively slow but newgeneration reasoners such as MoRE [32] which combineELK with DL reasoners such as HermiT may turn out tobe useful in this approach. A simple, if potentially in-complete alternative would be to simply run two ELmappings using ELK and generate a union of the results.56 terms were not mapped. Some were rejected fromthe pipeline as they were judged to be too close inmeaning to other RCV terms. The rest were rejected ascurrently un-mappable due to the lack of suitable termsor axiomatisation within the GO at this time. Forexample, GO currently has no formal way to groupaerobic or anaerobic metabolic processes, although itdoes reflect the aerobic or anaerobic nature of manymetabolic processes in their names and textualdefinitions.Making novel groupings of GO terms generally accessibleThe approach described here could be used to provide aview of the GO that groups terms in ways defined withreference to the complete range of cell-types, chemicaltypes and anatomical structures referenced by the GOand all of their ancestor classes. This is already reflectedin some of the newer functionalities of the GO browsingtool AMIGO, which now displays inferred annotationsto cell-types based on axioms in the GO recordingwhere processes occur [33]. An extended version of theGO with extended axiomatisation and imported termsfrom external ontologies includin CL, Uberon, ChEBI isavailable from [34].The system described bears some relationship toTermGenie [35] which is already used to generate 80%of new GO terms. One possible approach to fulfillingthe needs of external groups for types of classificationnot included in the GO would be to offer a TermGenie-like system to create bespoke terms.ConclusionsOur work demonstrates how the logical structure of theGO can be used to achieve biologically meaningful map-pings between concepts in external controlled vocabu-laries and corresponding sets of GO terms, even whenthere is no concept in the GO that is directly equivalentthe term to be mapped. This is possible as long as theconcept can be mapped to an OWL class expressionreferencing classes and relations in the full version ofthe GO. These classes may come from the GO, or fromontologies from which the GO imports classes such asChEBI, CL and Uberon. The resulting mapping isdynamic and so can easily be kept up to date as the GOevolves.While OWL 2 DL profile queries could be used forthese mappings, this would make mapping software slowto run, resource intensive, and may not be sustainable asthe GO becomes still larger and more complex [6]. Themapping system we describe uses class expressionsrestricted to the OWL 2 EL profile to ensure that map-ping is fast and scaleable. It also demonstrates howOWL property chains and property hierarchy can beused to partially overcome the absence of disjunction(OR) in OWL 2 EL.The RCV includes many terms that group GO termsin novel ways by their rela- tionship to some type of cell,molecule, tissue or cell component. One possible usageof these terms is to provide a mechanism for detectingthe signatures of particular cell or tissue types in tran-scriptomic data. We demonstrate the effectiveness ofthis by showing how gene sets derived from RCV termsfor cell types can be used to identify specific immunecell types and how gene sets derived from RCV termsfor cell, molecular and cell component types can be usedto identify tissue types. In some cases, gene sets deriveddirectly from transcriptomic data may be used for thesame purpose (see Figs 3 and 4). In these cases the RCVterm sets provide an alternative method using gene setswith very little overlap to those derived from differentialexpression (Fig. 3). Unlike gene sets derived fromtranscriptomic data, those derived from the GO and itsannotations are not limited by the ability to experimen-tally isolate suitable biological samples and can includebroad groupings of cell or tissue types that are unlikelyto be isolated together (e.g. all glial cells (Fig. 4), or allepithelia).Additional fileAdditional file 1: A complete over-representation analysis for RCV genesets against GTEx tissue type transcriptomes. The analysis is displayed asa heat map with RCV on the Y-axis, GTEx on the X-axis, over-respresentationin blue and under-respresentation in red. Both axes are clustered for similarity(see Methods for details). (PDF 91 kb)AcknowledgementsThe initial version of this paper was published in the Proceedings of the 8thInternational Conference on Semantic Web Applications and Tools for LifeSciences [36].FundingThis work was supported by direct funding from F. Hoffmann-La Roche Ltd. TheGene Ontology Consortium is supported by a P41 grant from the National HumanGenome Research Institute (NHGRI) [grant 5U41HG00227314].Availability of data and materialsAll files used in this mapping (mapping tables, OWL files, pattern documents)and the resulting mappings are available on a dedicated GitHub repository [20].Osumi-Sutherland et al. Journal of Biomedical Semantics  (2018) 9:10 Page 8 of 10Authors contributionsDOS designed and implemented the GO to RCV mapping strategy andcontributed to all other aspects of the work described here. LB developedthe original Roche controlled vocabulary and collaboratively developed thequery-based mapping to GO with the DOS. She also devised and executedout all of the enrichment analysis in this paper. EP manually inspected allmapping results and provided feedback on their accuracy. MC contributedto writing the manuscript and to framing and shaping its arguments. HP su-pervised the work of DOS and contributed to writing the manuscript. All au-thors read and approved the final manuscript.Ethics approval and consent to participateNot applicableConsent for publicationNot applicableCompeting interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1European Bioinformatics Institute (EMBL-EBI), Wellcome Trust GenomeCampus, Cambridge CB10 1SD, UK. 2Roche Pharma Research and EarlyDevelopment, Pharmaceutical Sciences, Roche Innovation Center Basel, F.Hoffmann-La Roche Ltd, Grenzacherstrasse 124, -4070 Basel, CH, Switzerland.Received: 16 May 2016 Accepted: 3 January 2018Metke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 https://doi.org/10.1186/s13326-018-0191-zSOFTWARE Open AccessOntoserver: a syndicated terminologyserverAlejandro Metke-Jimenez* , Jim Steel, David Hansen and Michael LawleyAbstractBackground: Even though several high-quality clinical terminologies, such as SNOMED CT and LOINC, are readilyavailable, uptake in clinical systems has been slow and many continue to capture information in plain text or usingcustom terminologies. This paper discusses some of the challenges behind this slow uptake and describes a clinicalterminology server implementation that aims to overcome these obstacles and contribute to the widespreadadoption of standardised clinical terminologies.Results: Ontoserver is a clinical terminology server based on the Fast Health Interoperability Resources (FHIR)standard. Some of its key features include: out-of-the-box support for SNOMED CT, LOINC and OWL ontologies, suchas the Human Phenotype Ontology (HPO); a fast, prefix-based search algorithm to ensure users can easily find contentand are not discouraged from entering coded data; a syndication mechanism to facilitate keeping terminologies up todate; and a full implementation of SNOMED CTs Expression Constraint Language (ECL), which enables sophisticateddata analytics.Conclusions: Ontoserver has been designed to overcome some of the challenges that have hindered adoption ofstandardised clinical terminologies and is used in several organisations throughout Australia. Increasing adoption is animportant goal because it will help improve the quality of clinical data, which can lead to better clinical decisionsupport and ultimately to better patient outcomes.Keywords: Clinical terminologies, Interoperability, SNOMED CT, FHIRBackgroundThe problem of sharing and reusing knowledge in soft-ware systems is common across many domains. In thearea of health there have been several efforts to create clin-ical ontologies to address this issue, such as SNOMEDCT,considered the most comprehensive clinical terminol-ogy currently available, with more than 300,000 medicalconcepts. However, building and maintaining clinical ter-minologies is considered a hard problem [1, 2]. Despitethe availability of SNOMED CT1 and other standardisedclinical terminologies, many clinical information systemsstill capture information in plain text or using customcode lists.There are several challenges that have hindered thewidespread adoption of clinical terminologies. Theheterogeneity and complexity of specifications results*Correspondence: alejandro.metke@csiro.auThe Australian e-Health Research Centre, CSIRO, Level 5 UQ Health SciencesBuilding, Royal Brisbane and Womens Hospital, QLD 4029 Herston, Australiain a significant effort for implementors. For exam-ple, SNOMED CT is distributed in Release Format 2(RF2) [3], a table-based format that is non-trivial to pro-cess (the SNOMED CT implementation guide is over 700pages long). Other clinical terminologies are modelledin completely different formats. For example, LOINC isdistributed in comma-separated values (CSV) files andspreadsheets. There are also many ontologies availablenatively in OWL format. All of this is compounded bya lack of expertise in the area, which usually requiresknowledge in a wide range of technologies, including pro-gramming, description logics and ontology reasoners. Aterminology server should be able to provide access toany clinical terminology in a standardised manner andthus shield implementers and end users from all of theunderlying complexity.Some terminologies include a vast amount of contentwhich makes finding a specific concept challenging. Inorder to drive their adoption, it is of fundamental impor-tance to provide a search mechanism that is effective and© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Metke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 2 of 10responsive. It should also be possible to define subsetsfor specific contexts, for example a subset of concepts foruse in an emergency department, in order to improve thequality of the results and reduce the search space.Another important difficulty when dealing with clini-cal terminologies is keeping them up to date. Accessingthe content of a clinical terminology usually involves anindexing process that can be time consuming and compu-tationally expensive. As releases become more frequent,this can impose a heavy burden on downstream servers.A terminology server should include a distribution mech-anism that facilitates keeping clinical terminologies up todate in a straightforward manner.Finally, when an existing clinical system does not sup-port coded data or uses proprietary code lists, updatingit to use a standardised terminology requires a significantamount of effort. Therefore, stakeholders need to be ableto clearly see the value of such an investment. Data analyt-ics is one of the benefits of adopting a standardised clinicalterminology that generates significant value. Therefore, aterminology server should be able to provide advancedconcept querying capabilities.In this paper we describe Ontoserver [4], a clinical ter-minology server based on the HL7s Fast Health Interop-erability Resources (FHIR) standard, which was designedto overcome the challenges mentioned before and there-fore contribute to the widespread adoption of clinicalterminology.ImplementationOntoserver is implemented as a Java application and itshigh level architecture is shown in Fig. 1. There are fourmain features that drove the development of this versionof Ontoserver: adopting a terminology service standard,supporting several key clinical terminologies out of thebox, designing a mechanism to easily keep the terminolo-gies up to date and providing effective concept search. Thefollowing sections describe the implementation of thesefeatures in detail.Terminology service standardOntoserver was implemented based on the terminol-ogy subset of the Fast Health Interoperability Resources(FHIR) standard [5, 6]. Other standards, such as the Com-mon Terminology Services 2 (CTS2) [7], could have alsobeen used for this purpose, but FHIR was chosen becauseof its developer-focused development approach. Adopt-ing the FHIR specification allows Ontoserver to providea unified API to access any clinical terminology, includ-ing LOINC, SNOMED CT and any local extensions withbespoke terms, in a simple and well-defined manner. Thisalso allows clients to easily switch to other FHIR-basedimplementations. The following is a brief overview of themain resources involved in implementing a FHIR termi-nology server2.A code system represents a set of codes from a system.Each clinical terminology of interest is represented by acode system resource within a FHIR server. Ontoserverprovides out-of-the-box support for SNOMED CT andLOINC. It also supports any OWL ontology through anexternal transformation service. Users can also create andupload custom code systems.The main operations defined by this FHIR resource arelookup and subsumes. The lookup operation retrievesdetails about a concept, such as properties and additionallabels (designations). The subsumes operation determineswhat subsumption relationship holds between the spec-ified codes, if any. More details about the code systemresource can be found in the FHIR documentation avail-able at http://hl7.org/fhir/codesystem.html.Fig. 1 High-level architecture of Ontoserver. Ontoserver is a RESTful server that provides three APIs: a FHIR API, implemented using the HAPI FHIRlibrary, to support terminology functionality, an administration API used to implement functionality that is not defined in the FHIR specification,such as uploading a SNOMED CT code system in RF2 format, and a syndication API that is used to consume and expose syndicated terminologyresources. It uses a Postgres database to store FHIR resources and a Lucene index to support searching. The application is deployed using DockerMetke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 3 of 10A value set represents a subset of codes drawn fromone or more code systems. A code system usually has acanonical value set that represents all of its codes. Valuesets can be implicitly or explicitly defined. Implicit valuesets may be defined for a specific code system (such asSNOMEDCT or LOINC), based on its underlying seman-tics. Table 1 shows some examples of implicit value sets inSNOMED CT.Explicit value sets can be defined by using a combi-nation of include and exclude statements. Within these,the users can refer to codes explicitly, by using filters,or by importing other value sets. The FHIR specifica-tion defines a set of filters that is common to all codesystems. Each code system can also define filters spe-cific to it. For example, the specification defines a filter,constraint, for SNOMED CT that uses the ExpressionConstraint Language (ECL), a language developed specif-ically for SNOMED CT to define subsets of concepts thatsatisfy certain criteria [8]. This filter is not used with othercode systems because the ECL is specific to SNOMEDCT.The expand operation on a value set resolves its mem-bers. When a value set is defined as a list of codes, theresult of this operation is trivial. However, complex valuesets that use filters or import other value sets need to beevaluated at runtime to produce the expansion. The otheroperation offered by the value set resource is validate-code which indicates if a code is part of a value set.More details about the value set resource can be found inthe FHIR documentation available at http://hl7.org/fhir/valueset.html.A concept map defines the relationships between twosets of codes, i.e., it defines the relationships between asource and a target value set. One of the most impor-tant operations supported by the concept map resource istranslate which returns a mapping between a code froma source value set to a code in a target value set, if such amapping exists. The mapping includes the type of equiva-lence between the codes. More details about the conceptmap resource can be found in the FHIR documentation athttp://hl7.org/fhir/conceptmap.html.Clinical terminology supportOntoserver supports several clinical terminologies out ofthe box. In order to expose them through the FHIR API,Table 1 Examples of SNOMED CT implicit value setsDescription URLAll concepts http://snomed.info/sct?fhir_vsAll concepts subsumed by theclinical finding concept (i.e. allclinical findings)http://snomed.info/sct?fhir_vs=isa/404684003All concepts that belong to theemergency department referencesethttp://snomed.info/sct?fhir_vs=refset/171881000036108Ontoserver needs to be able to import and index con-tent in each native format. The following sections describehow this support is implemented for SNOMED CT,LOINC and OWL ontologies.SNOMED CTSNOMED CT is currently distributed as a collectionof RF2 files. The set of core files is used to representconcepts, descriptions and relationships, which form theprimary content of the distribution. In addition to this,there is also an extensible pattern, referred to as ref-erence sets, that is used to provide additional informa-tion. The most important of these additional files is theModule Dependency Reference Set (MDRS), which rep-resents dependencies between different SNOMED CTmodules, for example, between the Australian extensionand the international version, as well as dependenciesbetween different module versions. All of these files canbe uploaded to Ontoserver in order to support import-ing and indexing multiple versions of SNOMED CTmodules.An extension to the RF2 specification that allows rep-resenting concrete domains is used in Australia to modelthe AustralianMedicines Terminology. Concrete domainsallowmodelling properties that have concrete values, suchas strings or integers, using predicates such as = or ?.SNOMED CT was originally built using a subset of theOWLEL profile [9] that did not include concrete domains.Even thoughmost of its content can be correctly modelledusing this subset, some concepts cannot be fully modelledwithout concrete domains. For example, it is impossible tofully represent aHydrochlorothiazide 50mg tablet withoutusing a data literal to represent the quantity of the activeingredient. To overcome this limitation, the AustralianDigital Health Agency (ADHA) proposed a mechanismto represent a limited form of concrete domains usingRF2 reference sets. This mechanism allows maintain-ing compatibility with existing RF2 processing tools thatdo not support concrete domains but adds significantcomplexity for developers that want to support the newfunctionality in their tools. Ontoserver also supports thisextension.SNOMED CT also defines the Expression ConstraintLanguage (ECL) which allows building expressions thatselect sets of concepts. FHIR defines a mechanism to useECL expressions as filters in value set definitions and thesearch operators also allow referring to value set mem-bership. Therefore, when systems have data coded withSNOMED CT, ECL can be used to create complex queriesthrough the use of value sets. Ontoserver provides a com-plete implementation of ECL. More details about thefeatures available for the SNOMED CT code system canbe found in the FHIR documentation at http://hl7.org/fhir/snomedct.html.Metke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 4 of 10LOINCLOINC is a database and universal set of test identifiersfor medical laboratory observations and other health data[10]. It is currently distributed as a collection of CSVfiles and spreadsheets packaged in a ZIP file. The LOINCimporter extracts the required files from the archive andthe LOINC indexer reads them and creates a Luceneindex. Each version of LOINC is distributed as a separatearchive, so the handling of versions is much simpler thanwith SNOMED CT. More details about the features avail-able for the LOINC code system can be found in the FHIRdocumentation at http://hl7.org/fhir/loinc.html.OWL ontologiesOWL ontologies are supported by providing an exter-nal service that transforms OWL files into FHIR codesystem resources. Implementing an external transforma-tion has several advantages over implementing a customFHIR importer. First, even though a set of sensible defaultsneeds to be defined for the result of the transformation,the resulting code systems can be easily tweaked by theusers before uploading them to the server. Also, an exter-nal transformation allows storing the resulting code sys-tem in any capable FHIR server, not just Ontoserver. Notethat an external transformation service is not suitable forontologies such as SNOMED CT because these includeelements such as implicit value sets that are impractical tocreate externally.Figure 2 shows the high level steps involved in thistransformation. First, the OWL ontology is loaded andclassified using any ontology reasoner that supports theOWL-API [11]. Our implementation supports the OWLEL profile but other profiles are supported as long as areasoner for that profile exists and implements the OWL-API. Then, a FHIR code system is created and its attributesare populated based on certain values in the ontology.These values are shown in detail in Table 2. Most of theseare straightforward mappings between attributes in thesource ontology and attributes in the target code sys-tem. However, the system provides configuration optionsto override these default mappings when more than onesource attribute can be used.The generated code system also includes a set of prop-erties that are mainly used to provide an easy way forclients to access some of the most important informationabout the concepts. Table 3 shows these properties andhow their values are calculated.The main challenge in this process is the mismatch interms of modularity supported by OWL ontologies andFHIR code systems. FHIR code systems do not supportmodularity, while OWL ontologies support it throughan import mechanism. Therefore, once the target codesystem is created, the system iterates over all the ontolo-gies referenced by the main ontology, including itself. Forexample, the Human Phenotype Ontology (HPO) imports11 additional ontologies, some of which also import otherontologies, so this process would iterate over 12 differentontologies, HPO and the 11 additional ontologies refer-enced by it. All the concepts from every ontology in theimports closure are added to the target code system.In addition to this, a value set is created for the mainontology (excluding its imports) and for every importedontology. This allows preserving the correct hierarchy inthe code system (because some elements in the hierar-chy might come from the imported ontologies) and atthe same time allows restricting search to just the mainontology (or any of the imported ontologies) by using thevalue sets.Effective concept searchFinding a concept in a clinical terminology is a keyoperation in many contexts, such as entering codedFig. 2 High level view of OWL to FHIR transformation. OWL ontologies are supported in Ontoserver through a service that transforms an OWL fileinto a FHIR Code System. The process involves classifying the ontology, merging all imported ontologies into a single code system and creatingvalue sets for all imported ontologiesMetke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 5 of 10Table 2 Code system elements and their correspondingelements in OWL ontologiesCodeSystemOWL CommentsId - The id of the resource is local so it can bepassed in as a parameter to thetransformation.Url OntologyIRIThe IRI is optional in an OWL ontology. Ifit is not present then the transformationstops.Version OntologyversionIf the ontology has no version, then theversion is set to NA. The user can mod-ify this or remove the version altogether.Name rdfs:label,OntologyIRIIf the ontology has been annotated withan RDFS label, then the first occurrenceis used as the code systems name.Otherwise the ontologys IRI is used.Publisher Publisher This element can be configured (thedefault value is http://purl.org/dc/elements/1.1/publisher).Description Subject,rdfs:commentThese elements can be configured(defaults values are http://purl.org/dc/elements/1.1/subject and http://www.w3.org/2000/01/rdf-schema#comment).Status - Always set to ACTIVE.ValueSet OntologyIRISet by default to the same URI as thecode system.HierarchyMeaning- Always set to SUBSUMES.data in a clinical system. In many cases entering struc-tured data is an additional burden on the end user, whowould typically prefer to capture information using natu-ral language. Therefore it is important that users are ableto quickly locate a concept of interest with minimal effortwhich means the system should return a good ranking ofpotential matches and should also do it quickly. The useof value sets is useful in this context because it constrainsthe search to a more manageable set. However, in manycases the search space can still be very large. Also, usersare unlikely to formulate entire queries as they would doTable 3 Properties added to the code systemCodeSystemOWL CommentsParent - This property is used to provide an easyway for clients to access the direct parentsof a concept. This is useful when creatinga graphical view of the code system. It iscalculated using the reasoner.Root - This property is used to inform clients if aconcept is a root. It is calculated using thereasoner.Deprecated Annotation In OWL a class is marked as deprecated byannotating it with a deprecationannotation.in a web search scenario but rather just start typing andexpect autocomplete-style results.There have been few studies on this type of search onlarge clinical terminologies. The most relevant one is thework by Sevenster, van Ommering and Qian [12], wherea user experiment was conducted to evaluate two auto-completion algorithms, standard breadth-first (SBF) andmulti-prefix matching (MPM), on a large medical vocabu-lary. The former extends the string the user is typing to theright. For example, if the user searches for acute append inSNOMED CT, the algorithm could yield the strings acuteappendicitis and acute appendicitis with peritonitis, forexample, but not the string acute focal appendicitis. Thelatter matches the terms whose prefixes match the stringtyped by the user. For example, the search string ac appwould match acute appendicitis, acute appendicitis withperitonitis and acute focal appendicitis. The user exper-iment concluded that the MPM algorithm performedbetter, requiring fewer keystrokes to obtain a certain tar-get concept. One of the key aspects of theMPM algorithmis how to rank the matches. The authors propose thefollowing scoring function?(F) = 1nm?i=1|qi||F(qi)| (1)where qi is a query prefix, F(qi) is a word in the label thatmatches the prefix, m is the number of query prefixes,and n is the number of words in the matching label. Notethat the order of the query prefixes has no impact on theranking score.Searching for a concept in a clinical terminology corre-sponds to a value set expansion with a filter parameter inFHIR, which is one of the most important and frequentlyused operations when implementing search interfaces ontop of a FHIR server. The FHIR specification does notmandate how the filters should be interpreted so eachserver is free to implement this functionality however itsees fit. We implement the MPM algorithm in Ontoserverusing Lucene with some slight modifications. These arerequired because the original algorithm does not han-dle certain cases properly, such as duplicate prefixes. Oneexample of this is pne pne, which is a search string thatcould be reasonable to expect from a user searching forthe concept Pneumococcal pneumonia.Clinical terminologymaintenanceOne of the key features of Ontoserver is its ability to actas a syndication client and server. The syndication func-tionality is implemented using the Atom standard [13].Figure 3 shows how Ontoserver can act both as a clientand a server and therefore create a chain of syndicationservers. The idea behind the syndication functionality isto provide an easy mechanism for terminology servers toMetke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 6 of 10Fig. 3 Ontoservers syndication model. Ontoserver can consume and publish terminology resources to syndicate, allowing the creation of asyndication chain. In Australia, the Australian Digital Health Agency published an Atom feed that can be used as the first node in the chainkeep up to date with new releases of clinical terminologies.In Australia, for example, the Australian Digital HealthAgency (ADHA) maintains an Atom feed that containsall the releases of SNOMED CT-AU, the local version ofSNOMED CT. When an instance of Ontoserver is con-figured to point at this feed, it can easily check if a newversion of SNOMED CT-AU has been published and inthat case retrieve it and install it locally.If an organisation produces its own version of a clinicalterminology, for example, an extension of SNOMED CT-AU, they can use their instance of Ontoserver to distributeit downstream. The syndication API includes operationsto turn an instance of Ontoserver into a syndicationserver, and allows selecting which content is to be exposedin the Atom feed it generates.The syndication mechanism supports both source filesand Ontoserver binary indexes. The binary index formatcan only be processed by Ontoserver but the source filesare standard RF2 and can be processed by any client.For example, the ADHA consumes the feed to generate aweb page that allows downloading the SNOMED CT-AUreleases by a human user.When a user requests to index a code system inOntoserver, the system first looks for a compatible binaryindex in the syndication feed. If it doesnt find one, thenit looks for the source files and builds the index locally. Ifthe source files are also unavailable then the request fails.This functionality was designed in this manner becausebuilding an index locally can be computationally expen-sive and therefore it is preferable to download a prebuiltbinary index if available.Results and discussionAs mentioned in the Effective concept search section,the search algorithm implemented by Ontoserver is amodified version of an algorithm available in the pub-lished literature [12] that has already been evaluatedagainst other algorithms in terms of quality of the searchresults. The modifications only deal with edge cases andtherefore the conclusions from the user study are stillvalid. Another aspect of the evaluation that is importantto consider is the performance of the implementation,because the value set expansion operation is likely to becalled constantly by user interfaces doing concept search.Having a responsive search widget is essential to providinga good user experience. However, the FHIR specificationdoes not mandate which search algorithm to use. A faircomparison should be performed in practice, balancingspeed and success rate, because simpler search algorithmsare likely to perform better than more complex ones, butthe quality of the search results will be very different.We refer the readers to our VSTool, available at https://ontoserver.csiro.au/vstool/, which allows doing interac-tive searches across different FHIR terminology serversand looking both at the results returned as well as theresponse times. Figure 4 shows a screenshot of the toolwith the results of the pne pne query run across severalpublicly available FHIR terminology servers, includingOntoserver.One of the main reasons for the relatively low adoptionof clinical terminologies is the perception of stakehold-ers that the cost of migrating to a standardised clin-ical terminology might be too high compared to theMetke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 7 of 10Fig. 4 The FHIR ValueSet $expand Comparison Tool. The VSTool can be used to interactively compare the results and response times of differentFHIR terminology serversvalue it generates. Many terminology servers have beenimplemented in the past, such as the VOSER vocabu-lary server [14], the UMLS Knowledge Source Server[15] and the GALEN terminology server [16], amongmany others. One of the disadvantages of implementinga proprietary terminology server is that clients will haveto write different code to interact with it. This is themain advantage of implementing a FHIR-compliant ter-minology server and is the main reason why Ontoserverwas migrated to use the FHIR standard in its latestversion. Using the FHIR standard mitigates the cost ofadoption and also prevents vendor lock-in because otherFHIR-compliant implementations can be used as drop-inreplacements.There are additional challenges that have hindered theadoption of standardised clinical terminologies, includingthe technical complexity of the formats used by differentterminologies, the difficulties in keeping them up to dateand the complexities of locating concepts in large termi-nologies. Ontoserver addresses the technical complexityof terminologies by providing out-of-the-box support forSNOMED CT and LOINC, and also implementing anOWL to FHIR transformation service. To our knowl-edge, no other FHIR terminology server has implementedsupport for OWL ontologies.The clinical domain is far from static and clinical ter-minologies change often. A key challenge when using asever is keeping it up to date. This is important becauseit gives users access to the latest content, which mightinclude new concepts and bug fixes, and also becausesome licenses impose limitations on the use of older ver-sions. This challenge is addressed by implementing anopen syndication mechanism. The National Clinical Ter-minology Service (NCTS), developed in Australia by theADHA in collaboration with our research group, devel-oped a syndication API standard3 that is implementedby Ontoserver. Having the possibility of acting as both aserver and a client for syndication of terminology contentenables a straightforward mechanism to maintain multi-ple instances up to date with the most recent version ofthe terminologies of interest.Finding the right concept is also a key challenge,especially when using very large terminologies. This isaddressed by implementing a smart search algorithmbased on the published literature and modifying it slightlyto account for certain edge cases that are present inSNOMED CT.Once a terminology server is adopted and data is avail-able in coded form, Ontoserver generates value by pro-viding the building blocks for doing more sophisticatedanalytics. FHIR supports a wide range of search operatorsthat can be used as the basis for this type of functional-ity, for example, the modifiers above and below for codedtypes, which allow users to search not only for a specificMetke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 8 of 10code but also for codes that subsume or are subsumed bya code. For example the query:http://myhost.com/fhir/Observation?code:below=http://snomed.info/sct|118188004will return all the observations that refer to findings ofneonates, that is, all the observations types that are sub-sumed by the concept 118188004 | Finding of neonate.Ontoserver is currently in use in several projects.Shrimp is a terminology browser that uses Ontoserverto implement fast concept search and display a graph-ical view of the search results when the terminologyis hierarchical4. A screenshot with the pne pne searchexample discussed previously is shown in Fig. 5.Escargot is a quality assurance tool that usesOntoservers validation capabilities to find code labelsthat have been changed by end users and attempts toautomatically identify problematic cases. Some clinicalsystems allow modifying the description of a code onceit is entered. This can lead to undesirable user behavioursuch as selecting a concept that seems close enough andmodifying its display text. This can seriously impactdata quality and could even have serious consequencesin the context of a clinical decision support system.Escargot retrieves data from a FHIR server and uses thevalue set validate-code operation to determine whichcode descriptions have been modified and no longermatch any of the original labels. It then uses patternsto automatically flag potentially problematic changes.Figure 6 shows a screenshot of the application.Finally, an example of the use of Ontoservers ECLimplementation can be found in a tool called SNOMap,which was developed as a way to automatically mapSNOMED CT codes into ICD10-AM codes. This isimportant because many organisations still rely on ICDcodes to support Activity Based Funding so migrating asystem to use SNOMED CT, which is clinically-focusedandmore granular, can impact this model. The tool is nowin use in several hospitals in Australia.ConclusionDespite the availability of many high-quality clinical ter-minologies such as SNOMED CT and LOINC, therehas been a slow uptake of terminology in clinical sys-tems. In this paper we identify several key challenges thathave hindered adoption and show how Ontoserver hasbeen designed to help overcome them. This is importantbecause adoption of standardised clinical terminologiesand the use of coded data is key to improving the qualityof clinical data, which can lead to better clinical decisionsupport and ultimately to better patient outcomes. Wealso show examples where Ontoserver is currently beingused.There are threemain features that are planned for futureversions of Ontoserver. The first one is boosting the rank-ing of search results based on a value set. This is usefulwhen constraining the results to a value set is too restric-tive and it is still desirable to give the user the possibilityof selecting a concept from a bigger set. The second oneis support for post-coordination. FHIR defines a closureoperation that can be used to maintain a client-side clo-sure table based on the terminological logic in the serverthat is built incrementally. When a client encounters aFig. 5 The Shrimp terminology browser Shrimp is a browser. for hierarchical terminologies like SNOMED CT and AMT. It is HTML5, SVG, andJavascript based, and runs in most modern web browsersMetke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 9 of 10Fig. 6 The Escargot data validation tool. Escargot is a web tool that retrieves data from a FHIR server and uses the value set validate-code operationto determine which code descriptions have been modified and no longer match any of the original labelscode and needs to run searches that involve accessing itshierarchy, it requests the data needed to complete the clo-sure table from the server. One of the most interestingfeatures of the closure operation is that it can also workwith post-coordinated concepts, i.e., concepts that can bebuilt on the fly if they dont exist as pre-coordinated con-cepts in the terminology. Future work will focus on addingpost-coordination support to the closure operation, whichis currently implemented in Ontoserver but does notsupport post-coordination. Finally, when building conceptmaps, it is often necessary to search for potential matchesfor an entire string, not just a prefix. We refer to this typeof search as automap because it can be used to producean initial set of candidates when building a map betweentwo code systems. The current implementation is verysimple and future work will explore more sophisticatedapproaches with the goal of producingmuch better qualitymaps automatically.Availability of data andmaterialsProject name: Ontoserver. Project home page: http://ontoserver.csiro.au/. Operating system(s): Platformindependent. Programming language: Java. Otherrequirements: Docker. License: Free to use in Australia -sublicences available from the Australian Digital HealthAgency after 1 July 2016. Any restrictions to use bynon-academics: Licence required for commercial useoutside Australia. A public Ontoserver instance is avail-able worldwide for free for research purposes at https://ontoserver.csiro.au/stu3-latest/.Metke-Jimenez et al. Journal of Biomedical Semantics  (2018) 9:24 Page 10 of 10Endnotes1 SNOMED CT is freely available to all members coun-tries of SNOMED International (formerly the Inter-national Health Terminology Standards DevelopmentOrganisation (IHTSDO)).2 The descriptions in this section correspond to ver-sion 3.0.0 of the FHIR specification, which was used byOntoserver v5.0.0 at the time of writing. FHIR is anevolving standard and some of these resources mighthave changed in more recent versions. A directory ofpublished versions can be found at http://hl7.org/fhir/directory.html.3NCTS Conformant Server Applications TechnicalServices Specification, which can be found here: https://www.healthterminologies.gov.au/ncts/#/learn4Available at https://ontoserver.csiro.au/shrimp-fhir/.AbbreviationsADHA: Australian digital health agency; API: Application programminginterface; CSV: Comma separated values; CTS2: Common terminology services2; ECL: Expression constraint language; FHIR: Fast health interoperabilityresources; HPO: Human phenotype ontology; IHTSDO: International healthterminology standards development organisation; IRI: Internationalisedresource identifier; MDRS: Module dependency reference set; MPM: Multi-prefix matching; OWL: Web ontology language; RDFS: Resource descriptionframework schema; RF2: Release format 2; SBF: Standard breath-firstAuthors contributionsDH wrote the first version of Ontoserver. AM, ML and JS were involved in thedesign and implementation of the current version of Ontoserver (described inthis paper). AM wrote the initial version of the manuscript. JS, ML and DHrevised the manuscript. All authors read and approved the final manuscript.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Received: 13 August 2017 Accepted: 27 August 2018RESEARCH Open AccessEAPB: entropy-aware path-based metric forontology qualityYing Shen1, Daoyuan Chen1, Buzhou Tang2, Min Yang3 and Kai Lei1*AbstractBackground: Entropy has become increasingly popular in computer science and information theory because it canbe used to measure the predictability and redundancy of knowledge bases, especially ontologies. However, currententropy applications that evaluate ontologies consider only single-point connectivity rather than path connectivity,and they assign equal weights to each entity and path.Results: We propose an Entropy-Aware Path-Based (EAPB) metric for ontology quality by considering the pathinformation between different vertices and textual information included in the path to calculate the connectivity pathof the whole network and dynamic weights between different nodes. The information obtained from structure-basedembedding and text-based embedding is multiplied by the connectivity matrix of the entropy computation. EAPB isanalytically evaluated against the state-of-the-art criteria. We have performed empirical analysis on real-world medicalontologies and a synthetic ontology based on the following three aspects: ontology statistical information (dataquantity), entropy evaluation (data quality), and a case study (ontology structure and text visualization). These aspectsmutually demonstrate the reliability of the proposed metric. The experimental results show that the proposed EAPBcan effectively evaluate ontologies, especially those in the medical informatics field.Conclusions: We leverage path information and textual information to enrich the network representational learningand aid in entropy computation. The analytics and assessments of semantic web can benefit from the structureinformation but also the text information. We believe that EAPB is helpful for managing ontology development andevaluation projects. Our results are reproducible and we will release the source code and ontology of this work afterpublication. (Source code and ontology: https://github.com/AnonymousResearcher1/ontologyEvaluate).Keywords: Ontology evaluation, Ontology modeling, Entropy-based metric, Knowledge representation, Big data andsemanticsBackgroundThe term ontology refers to a representation and definitionof the categories, properties, and relations of the concepts,data, and entities that substantiate one, many, or all do-mains. [1] Ontology has attracted increasing attention re-cently due to its broad applications such as informationretrieval, relation extraction, and question answering. Sig-nificant progress has been made in the ontology construc-tion [2]. However, the ontology evaluation is still a relativelynew territory and under-explored. As a result, there are fewcommonly agreed-upon methodologies and metrics forontology evaluation.Considering each ontology as a graph or a network,entropy can be used as a measure of the complexity andredundancy of the graph. Ontologies may contain dataand concepts redundancy that could be removed for thesake of consolidation and conciseness without changingthe overall meaning. The information density is opera-tionalized based on the normalized entropy measuredbetween all concept pairs in the ontology [3]. States oflower entropy occur when ontology become organized.In the literature, the entropy evaluation of the lexical in-formation included in the ontology has been studied inthe last decade and have been proved to be helpful forontology evaluation [4].* Correspondence: leik@pkusz.edu.cnYing Shen and Daoyuan Chen contributed equally to this work.1Shenzhen Key Lab for Information Centric Networking & BlockchainTechnology (ICNLAB), School of Electronics and Computer Engineering,Peking University Shenzhen Graduate School, 518055 Shenzhen, PeoplesRepublic of ChinaFull list of author information is available at the end of the article© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link tothe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Shen et al. Journal of Biomedical Semantics  (2018) 9:20 https://doi.org/10.1186/s13326-018-0188-7Despite the effectiveness of previous studies, currententropy applications used to evaluate ontology havethree limitations, in that they (1) Exclusively considersingle point connectivity rather than paths [5], which ne-glects information pertaining to non-adjacent nodes. (2)Assign equal weights to edges and paths [6], which in-duces a loss of diversity. (3) Assume vertices are static,which ignores the various aspects of vertices when inter-acting with different neighboring vertices [7].To address these three limitations, this article de-scribes an Entropy-Aware Path-Eased quality metric forontologies (EAPB) by comparing their information dens-ities to those of other ontologies. We consider the pathinformation between different vertices in ontology aswell as the textual information included in the path tocalculate the dynamic weight between different nodesand the connectivity path of the entire network. Specific-ally, we first apply CNN to learn the structure-based em-bedding and text-based embedding to capture both theontology network structures and their encapsulated text-ual information. Subsequently, the information gainwhich is in the form of a matrix obtained by a cosinesimilarity calculation of the relevancy between nodes uand v, is multiplied by the connectivity matrix of entropycomputations. Finally, we validate the effectiveness androbust superiority of our model on four real-worldontologies.Three infectious disease-relevant ontologies, i.e. Infec-tious Disease Ontology1 (IDO), Infectious Disease Ontol-ogy for Dengue2 (IDODEN), and Disease Ontology3 (DO)are adopted as baselines. Our material includes anin-house ontology that is used to develop anontology-driven clinical decision support system for infec-tious disease diagnosis and antibiotic prescription(IDDAP) [8]. To demonstrate the applicability and gener-ality of our quality metric for ontologies, we conduct eval-uations on real-world ontologies with different structuresand different textual information. To verify whether ourquality metric can make a significant performance boostby incorporating textual information into the EAPB archi-tecture, we assess ontologies with the same structures butdifferent textual information, as well as report the ablationtests in terms of discarding the textual information. Thetextual attention visualization and ontology statistical in-Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 DOI 10.1186/s13326-017-0174-5RESEARCH Open AccessOpenBiodiv-O: ontology of theOpenBiodiv knowledge management systemViktor Senderov1,2* , Kiril Simov3, Nico Franz4, Pavel Stoev1,7, Terry Catapano5, Donat Agosti5,Guido Sautter5, Robert A. Morris6 and Lyubomir Penev1,2AbstractBackground: The biodiversity domain, and in particular biological taxonomy, is moving in the direction ofsemantization of its research outputs. The present work introduces OpenBiodiv-O, the ontology that serves as the basisof the OpenBiodiv Knowledge Management System. Our intent is to provide an ontology that fills the gaps betweenontologies for biodiversity resources, such as DarwinCore-based ontologies, and semantic publishing ontologies, suchas the SPAR Ontologies. We bridge this gap by providing an ontology focusing on biological taxonomy.Results: OpenBiodiv-O introduces classes, properties, and axioms in the domains of scholarly biodiversity publishingand biological taxonomy and aligns them with several important domain ontologies (FaBiO, DoCO, DwC, Darwin-SW,NOMEN, ENVO). By doing so, it bridges the ontological gap across scholarly biodiversity publishing and biologicaltaxonomy and allows for the creation of a Linked Open Dataset (LOD) of biodiversity information (a biodiversityknowledge graph) and enables the creation of the OpenBiodiv Knowledge Management System.A key feature of the ontology is that it is an ontology of the scientific process of biological taxonomy and not of anyparticular state of knowledge. This feature allows it to express a multiplicity of scientific opinions. The resultingOpenBiodiv knowledge system may gain a high level of trust in the scientific community as it does not force ascientific opinion on its users (e.g. practicing taxonomists, library researchers, etc.), but rather provides the tools forexperts to encode different views as science progresses.Conclusions: OpenBiodiv-O provides a conceptual model of the structure of a biodiversity publication and thedevelopment of related taxonomic concepts. It also serves as the basis for the OpenBiodiv Knowledge ManagementSystem.Keywords: Biodiversity, Biodiversity informatics, Semantic web, Semantic publishing, Ontology, Knowledgemanagement, Linked open data, RDF, OWL, Taxonomy, Concept taxonomy, Biological systematics, Data modelingBackgroundThe desire for an integrated information system servingthe needs of the biodiversity community dates at least asfar back as 1985 when the Taxonomy Database WorkingGroup (TDWG)later renamed to Biodiversity InformaticsStandardswas established [1]. In 1999, the GlobalBiodiversity Information Facility (GBIF) was createdafter the Organization for Economic Cooperation andDevelopment (OECD) had arrived at the conclusion that*Correspondence: vsenderov@gmail.com1Pensoft Publishers, Prof. Georgi Zlatarski 12, 1700 Sofia, Bulgaria2Institute of Biodiversity and Ecosystems Research, Bulgarian Academy ofSciences, Sofia, BulgariaFull list of author information is available at the end of the articlean international mechanism is needed to make biodi-versity data and information accessible worldwide [2].The Bouchout declaration [3] crowned the results ofthe pro-iBiosphere project (2012 - 2014) [4] dedicatedto the task of creating an integrated biodiversity infor-mation system. The Bouchout declaration proposes tomake scholarly biodiversity knowledge freely available asLinked Open Data. A parallel process in the U.S.A. startedeven earlier with the establishment of the Global NamesArchitecture [5, 6].The specification and design of a semantic system,the Open Biodiversity Knowledge Management System(OBKMS, later simply OpenBiodiv), implementing theobjectives of the Bouchout Declaration by focusing© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, andreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to theCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 2 of 15on knowledge extraction from academic journals andresearch databases, were outlined amongst others in [7, 8].In this publication we present the OpenBiodiv Ontology(OpenBiodiv-O)the knowledge and inferencing modelof OpenBiodiv [9]. OpenBiodiv-O provides a conceptualmodel of the structure of a biodiversity publication andthe development of related taxonomic concepts.Previous workIn the biomedical domain there are well-establishedefforts to extract information and discover knowledgefrom literature [1012]. The biodiversity domain, andin particular biological systematics and taxonomy (fromhere on in this paper referred to as taxonomy), is alsomoving in the direction of semantization of its researchoutputs [1315]. The publishing domain has been mod-eled through the Semantic Publishing and ReferencingOntologies (SPAR Ontologies) [16]. The SPAR Ontologiesare a collection of ontologies incorporatingamongstothersFaBiO, the FRBR-aligned Bibliographic Ontology[17], and DoCO, the Document Component Ontology[18]. The SPAR Ontologies provide a set of classes andproperties for the description of general-purpose jour-nal articles, their components, and related publishingresources. Taxonomic articles and their components, onthe other hand, have been modeled through the TaxPubXML Document Type Definition (DTD) (also referredto loosely as XML schema) and the Treatment Ontolo-gies [19, 20]. While TaxPub is the XML-schema of taxo-nomic publishing for several important taxonomic jour-nals (e.g. ZooKeys, Biodiversity Data Journal), the Treat-ment Ontologies are still in development and have servedas a conceptual template for OpenBiodiv-O. In fact, theyshare many of the same authors.Taxonomic nomenclature is a discipline with a verylong tradition. It transitioned to its modern form withthe publication of the Linnaean System [21]. Already bythe beginning of the last century, there were hundreds ofvocabulary terms (e.g. types) [22]. At present the namingof organismal groups is governed by by the InternationalCode of Zoological Nomenclature (ICZN) [23] and bythe International Code of Nomenclature for algae, fungi,and plants (Melbourne Code) [24]. Due to their com-plexity (e.g. ICZN has 18 chapters and 3 appendices), itproved challenging to create a top-down ontology of bio-logical nomenclature. Example attempts include the rela-tively complete NOMEN ontology [25] and the somewhatless complete Taxonomic Nomenclatural Status Terms(TNSS) [26].There are several projects that are aimed at model-ing the broader biodiversity domain conceptually. DarwinSemantic Web (Darwin-SW) [27] adapts the previouslyexisting Darwin Core (DwC) terms [28] as RDF. Thesemodels deal primarily with organismal occurrence data.Modeling and formalization of the strictly taxonomicdomain has been discussed by Berendsohn [29] and later,e.g., in [30, 31]. Noteworthy efforts are the XML-basedTaxonomic Concept Transfer Schema [32] and a nowdefunct Taxon Concept ontology [33].AimsThe present work introduces OpenBiodiv-O, which servesas the basis of OpenBiodiv. By developing an ontologyfocusing on biological taxonomy, our intent is to providean ontology that fills in the gaps between ontologies forbiodiversity resources such as Darwin-SW and semanticpublishing ontologies such as the ontologies comprisingthe SPAROntologies. Moreover, we take the view that it isadvantageous tomodel the taxonomic process itself ratherthan any particular state of knowledge.OpenBiodiv [8] lifts biodiversity information fromscholarly publications and academic databases into a comput-able semantic form. The implementation of the system willbe treated in future works. In this contribution, we discussOpenBiodiv-O by first introducing the modeled domainconceptually and then formalizing it in Results section.Domain descriptionBiological taxonomy is a very old discipline dating backpossibly to Aristotle, whose fundamental insight was togroup living things in a hierarchy [34]. The discipline tookits modern form after Carl Linnaeus (1707 - 1778) [34]. Inhis Systema Naturae Linnaeus proposed to group organ-isms into kingdoms, classes, orders, genera, and speciesbearing latinized scientific names with a strictly pre-scribed syntax. Linnaeus listed possible alternative namesand gave a characteristic description of the groups [21].These groups are called taxa, which is a Greek word forarrangement. The hierarchy that taxa form is called tax-onomy. The etymology of the word is Greek and roughlytranslates tomethod of arranging. Note the polysemy here:the science of biological taxonomy is called taxonomy asis the arrangement of taxa itself. We believe, however,that it is sufficiently clear from context what is meant bytaxonomy in any particular usage throughout this paper.Even though Linnaeus and his colleagues may havehoped to describe life on Earth during their lifetimes, wenow know that there are millions of species still undiscov-ered and undescribed [35]. On the other hand, our under-standing of species and higher-rank taxonomic conceptschanges as evolutionary biology advances [36]. Therefore,an accurate and evolutionarily reliable description of lifeon Earth is a perpetual process and cannot be completedwith a single project that can be converted into an ontol-ogy. Thus, our aim is not to create an ontology capturing afixed view of biological taxonomy, but to create an ontol-ogy of the taxonomic process. The ongoing use of thisontology will enable the formal description of taxonomicSenderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 3 of 15biodiversity knowledge at any given point in time. In thefollowing paragraphs, we introduce what the taxonomicprocess entails and reflect on the resources that needmodeling.An examination of the taxonomic process reveals thattaxonomy works by employing the scientific method:researchers examine specimens and, based on the phe-notypic and genetic variation that they observe, form ahypothesis [37]. This hypothesis may be called a taxo-nomic concept, a potential taxon, a species hypothesis[29], or an operational taxonomic unit (OTU) [38] in thecase of a numerically delimited taxon.A taxonomic concept describes the allowable pheno-typic, genomic, or other variation within a taxon bydesignating type specimens and describing charactersexplicitly. It is a valid falsifiable scientific claim as it needsto fulfill certain verifiable evolutionary requirements. Forexample, a species-rank taxonomic hypothesis needs tofit our current understanding of species (species con-cept, [36]). More generally, the aspiration is that speciesconcepts are adequate and give certain tangible criteriafor species delimitation. However, valid scientific discus-sions continue about concept adequacy. The discussionsare nuanced because they often draw on different con-ceptions of the relative weight of certain evolutionaryphenomena. This leads to having quite a few differ-ent species conceptsmorphological, ecological, phylo-genetic, genomic, biological, etc. [36]. Nevertheless, if wefix a species conceptlet us say we take the biologicalspecies conceptwe can falsify any given species-ranktaxonomic hypothesis against our fixed species concept.Similarly, hypotheses of higher rank (representing upperlevels of the taxonomic hierarchy) also need to fulfill cer-tain evolutionary requirements. For example, a moderngenus concept requires all species assigned to it to bedescendants of a separate lineage and to form a mono-phyletic clade.The ranks (taxonomy hierarchy levels) are not com-pletely fixed. The usage of lower ranks (species, genus,family, order) is governed by international Codes [23, 24].In the example of Linnaeus ranks, each organism is first amember of its species, then genus, then order, then class,and finally kingdom. Which specific ranks a given taxo-nomic study employs is dependent on the field (e.g. botanyvs. zoology), on the particular author, on the level of tax-onomic resolution required, as well as on the history ofclassifying in that particular group.Once the researchers have formed their concept, itmust be published in a scientific outlet (journal or book).The biological Codes put some requirements and recom-mendations aimed at ensuring the quality of publishedresearch but ultimately it is a democratic process guar-anteeing that everyone may publish taxonomic conceptsprovided they follow the rules of the Codes. This meansthat in order to create a knowledge base of biodiversity, weneed to be able to mine taxonomic papers from legacy andmodern journals and books.As a first good approximation, a taxonomic concept isbased on a number of specimens or occurrences that arelisted in a section usually called Materials Examined. Ingeneral terms, we can say that a sighting of a living thing,i.e. an organism, at a given location and at a given time isreferred to as an occurrence, and a voucher for this occur-rence (e.g. the sampling of the organism itself ) is referredto as a specimen [27]. Moreover, a taxonomic article mayinclude other specialized sections such as the Checklistsection, where one may list all taxa (in fact: the taxonomicconcepts for those taxa) for organisms observed in a givenregion.Typically, the information content of a treatment con-sists of several units. First, we have the aforementionednomenclatural information that pertains to the scien-tific nameits authorship, etymology, related names, etc.Then, we have the taxonomic concept information thatcan be considered to have two components, as well: thefirst one is the intensional component of the taxonomicconcept made up mostly of traits or characters. Traitsare an explicit definition of the allowable variation (e.g.phenotypic, genomic, or ecological) of the organisms thatmake up the taxon. For example, we can define the orderof spiders, Araneae, to be the class of organisms that havespecialized appendages used for sperm transfer calledpedipalps [39]. Knowledge of this kind is found in theDiagnosis, Description, Distribution and other subsec-tions of the treatment.Non-traditionally delimited taxonomic hypotheses arecalled operational taxonomic units (OTUs). In the caseof genomic delimitation, sometimes the concepts arepublished directly as database entries and not as Code-compliant taxonomic articles [40]. A genomic delimita-tion can, for example, be based on a barcode sequence andon a statistical clustering algorithm specifying the allow-able sequence variability that an organism can possessin order to be considered part of the barcode sequence-bearing operational taxonomic unit. However, as, in thegeneral case, we dont have a Linnaean name or a mor-phological description for an operational taxonomic unit,we refer to it as a dark taxon [40]. The term dark is,however, usually reserved for concepts at lower ranks.Operational taxonomic units are published, for example,in the form of barcode identification numbers (BINs) inthe Barcode of Life Data Systems (BOLD) [41], or asspecies hypotheses in Unified system for the DNA basedfungal species linked to the classification (UNITE) [42].The second part of the information content of ataxonomic concept is the ostensive component: a list-ing of some (but not necessarily all) of the organismsthat belong to the taxonomic concept. This informationSenderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 4 of 15is found in the Materials Examined subsection of thetreatment.Finally, the relationships between taxonomic conceptssimple hierarchical (is a) or more fine-grained RegionConnection Calculus 5 (RCC-5) [30, 43]can be bothintensionally defined in the nomenclature section orostensively inferred from the Materials Examined. How-ever, given the customary idiosyncrasies of biologicaldescriptions, providing an initial set of RCC-5 relation-ships for a machine reasoner to work with often requiresexpert assessment and cannot be easily lifted from thetext.Thus, in order to model the taxonomic process, ourontology models scholarly taxonomic papers, databaseentries, agents responsible for their creation, treatments,taxonomic concepts, scientific names, occurrence andspecimen information, other entities (e.g. ecological, geo-graphical) part-taking in the taxonomic process, as well asrelationships among these.MethodsOpenBiodiv-O is expressed in Resource DescriptionFramework (RDF). At the onset of the project [8], a con-sideration was made to use RDF in favor of a more com-plex data model such as Neo4Js. The choice of RDF wasmade in order to be able to incorporate the multitude ofexisting domain ontologies into the overall model.To develop the conceptualization of the taxonomic pro-cess and then the ontology we utilized the following pro-cess: (1) domain analysis and identification of importantresources and their relationships; (2) analysis of existingdata models and ontologies and identification of missingclasses and properties for the successful formalization ofthe domain.The formal structure of the ontology is specified byemploying the RDF Schema (RDFS) and the Web Ontol-ogy Language (OWL). It is encoded as a part of a lit-erate programming [44] document titled OpenBiodivOntology and Guide [45]. The structure has beenextracted from that file via knitr and provided hereas Additional file 1. It is also possible to request theontology via Curl from the endpoint with the indi-cation of content-type: application/rdf+xml.The vocabularies can be found as more additionalfiles: Taxonomic Statuses (Additional file 2) and RCC-5(Additional file 3), on the website [9], and on the GitHubpage [46] (under ontology/).A partial dataset from Pensofts journals has been gener-ated with OpenBiodiv-O and can be found at the SPARQLEndpoint <http://graph.openbiodiv.net/>, select reposi-tory obkms_i6. The endpoint is also accessible from thewebsite, <http://openbiodiv.net/>, under SPARQL End-point. Demos are available as Saved Queries from theworkbench.ResultsWe understand OpenBiodiv-O to be the shared formalspecification of the conceptualization [4749] that we haveintroduced in Background. OpenBiodiv-O describes thestructure of this conceptualization, not any particularstate of it.There are several domains in which the modeledresources fall. The first one is the scholarly biodiversitypublishing domain. The second domain is that of taxo-nomic nomenclature. The third domain is that of broadertaxonomic (biodiversity) resources (e.g. taxonomic con-cepts and their relationships, species occurrences, traits).To combine such disparate resources together we rely onSKOS [50]. Unless otherwise noted, the default names-pace of the classes and properties for this paper is <http://openbiodiv.net/>. The prefixes discussed in this paper arelisted in Additional file 1, at the beginning of the ontology.Semantic modeling of the biodiversity publishing domainAn article as suchmay be represented by a set of metadata,while its content consists of article components such assections, tables, figures and so on [51].To accommodate the specific needs of scholarly biodi-versity publishing, we introduce a new class for taxonomicarticles, Taxonomic Article (:TaxonomicArticle),new classes for specific subsections of the taxonomic arti-cle such as Taxonomic Treatment, Taxonomic Key, andTaxonomic Checklist, and a new class, Taxonomic NameUsage (:TaxonomicNameUsage), for the mentioningof a taxonomic name (see next subsection) in an article.These new classes are summarized in Table 1.The classes from this subsection are based on the Tax-Pub XML Document Type Definition (DTD) [19] (alsoTable 1 New biodiversity publishing classes introducedClass QName Comment:Treatment Section of a taxonomicarticle:NomenclatureSection Subsection of Treatment:NomenclatureHeading Contains a nomenclaturalact:NomenclatureCitationList List of citations of relatedconcepts:MaterialsExamined List of examinedspecimens:BiologySection Subsection of Treatment:DescriptionSection Subsection of Treatment:TaxonomicKey Section with anidentification key:TaxonomicChecklist Section with a list of taxafor a region:TaxonomicNameUsage Mention of a taxonomicnameSenderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 5 of 15referred to loosely as XML schema), on the structure ofBiodiversity Data Journals taxonomic paper [52], and andon the Treatment Ontologies [20].Furthermore, we introduce two properties: contains(:contains) and mentions (:mentions). Contains isused to link parts of the article together andmentions linksparts of the article to other concepts.A graphical representation of the relationships betweeninstances of the publishing-related classes that OpenBio-div introduces is to be found in the diagram in Fig. 1.Semantics, alignment, and usageOur bibliographic model has the Semantic Publishing andReferencing Ontologies (SPAROntologies) at its core witha few extensions that we have written to accommodatefor taxonomic elements. The SPAR Ontologies FRBR-aligned Bibliographic Ontology (FaBiO) uses the Func-tional Requirements for Bibliographic Records (FRBR)[53] model to separate publishable items into less or moreabstract classes. We deal primarily with the Work class,i.e. the conceptual idea behind a publishable item (e.g. thestory of War and Peace as thought up by Leo Tolstoy),and the Expression class, i.e. a version of record of a Work(e.g. War and Peace, paperback edition by WordsworthClassics).Taxonomic Article is a subclass of FaBiOs Journal Arti-cle. Furthermore Journal Article is a FRBR Expression.This implies that taxonomic articles are FRBR expres-sions as well. This has important implications later onwhen discussing taxonomic concept labels. Also, it meansthat we separate the abstract properties of an article (in aFaBiO Research Paper instance, which is aWork) from theversion of record (in a Taxonomic Article, an Expression).The taxonomic-specific section and subsectionclasses are introduced as subclasses of DiscourseElement Ontologys (DEO) Discourse Element(deo:DiscourseElement, [18]). So is the classMention (:Mention), meant to represent an areaof a document that can be considered a mention ofsomething. This class, and the corresponding property,mentions, are inspired by pext:Mention and its corre-sponding property from PROTON [54]. The redefinitionis necessary by the fact in OpenBiodiv-O they possess aslightly different semantics and a different placement inthe upper-level hierarchy. We then introduce TaxonomicName Usage as a subclass of Mention.This placement of the document component classesthat weve introduced in Discourse Element means thatthey ought to be used exactly in the same way as onewould use the other discourse elements from DEO andDoCO (analogous to e.g. deo:Introduction). Note:DEO is imported by DoCO. Figures 2 and 3 give exam-ple usage in Turtle illustrating these ideas. A caveat hereis that while the SPAR Ontologies use po:contains intheir examples, we use contains, which is a subpropertyof po:contains with the additional property of beingtransitive. We believe this definition is sensible as surely asub-subcomponent is contained in a component. All otherFig. 1 Taxonomic article diagram. A graphical representation of the relationships between instances of the publishing-related classes thatOpenBiodiv introducesSenderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 6 of 15Fig. 2 Example article metadata. This example shows how to express the metadata of a taxonomic article with the SPAR Ontologies model and theclasses that OpenBiodiv defines. The code is in Turtleaspects of expressing a taxonomic article in RDF accord-ing to OpenBiodiv-O are exactly the same as according tothe SPAR Ontologies.Semantic modeling of biological nomenclatureWhile NOMEN and TNSS (introduced in subsectionPrevious work) take a top-down approach of modelingthe nomenclatural Codes, OpenBiodiv-O takes a bottom-up approach of modeling the use of taxonomic names inarticles. Where possible we align OpenBiodiv-O classes toNOMEN.Based on the need to accommodate taxonomic con-cepts, we have defined the class hierarchy of tax-onomic names found in Fig. 4. Furthermore, wehave introduced the class Taxonomic Name Usage(:TaxonomicNameUsage). Taxonomic name usageshave been discussed widely in the community (e.g. in[55]); however, the meaning of term remains vague. Theabbreviation TNU is used interchangeably for taxonname usage and for taxonomic name usage. InOpenBiodiv-O, a taxonomic name usage is the mention-ing of a taxonomic name in the text, optionally followedby a taxonomic status.For example, Heser stoevi Deltschev 2016, sp. n. is ataxonomic name usage. The cursive text followed by theauthor and year of the original species description is thelatinized scientific name. The abbreviation sp. n. standsfor the Latin species novum, indicating the discovery of anew taxon.We also introduce the class Taxonomic Concept Label(:TaxonomicConceptLabel). A taxonomic conceptlabel (TCL) is a Linnaean name plus a reference to a pub-lication, where the discussed taxon is circumscribed. Thelink is via the keyword sec. (Latin for secundum) [29].An example would be "Andropogon virginicus var. tenuis-patheus sec. Blomquist (1948)". Here, Blomquist (1948) isSenderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 7 of 15Fig. 3 Example article structure. This examples shows how to express the article structure with the help of :contains. The code is in TurtleFig. 4 Taxonomic name class hierarchy diagram. We created this class hierarchy to accommodate both traditional taxonomic name usages and theusage of taxonomic concept labels and operational taxonomic unitsSenderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 8 of 15Table 2 OpenBiodiv taxonomic status vocabularyVocabulary instance QName Example abbrev Comment:TaxonomicUncertainty incertae sedis Taxonomic uncertainty:TaxonDiscovery sp. n. Taxonomic discovery:ReplacementName comb. n. Replacement name:UnavailableName nomen dubium Unavailable name:AvailableName stat. rev. Available name:TypeSpecimenDesignation lectotype designation Type specimen designation:TypeSpeciesDesignation type species Type species designation:NewOccurrenceRecord new country record New occurrence record (for region)a reference to [56], the publication where the concept iscircumscribed.We extracted taxonomic status abbreviations fromabout 4000 articles across four taxonomic journals(ZooKeys, Biodiversity Data Journal, PhytoKeys, andMycoKeys) in order to create a taxonomic status vocabu-lary (Additional file 2) that covers the eight most commoncases (Table 2). The Latin abbreviations that have beenclassified into these classes can be found on the OpenBiodivGitHub page [46] (See Methods section for more details).Based on our analysis of taxonomic statuses, we haveidentified two Code-compliant patterns of relationshipbetween latinized scientific names (Fig. 5). The pat-tern replacement name, implemented via the property:replacementName, indicates that a certain Linnaeanname should be used instead of another Linnaean name.It covers a wide variety of cases in the Codes, such as,for example, the placement of one species taxon in a newgenus (comb. n.), the correction of a name for nomen-clatural reasons (nomen novum), or the application ofthe Principle of Priority for the discovery of synonyms(syn. nov.) [23].The other pattern is that of related names(:relatedName). It is a broader pattern, indicatingthat two names are somehow related. For example, theymay be synonyms, with one replacing the other, or theymay point to taxonomically related taxonomic concepts.For example, Harmonia manillana (Mulsant, 1866) isrelated to Caria manillana Mulsant 1866 since, as perFig. 5 Scientific name patterns diagram. Chains of replacement namescan be followed to find the currently used name. Related nameindicates that two names are related somehow, but not which one ispreferable[57], a name-bearing type (lectotype) ofHarmonia manil-lana (Mulsant, 1866) sec. Poorani [57] is named CariamanillanaMulsant 1866.Semantics, alignment and usageAs evident from Fig. 4, OpenBiodiv-O taxonomic namesare aligned to NOMEN names.The linking between text and taxonomic names mustpass through the intermediary class Taxonomic NameUsage. As parts of the manuscript, taxonomic nameusages link document components to taxonomic names.Taxonomic name usages are contained in sections such asTreatment, and mention a taxonomic name as illustratedin the example in Fig. 6.Semantic modeling of the taxonomic conceptsIn OpenBiodiv-O taxonomic names are not the car-riers of semantic information about taxa. This taskis accomplished by a new class, Taxonomic Concept(:TaxonomicConcept). A taxonomic concept is thetheory that a taxonomist forms about a taxon ina scholarly biological taxonomic publication and thusalways has a taxonomic concept label. We also intro-duce a more general class, Operational Taxonomic Unit(:OperationalTaxonomicUnit) that can be used forall kinds of taxonomic hypotheses, including ones thatdont have a proper taxonomic concept label. The classhierarchy has been illustrated in Fig. 7.Taxonomic concepts are related to taxonomic namesincluding taxonomic concept labelsvia the propertyhas taxonomic name (:taxonomicName) and its sub-properties mimicking in their range the hierarchy oftaxonomic names that we introduced earlier. We havedefined a property specifically to link taxonomic conceptsto taxonomic concept labels, has taxonomic concept label(:taxonomicConceptLabel). The property hierarchydiagram is shown in Fig. 8.There are two ways to relate taxonomic concepts toeach other (Fig. 9). As we pointed out earlier, histori-cally taxonomic concepts form the hierarchy known asbiological taxonomy. To express such simple semanticSenderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 9 of 15Fig. 6 Example taxonomic name usage. This examples shows how taxonomic name usages link document components to taxonomic names. Thecode is in Turtlerelations, it is fully sufficient to use the SKOS semanticvocabulary [50].However, these simple relationships are not well suitedfor machine reasoning. This is why Franz and Peet [30]suggested, building on previous work by e.g. [58], touse the RCC-5 language to express relationships betweentaxonomic concepts. Furthermore, the Euler [59] programwas developed, which uses Answer Set Programming(ASP) to reason over RCC-5 taxonomic relationships. Ananswer set reasoner is not part of OpenBiodiv as this taskcan be accomplished by Euler; however, we have providedan RCC-5 dictionary class (:RCC5Dictionary), anFig. 7 Taxonomic concept diagram. A taxonomic concept is a skos:Concept, a frbr:Work, a dwc:Taxon and has at least one taxonomicconcept labelSenderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 10 of 15Fig. 8 Taxonomic name property hierarchy diagram. Property hierarchy is aligned with the taxonomic name class hierarchy and with DarwinCoreRCC-5 relation term class (:RCC5Relation), a vocab-ulary of such terms to express the RCC-5 relationshipsin RDF (Additional file 3), as well as a class and prop-erties to express RCC-5 statements (:RCC5Statement,:rcc5Property, and subproperties).Semantics and alignmentWe introduce Taxonomic Concept as equivalent(owl:equivalentClass) to the DwC term Taxon(dwc:Taxon) [60]. However, by including concept inthe class name, we highlight the fact that the semantics itcarries reflect the scientific theory of a given author abouta taxon in nature. As we mentioned earlier, our ontologymodels the ongoing still unfinished process of taxonomicdiscovery. For this reason, we also derive TaxonomicConcept from Work. This derivation fits the definitionof Work in FRBR/FaBiO, which is a distinct intellectualor artistic creation. Finally, as we use SKOS to connectFig. 9 Taxonomic concept relationships diagram. In order to express an RCC-5 relationship between concepts, create an :RCC5Sgtatement anduse the corresponding properties to link two taxonomic concepts via it. Further, taxonomic concepts are linked to traits (e.g. ecology in ENVO),occurrences (e.g. Darwin-SW) and realize treatmentsSenderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 11 of 15taxonomic concepts to each other, we derive TaxonomicConcept from SKOS Concept.As with other semantic publishing-related aspects of theontology, the creation of the RCC-5 vocabulary followsthe SPAR Ontologies model. Thus OpenBiodiv RCC-5Vocabulary (:RCC5RelationshipTerms) is a SKOSconcept scheme and every RCC-5 Relation is a SKOSconcept. This allows to seamlessly share this vocabularywith other publishers of biodiversity information that alsofollow the SPAR Ontologies model.It is important to note that we have aligned thesubproperty of has taxonomic name, has scientificname (:scientificName), to the DwC propertydwciri:scientificName. The difference is thatwhile the DwC property is unbound and provides moreflexibility, the OpenBiodiv-O property has the domainTaxonomic Concept and the range Scientific Name andprovides for inference. Furthermore, has taxonomic con-cept label is an inverse-functional property with thedomain Taxonomic Concept. This means that a given tax-onomic concept label uniquely determines its taxonomicconcept. This is accomplished by a minimum cardinalityrestriction on the property.Together with the declaration of has taxonomic conceptlabel to be an inverse functional property, we can nowlist what types of relationships between names and taxo-nomic concepts are allowed: (1) The relationship betweena taxonomic concept and a name that is not a taxonomicconcept label is many-to-manyi.e. one Linnaean namecan be amention of multiple taxonomic concepts, and onetaxonomic concept may have multiple Linnaean names.(2) The relationship between a taxonomic concept anda taxonomic concept label is one-to-many: while a taxo-nomic concept may have more than one (at least one isneeded) labels, every label uniquely identifies a concept.These logical restrictions make taxonomic concept labelsinto unique identifiers to taxonomic concepts, somethingthat Linnaean names are not.UsageFor an example of linking two taxonomic concepts toeach other, let us look at the species-rank conceptCasuarinicola australis Taylor, 2010 sec. Thorpe [61]. Itis a narrower concept than the genus-rank concept ofCasuarinicola Taylor, 2010 sec. Taylor [62]. As we havealigned our concepts to SKOS, we can use its vocabularyto express this statement as seen in the example in Fig. 10.A further example of how to utilize the OpenBiodiv RCC-5 vocabulary is found in Fig. 11.Furthermore, thanks to the alignment to DwC, we treatinstances of our class Taxonomic Concept as function-ally equivalent to DwC Taxa. This makes linking to otherbiodiversity ontologies possible. For example, the OpenBiomedical Ontologies (OBO) Population and Commu-nity Ontology (PCO) [63] has a class collection of organ-isms (http://purl.obolibrary.org/obo/PCO_0000000) thatcan be considered a superclass of DwC Taxon. There-fore, every taxonomic concept is a collection of organ-isms and the application of OBO properties on it isallowed.In the paper that inspired our Casuarinicola exam-ple [61], we read: On 26 February 2013, the specieswas found to be fairly common on Casuarina trees atThomas Bloodworth Park, Auckland. This statement canbe interpreted (in ENVO) as meaning that the taxo-nomic concept that the author formulated implies that itincludes the habitat forest biome (http://purl.obolibrary.org/obo/RO_0002303). The RDF example is shown inFig. 12.As we pointed out earlier, taxonomic concepts havean intensional component (traits or characters) and anostensive component (a list of occurrences belonging tothe concept). The ostensive component can be expressedby linking occurrences to the taxonomic concepts viaDarwin-SW. This is possible as we have aligned the TaxonConcept class to DwC Taxon used by Darwin-SW. For anexample refer to [27].Lastly, describing traits is an active area of ontolog-ical research [64]. Due to the very complex languageused to describemorphological characteristics, the Ontol-ogy Term Organizer (OTO) [64] software was developedto allow for user-created vocabularies. We will rely onsuch external efforts for expressing traits and trait equiv-alences (in the taxonomic sense) during the populationof OpenBiodiv with triples. We are tightly working withthe developers of OTO to integrate their efforts intoOpenBiodiv [65].Further, the interpretation of Taxonomic Concepts asWork means that they are realized by taxonomic treat-ments (e.g. Fig. 13).Fig. 10 Example simple taxonomic concept relationships. We can use SKOS semantic properties to illustrate simple relationships betweentaxonomic conceptsSenderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 12 of 15Fig. 11 Example of RCC-5 taxonomic concept relationships. In order to express an RCC-5 relationship between concepts, create an:RCC5Sgtatement and use the corresponding properties to link two taxonomic concepts via it. SKOS relations relate concepts directlyDiscussionOpenBiodiv-O istogether with the Treatment Ontolo-gies [20]the first effort to model taxonomic articles asRDF. It introduces classes and properties in the domainsof biodiversity publishing and biological taxonomy andaligns them with the SPAR Ontologies, the TreatmentOntologies, the Open Biomedical Ontologies (OBO), Tax-Pub, NOMEN, and DarwinCore. We believe this intro-duction bridges the ontological gap that we had outlinedin our aims and allows for the creation of a Linked OpenDataset (LOD) of biodiversity information (biodiversityknowledge graph [8, 66]).Furthermore, this biodiversity knowledge graph,together with this ontology, additional semantic rules,and user software will form the OpenBiodiv KnowledgeManagement System. This system, as any taxonomicinformation system should, has taxonomic names as a keybuilding block. For any given taxonomic name, the userwill be able to rely on two patternsreplacement nameand related nameto get answers to two questions ofhigh importance to the working taxonomist. First: whatis the current and historical usage of any given Linnaeanname? Second: given a particular name, what otherrelated names ought to be considered in a taxonomicdiscussion?Both may be useful in building semantic search appli-cations and the latter, in particular, is actively beingresearched by a group at the National Center for TextMining in the UK (NaCTeM) [67]. OpenBiodiv-O properdoes not include a mechanism for inferring replacementnames and related names; however, such mechanisms arepart of the OpenBiodiv knowledge system via SPARQLrules using information encoded in the document struc-ture (Nomenclature section). Another way to infer relatednames is via a machine learning approach to obtain fea-ture vectors of taxonomic names. Note that the ontologycan describe related names independent of the process oftheir generation and will enable the comparison of bothapproaches in a future work.On the other hand, by using OpenBiodiv-O, aknowledge-based system does not have to have a back-bone name-based taxonomy. A backbone taxonomy is asingle, monolithic hierarchy in which any and all conflictsor ambiguities have been pragmatically (socially, algorith-mically) resolved, even if there is no clear consensus in thegreater taxonomic domain. Such backbone taxonomiesare used in systems that rely solely on taxonomic names(and not concepts) as bearers of information. They areneeded as it is impossible, in such a system, to express twodifferent sets of statements for a single name.In OpenBiodiv, however, multiple hierarchies of taxo-nomic concepts may exist. For example, large synthetictaxonomies such as GBIFs backbone taxonomy [68] orCatalogue of Life [69] may not agree or may have someissues [70]. With OpenBiodiv-O, we may, in fact, incorpo-rate both these taxonomies at the same time! It is possibleaccording to the ontology to have two sets of taxonomicconcepts (even with the same taxonomic names) with adifferent hierarchical arrangement. By allowing this, weleave some room for human interpretation as an addi-tional architectural layer. Thus, we delay the decision ofwhich hierarchy to use to the user of the system (e.g. apracticing taxonomist) and not to the systems architect.Due to this design feature, it is likely that our systemstands a better chance to be trusted as a science process-enabling platform as the system architects dont force ataxonomic opinion on the practicing taxonomist.It should be noted that a successful concept-based sys-tem exists for the taxonomic order Aves (birds) [71]. Themain issue that we will face is to develop tools to enableexpert users to annotate taxonomic concepts with theproper relationships as only recently individual articlesutilizing concept taxonomy in addition to nomenclaturehave been published [43, 72, 73]. We do believe that theirFig. 12 Example of combining ENVO with OpenBiodiv-O. We create a shortcut for has habitat and instance of the forest biome and link them toour taxonomic concept in order to express the fact that specimens of it have been found to live in Casuarina treesSenderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 13 of 15Fig. 13 Example connection between a treatment and a taxonomic concept. A treatment is the realization of a taxonomic conceptnumbers will rise driven by the realization that there aresome problems with relying solely on Linnaean names forthe identification of taxonomic concepts [5, 74, 75]. Con-cept taxonomy may, in fact, become even more importantin the future as conservation efforts face challenges due tounresolved taxonomies [76]. Properly aligning taxonomicconcepts to nomenclature across revisions [77] may be thesolution.Together with taxonomic information, the ontologyallows modeling the source information in a knowledgebase. This will be useful for metastudies, for the purposesof reproducible research, and other scholarly purposes.Moreover, it will be an expert system as the knowledgeextracted will come from scholarly publications. We envi-sion the system to be able to address a wide variety oftaxonomic competency questions raised by researchersduring pro-iBiosphere [78]. Examples include: Is X a validtaxonomic name (in a nomenclatorial sense)? Whichtreatments use different names for the same taxon con-cepts? Which treatments are nomenclatorially linked(including homonyms!) to another treatment?Out immediate next efforts will be concentrated on pop-ulating the ontology with triples extracted from prospec-tively published Pensoft journals [79], legacy journals text-mined by Plazi [80], as well as databases such as GBIFand Bioimages [81]. Special effort will be made to link thedataset to the Linked Open Data cloud via resources suchas geographic or institution names. In terms of extend-ing the ontological model, more research needs to gointo modeling the taxonomic concept circumscriptioncreating ontologies for morphological, genomic, or eco-logical traits. Also possibly refining the RCC-5 statementsinformed by the actual implementation. A study will becarried out to investigate the usefulness of the ontologyonce the LOD dataset had been created in a real-worldscenario.ConclusionsThe paper provides an informal conceptualization of thetaxonomic process and a formalization in OpenBiodiv-O.It introduces classes and properties in the domains of bio-diversity publishing and biological systematics and alignsthem with the important domain-specific ontologies. Bybridging the ontological gap between the publishing andthe biodiversity domains, it will enable the creation ofOpen Biodiversity Knowledge Management System, con-sisting of (1) the ontology itself; (2) a Linked OpenDataset (LOD) of biodiversity information (biodiversityknowledge graph); and (3) user interface componentsaimed at searching, browsing and discovering knowledgein big corpora of previously dispersed scholarly publica-tions. Through the usage of taxonomic concepts, we haveincludedmechanisms for democratization of the scholarlyprocess and not forcing a taxonomic opinion on the users.Additional filesAdditional file 1: Ontology is a plain text file containing statements in theTurtle syntax forming OpenBiodiv-O. It can be edited in a text (e.g. SublimeText, Emacs, etc.) or in an ontology editor (e.g. Protégé). It can be loaded itinto a triple store (e.g. GraphDB). The prefixes that are used throughout thismanuscript are defined at the beginning. This file corresponds to <http://openbiodiv.net/openbiodivo-20171103>. (TXT 22 kb)Additional file 2: Vocabulary of Taxonomic Statuses is a plain text filecontaining statements in the Turtle syntax forming the OpenBiodivVocabulary of Taxonomic Statuses. Like the ontology [Additional file 1] itcan be edited in a text or ontology editor or loaded in a triple store. Makesure you also load the ontology first. (TXT 7 kb)Additional file 3: RCC-5 Vocabulary is a plain text file containingstatements in the Turtle syntax forming the OpenBiodiv RCC-5 Vocabulary.Like the ontology [Additional file 1] it can be edited in a text or ontologyeditor or loaded in a triple store. Make sure you also load the ontology first.(TXT 5 kb)AbbreviationsLOD: Linked open data; OWL: W3C web ontology language; RCC-5: Regionconnection calculus 5; RDF: Resource description framework; RDFS: RDFschema; SPARQL: SPARQL protocol and RDF query language; XML: Extensiblemarkup languageAcknowledgementsWe acknowledge É. Ó Tuama and D. Mietchen for the many helpfuldiscussions that lead to theoretical contributions. We also acknowledge theprogramming team at Pensoft and in particular Georgi Zhelezov forweb-development in PHP and JavaScript.FundingResearch financed through the European Unions Horizon 2020 research andinnovation program under the Marie Sklodowska-Curie grant agreement No.642241.Availability of data andmaterialsA partial dataset from Pensofts journals has been generated withOpenBiodiv-O and can be found at the SPARQL Endpoint <http://213.145.125.72:7777/>, select repository obkms_i6. The endpoint is also accessible fromthe website, <http://openbiodiv.net/>. Demos are available as Saved Queriesfrom the workbench and from the website.Authors contributionsVS: Marie-Sklodowska-Curie Ph.D. student, whose main project is OpenBiodiv.LP: principal investigator, the main academic supervisor of VS, supported eachstep of the way. KS and NF are co-advisors. PS consulted on the taxonomicprocess and on the development of the Taxonomic Status Vocabulary. KSconsulted on ontological development, proof-read and improved themanuscript. NF consulted on concept taxonomy and the vision of the system,also proof-read and improved the manuscript. TC and RAM are the mainauthors of the Treatment Ontologies, which serve as a conceptual templatefor OpenBiodiv-O. They also provided proof-reading and improvements to thetext. DA and GS provided many insights into biodiversity publishing. Allauthors read and approved the final manuscript.Senderov et al. Journal of Biomedical Semantics  (2018) 9:5 Page 14 of 15Ethics approval and consent to participateNot applicableConsent for publicationNot applicableCompeting interestsThe authors declare that they have no competing interests.Publishers NoteSpringer Nature remains neutral with regard to jurisdictional claims inpublished maps and institutional affiliations.Author details1Pensoft Publishers, Prof. Georgi Zlatarski 12, 1700 Sofia, Bulgaria. 2Institute ofBiodiversity and Ecosystems Research, Bulgarian Academy of Sciences, Sofia,Bulgaria. 3Institute of Information and Communication Technologies,Bulgarian Academy of Sciences, Sofia, Bulgaria. 4Arizona State University,School of Life Sciences, Tempe Campus, 4501 Tempe, AZ, USA. 5Plazi, Bern,Switzerland. 6University of Massachusetts at Boston, Boston, USA. 7NationalMuseum of Natural History, 1 Tsar Osvoboditel Blvd., Sofia 1000, Bulgaria.Received: 18 August 2017 Accepted: 28 December 2017Faria et al. Journal of Biomedical Semantics  (2018) 9:4 DOI 10.1186/s13326-017-0170-9RESEARCH Open AccessTackling the challenges of matchingbiomedical ontologiesDaniel Faria1* , Catia Pesquita2, Isabela Mott2, Catarina Martins3, Francisco M. Couto2 and Isabel F. Cruz4AbstractBackground: Biomedical ontologies pose several challenges to ontology matching due both to the complexity ofthe biomedical domain and to the characteristics of the ontologies themselves. The biomedical tracks in the OntologyMatching Evaluation Initiative (OAEI) have spurred the development of matching systems able to tackle thesechallenges, and benchmarked their general performance. In this study, we dissect the strategies employed bymatching systems to tackle the challenges of matching biomedical ontologies and gauge the impact of the challengesthemselves on matching performance, using the AgreementMakerLight (AML) system as the platform for this study.Results: We demonstrate that the linear complexity of the hash-based searching strategy implemented by moststate-of-the-art ontology matching systems is essential for matching large biomedical ontologies efficiently. We showthat accounting for all lexical annotations (e.g., labels and synonyms) in biomedical ontologies leads to a substantialimprovement in F-measure over using only the primary name, and that accounting for the reliability of different types