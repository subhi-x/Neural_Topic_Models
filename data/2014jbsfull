JOURNAL OF
BIOMEDICAL SEMANTICS
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51
http://www.jbiomedsem.com/content/5/1/51RESEARCH Open AccessComparative analysis of knowledge representation
and reasoning requirements across a range of life
sciences textbooks
Vinay K Chaudhri1*, Daniel Elenius1, Andrew Goldenkranz2, Allison Gong3, Maryann E Martone4, William Webb5
and Neil Yorke-Smith6,7Abstract
Background: Using knowledge representation for biomedical projects is now commonplace. In previous work, we
represented the knowledge found in a college-level biology textbook in a fashion useful for answering questions.
We showed that embedding the knowledge representation and question-answering abilities in an electronic
textbook helped to engage student interest and improve learning. A natural question that arises from this
success, and this paper’s primary focus, is whether a similar approach is applicable across a range of life science
textbooks. To answer that question, we considered four different textbooks, ranging from a below-introductory college
biology text to an advanced, graduate-level neuroscience textbook. For these textbooks, we investigated the following
questions: (1) To what extent is knowledge shared between the different textbooks? (2) To what extent can the same
upper ontology be used to represent the knowledge found in different textbooks? (3) To what extent can
the questions of interest for a range of textbooks be answered by using the same reasoning mechanisms?
Results: Our existing modeling and reasoning methods apply especially well both to a textbook that is
comparable in level to the text studied in our previous work (i.e., an introductory-level text) and to a textbook
at a lower level, suggesting potential for a high degree of portability. Even for the overlapping knowledge
found across the textbooks, the level of detail covered in each textbook was different, which requires that
the representations must be customized for each textbook. We also found that for advanced textbooks, representing
models and scientific reasoning processes was particularly important.
Conclusions: With some additional work, our representation methodology would be applicable to a range of
textbooks. The requirements for knowledge representation are common across textbooks, suggesting that a
shared semantic infrastructure for the life sciences is feasible. Because our representation overlaps heavily with
those already being used for biomedical ontologies, this work suggests a natural pathway to include such
representations as part of the life sciences curriculum at different grade levels.
Keywords: Ontology, Textbook knowledge, Knowledge representation, Reasoning, Question answering,
Semantic infrastructure* Correspondence: Vinay.Chaudhri@sri.com
1SRI International, Menlo Park, CA 94025, USA
Full list of author information is available at the end of the article
© 2014 Chaudhri et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain
Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article,
unless otherwise stated.
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 2 of 19
http://www.jbiomedsem.com/content/5/1/51Background
Using knowledge representation is now commonplace
across a range of biomedical projects [1-3]. This usage
is evidenced by the success of the National Center of
Biomedical Ontologies, which, as of 2014, publishes and
disseminates more than 350 ontologies [4]. Despite this
widespread application of knowledge representation in
biomedical projects, further significant value could be
reaped: The Journal of Nucleic Acids Research catalogues
thousands of databases that could substantially benefit if
they were accompanied by an explicit ontology [5]. We
anticipate that knowledge representation will play a cru-
cial role in future biomedical research, especially for
exploiting, leveraging, and understanding big data.
During an artificial intelligence (AI) project called Project
Halo, we developed an intelligent textbook technology that
leverages an explicit ontology and a question-answering
system, and that helps students learn better [6]. Obvious
overlaps exist between the technologies used in our
project and the methods that are commonplace for bio-
medical ontologies [7,8]. This convergence presents an
unprecedented pathway for synergy between work on on-
tologies and life sciences education. If textbook knowledge
could be represented and encoded in an educational con-
text, as we propose here, then it could eventually be more
widely incorporated into biomedical projects, thus com-
plementing the existing knowledge resources.
Our work on the intelligent textbook [6] focused on
an introductory college-level biology textbook called
Campbell Biology [9]. We encoded substantial portions
of Campbell and then used this knowledge representa-
tion as a basis for an intelligent textbook called Inquire,
which enables students to explore topics across multiple
levels of organization and to pose their own questions,
which are then answered by machine reasoning. The
intelligent textbook is a powerful learning tool that both
gives students information such as definitions and de-
scriptions of terms, and enables them to explore structure,
function, and concepts across different levels of biological
organization.
The current paper’s focus is on investigating the ques-
tion: To what extent can a generic methodology for cap-
turing textbook knowledge be developed that is applicable
across a range of life sciences textbooks? We have broken
this high-level question into three sub-questions: (1) To
what extent is knowledge shared between the different
textbooks? (2) To what extent can the same upper ontol-
ogy be used to represent the knowledge found in different
textbooks? (3) To what extent can the questions of inter-
est for a range of textbooks be answered by using the
same reasoning mechanisms? A desired outcome is quan-
tifying the extent to which the already developed methods
apply to different textbooks and quantifying any differ-
ences or novel requirements across textbooks. Thesequestions are important, because if we could apply the
same methodology to textbooks at both lower and higher
grade levels, then this generalizability would enable mak-
ing semantics integral to science textbooks. The answers
to these questions will also be informative to others as
they seek insights both into generic techniques for ontol-
ogy design and into the requirements that differ across
domains.
For the remainder of this section, we give an overview
of our project, review the prior work on knowledge repre-
sentation, describe the ontology, and provide the rationale
for the textbooks that were selected for comparison. We
follow that by a description of our methods and results.
Context of Project Halo
Project Halo was an AI project funded by Vulcan, Inc.,
with the goal of creating a system called Digital Aristotle
that could answer questions on a wide variety of science
topics. SRI International participated in this project from
2003–2013 [6,10,11]. During this period, we advanced
the state of the art in knowledge base (KB) systems by
enabling domain experts with little background in know-
ledge representation to author knowledge that could be
used for answering questions. This work’s results are
embodied in a knowledge-authoring system called AURA
[11]. To demonstrate the scalability of the approach, we
used AURA to encode substantial fractions of Campbell
Biology [9], which resulted in the knowledge base KB
Bio 101 [12]. A team of biologists trained in AURA but
having no background in knowledge representation per-
formed the encoding work. We designed a knowledge-
factory process that the biologists used to systematically
convert the textbook content into KB Bio 101 [12]. Al-
though accurately assessing the total effort invested in
the encoding is difficult, we estimate that the effort was
at least twelve person years. KB Bio 101 represents a
substantial fraction of Campbell Biology and contains
more than 100,000 axioms [13].
We incorporated KB Bio 101 into an electronic textbook
application called Inquire, which helps students with read-
ing and homework problem solving [6]. An evaluation of
Inquire with students showed the practical utility of in-
corporating a KB into an electronic textbook, as the
Inquire students exhibited higher scores than did the con-
trol group and received no grades D or F, while these lower
grades were seen in the control group. A video based
on Inquire won the best video award at the annual confer-
ence of the Association for Advancement of Artificial
Intelligence (AAAI) in 2012a.
Knowledge representation in AURA
The AURA knowledge-authoring system uses Knowledge
Machine (KM) as its knowledge representation and rea-
soning engine [14]. KM supports standard representational
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 3 of 19
http://www.jbiomedsem.com/content/5/1/51features such as classes; individuals; class-subclass hier-
archy; disjointness; slots; slot hierarchy; necessary and suf-
ficient properties; and deductive rules. The representation
in KM can be formally understood as first-order logic with
equality. Uniquely, KM’s representation supports graph-
structured class descriptions. We illustrate KM in the
following example.
Suppose we wish to represent the statement: “Every cell
is an entity that has a ribosome and a chromosome as its
parts”. We can express this statement in first-order logic
as follows. (We implicitly assume that the statements hold
over all times).
Axiom A1:
? x : Cell xð Þ?? y1; y2 : Entity xð Þ
?has?part x; y1ð Þ ? has?part x; y2ð Þ
? Ribosome y1ð Þ ? Chromosome y2ð Þ
Next, suppose we wish to represent: “Every eukaryotic
cell has as parts a ribosome, a nucleus, and a eukaryotic
chromosome such that the chromosome is inside the
nucleusb”. We can capture this statement in first-order
logic as follows:
Axiom A2:
? x : Eukaryotic?Cell xð Þ?? y1; y2; y3 : Cell xð Þ
? has?part x; y1ð Þ ? has?part x; y2ð Þ
? has?part x; y3ð Þ ? is?inside y2; y3ð Þ
? Ribosome y1ð Þ ? Eukaryotic?Chromosome y2ð Þ
? Nucleus y3ð Þ
In the class definition of a Eukaryotic-Cell, specifying
the is-inside relationship between the Chromosome and
the Nucleus violates the tree model property [15]. In
models satisfying tree model property, each node has (at
most) a unique direct predecessor, and in general, it is a
good indicator of decidability. To see how the valid
models for A2 violate the tree model property, we create
a directed graph as follows: each variable in the axiom is
represented by a node, and a directed edge exists be-
tween the nodes representing a variable x and a variable
y if they both participate in the same predicate such that
x appears in the first position and y appears in the sec-
ond position. (Because DLs are limited to binary predi-
cates, we limit our discussion to only binary predicates.)
For a graph for axiom A2, the node y3 has two incoming
edges from x and y2, and thus, violates the tree model
property. DL systems achieve decidable reasoning by
limiting the representation to only allow tree models,
and this limitation is well known [16]. Active research is
in progress to address this limitation [17-20].
Next, suppose we wish to explicitly state the inherit-
ance relationships in our representation by asserting that
a Eukaryotic-Cell inherits a Chromosome and Ribosome
from a Cell, and further, by specifying the inheritedChromosome as a Eukaryotic-Chromosome. We can cap-
ture such relationships if we rewrite A1 and A2 by using
Skolemization, a well-known technique to approximate
existential variables in the antecedent of an axiom [21].
With Skolemization, in an axiom of the form ? Y1… Yn ?
X… ?, the existential variable X can be removed and re-
placed everywhere in ? with the function term f(Y1…
Yn), where f is a new function symbol that does not
occur anywhere else in the axiom. The rationale for such
a substitution is that, for any query, the original axiom is
unsatisfiable if and only if the transformed axiom is
unsatisfiable [21]. This implies that a query with an ori-
ginal axiom in the KB can be answered if and only if it
can be answered when posed against the KB with the
Skolemized version of the same axiom. However, from
the point of view of logical entailment, the Skolemized
KB is stronger than the original one, which is why we
say that Skolemization only approximates existential
quantification and is not equivalent to it. Skolemization
of A1 and A2 enables referring to the Skolem functions
introduced in them outside the scope of the existential
quantifier. In the Skolemized versions of axioms A1 and
A2 shown below, we can see that A4 refers to the Skolem
functions introduced in A3.
Axiom A3:
? x : Cell xð Þ?Entity xð Þ?
has?part x; f cell1 xð Þð Þ ? has?part x; f cell2 xð Þð Þ
? Ribosome f cell1 xð Þð Þ ? Chromosome f cell2 xð Þð Þ
Axiom A4:
? x : Eukaryotic?Cell xð Þ? Cell xð Þ ?
has?part x; f ecell1 xð Þð Þ ? has?part x; f ecell2 xð Þð Þ
? has?part x; f ecell3 xð Þð Þ
? Eukaryotic?Chromosome f ecell3 xð Þð Þ
? Nucleus f ecell1 xð Þð Þ ? Ribosome f ecell2 xð Þð Þ
? is?inside f ecell3 xð Þ; f ecell1 xð Þð Þ
? f ecell3 xð Þ ¼ f cell2 xð Þ ? f ecell2 xð Þ ¼ f cell1 xð Þ
The equality statement used in A4 proves to be a power-
ful tool that explicitly shows the inheritance relationship.
In some cases, equality statements can be inferred. For ex-
ample, if a cardinality constraint asserts that a Cell has
exactly one Chromosome, then one can deductively con-
clude that the Eukaryotic-Chromosome must be the same
as the inherited Chromosome. However, associating such
constraints is incorrect in many situations, as is the case
for a Eukaryotic-Cell.
More details about our approach to knowledge repre-
sentation [22] and reasoning are available in previously
published papers [23-25]. We have translated KB Bio 101
into multiple different formats including Web Ontology
Language Version 2 (OWL2) functionalc, answer set pro-
gramming, and the Thousands of Problems about Theorem
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 4 of 19
http://www.jbiomedsem.com/content/5/1/51Proving syntaxd. The translation into OWL2 is lossy, as it
cannot fully capture the graph structures represented in
the KB; the other translations are non-lossy. These trans-
lations are available through our websitee, and an OWL
version is available through BioPortalf.
Upper ontology in AURA
AURA uses an upper ontology called Component Library
or CLIB [26]. CLIB is a linguistically motivated ontology
designed to support representation of knowledge for au-
tomated reasoning. CLIB uses four simple, upper-level
distinctions: (1) Entity (things that are); (2) Event (things
that happen); (3) Relation (associations between things);
and (4) Role (ways in which entities participate in events).
A unique feature of CLIB is that it provides a vocabu-
lary of actions for modeling biological processes. An Ac-
tion is a subclass of Event. In CLIB, the class Action has
42 direct subclasses, with 147 subclasses in all. Examples
of direct subclasses include Attach, Impair, and Move.
Other subclasses include Move-Through (which is a sub-
class of Move) and Break (which is a subclass of Damage,
which is a subclass of Impair). To ensure generality, these
subclasses were developed by consulting lexical resources,
such as WordNet [27]; the Longman Dictionary of Con-
temporary English [28]; and Roget’s Thesaurus [29].
CLIB provides semantic relationships to define the
participants of an action. These relations are based on a
comprehensive study of case roles in linguistics [30] and
include agent, object, instrument, raw-material, result,
source, destination, and site. (The syntactic and se-
mantic definitions that we developed for these relations
are available elsewhere [31].) As an example, we considerFigure 1 A simplified view of the structure of Biomembrane represen
quantified, and every other node (shown in gray) is existentially quantified.
Biomembrane, there exists an instance of Phospholipid-Bilayer and an instan
the instance of Glycoprotein is-inside the instance of Phospholipid-Bilayer. T
instance-instance relationships [33].the definition of raw-material. The semantic definition of
raw-material is any entity that is consumed as an input to
a process. The syntactic definition of raw-material is ei-
ther it is the grammatical object of verbs such as “to use”
or “to consume”, or the word “using” precedes it.
CLIB also provides the vocabulary needed to define
the relationships that exist between entities, and between
entities and events, and to associate properties with both
entities and events. For example, the most frequent rela-
tionships help define the structural relationships that
exist between entities [32]. We use such relationships for
representing structure: has-part, has-region, material,
element, and possesses. We have developed detailed defi-
nitions and guidelines for their usage. For example, we say
that X has-region Y if Y is a region of space or a Spatial-
Entity defined only in relation to X. The complete defini-
tions of the CLIB concepts and relationships are available
onlineg.
As an illustration of the use of CLIB, in Figure 1, we
show a simplified representation of the structure of a
Biomembrane. From the representational point of view,
the graph in Figure 1 represents an existential rule of the
sort seen in axioms 1 and 2. In this figure, the node shown
in white is universally quantified, and every other node,
shown in gray, is existentially quantified. Therefore, we can
read a portion of Figure 1 as follows: for every instance of
Biomembrane, there exists an instance of Phospholipid-
Bilayer and an instance of Glycoprotein that are in has-part
relationship to it, and further the instance of Glycoprotein
is-inside the instance of Phospholipid-Bilayer. In the
context of the relationships used in biomedical ontol-
ogies, our usage of has-part and other relationshipsted in AURA. The Biomembrane node (shown in white) is universally
We can read a portion of this figure as follows: for every instance of
ce of Glycoprotein that are in has-part relationship to it, and further
he usage of has-part and other relationships corresponds to
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 5 of 19
http://www.jbiomedsem.com/content/5/1/51corresponds to instance-instance relationships [33]. The
arrows go from the first argument of a predicate to the sec-
ond argument. For example, an arrow from Biomembrane
to a Phospholipid-Bilayer labeled as has-part corresponds
to the predicate has-part(b,p), where b is an instance of a
Biomembrane, and p is an instance of a Glycoprotein.
The numbers on some of the edges indicate cardinality
constraints. For example, the instance of Phospholipid-
Bilayer in Figure 1 has exactly two phospholipid layers
that are in a has-region relationship to it. In Figure 2,
we show the functions of a Biomembrane. A portion of
this figure can be read analogously to Figure 1 as fol-
lows: for every instance of a Biomembrane, there exists a
function Block in which the agent is a Hydrophobic-
Core, the object is a Hydrophilic-Compound, and an
instrument is a Fatty-Acid-Tail. More details about our
representation of functions are available elsewhere [32].
Reasoning in AURA
The KM system [14] provided the core reasoning ser-
vices for AURA. KM’s reasoning combines description-
logic-style classification [34] with backward chaining
on rules. We extended KM’s basic reasoning with sev-
eral higher-level reasoning methods to answer ques-
tions [24,25]. AURA also contained a natural language
processing interface that processed an input English ques-
tion and converted it to a formal representation for evalu-
ation by the reasoner [11]. We list below several abstract
question templates, each followed by an example of its
instantiation. A detailed formalization of different rea-
soning processes in AURA has been published else-
where [24,25]. To make this paper self-contained, we
follow each question either by giving a high-level de-
scription of how that question was formalized or by
specifying a logical query that could be evaluated by
a general-purpose reasoner.Figure 2 Functions of Biomembrane. The top half of this figure can be rea
of chemical entities that it is permeable to, and that this movement is througQ1. What are the R of X? (e.g., What are the parts of a
cell?)
Q1 is a very common and basic form of query with
numerous variations. Because the relevant knowledge to
answer Q1 is in the form of axioms such as A1, the
formalization of Q1 contains a premise that extends the
KB to KB’ by creating a sample instance of Cell. For ex-
ample, for the class Cell, and corresponding to the axiom
A1, KB’ will contain the individual c1. By the application
of A1, KB’ is further extended by adding r1 and ch1 such
that they are instances of Ribosome and a Chromosome,
respectively, and by adding the assertions (has-part c1 r1)
and (has-part c1 ch1), which are conclusions derived by
using A1. To answer Q1, we query for all literals matching
(has-part c1 ?x), returning c1 and ch1 as answers. In more
complex examples, query evaluation can involve inheriting
information from super-classes and applying multiple
rules.
Some instantiations of Q1 leverage the relation hier-
archy in the KB. For example, “What is the structure of
a cell?” Here, the word structure maps to the has-struc-
ture relationship in our ontology, which has four sub-
relations: has-part, has-region, material, and possesses.
For the values returned for each of these relationships,
the system further retrieves spatial relationships to
complete the structural description.
In more complex forms of Q1, further constraints on the
values returned can exist. For example, consider: What does
X do during Y? Assuming that we are interested in those
steps such that X is a raw-material, those steps must also
satisfy an additional constraint that they must be sub-steps
of Y. Here, steps correspond to the phases of a process.
Q2. What are the subclasses of X? (e.g., What are the
subclasses of a eukaryotic cell?)d as follows: every Biomembrane has a function to allow Move-Through
h its Hydrophobic-Core, which is a region of its Phospholipid-Bilayer.
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 6 of 19
http://www.jbiomedsem.com/content/5/1/51Q2 is an example of a taxonomic query that queries
for all subclass relationships for a class. In AURA, this
query is answered by traversing the class-subclass hier-
archy. Other queries similar to Q2 are: What are the
super-classes of X? Is X a subclass of Y?
Q3. How many X does a Y have for a relation R? (e.g.,
How many chromosomes does a human cell have
as its part?)
Q3 queries for the cardinality constraints on the has-part
relationship for a human cell. AURA answers this query by
a straightforward lookup of cardinality constraints.
Q4. Describe X? (e.g., Describe a Cell?)
To answer Q4, AURA computes all the facts known
about a class. The facts about a class include taxonomic re-
lationships (i.e., its super-classes and subclasses as com-
puted in Q2); its relation values (as computed in Q1); and
its cardinality constraints (as computed in Q3). AURA
evaluates Q4 by issuing Q1, Q2, and Q3 as sub-queries,
and then organizes the results in a concept description
page.
Q5. What is the difference/similarity between X and Y?
(e.g., What is the difference/similarity between an
integral protein and a peripheral protein?)
AURA computes the answer to Q5 in three steps: (1)
computing descriptions of X and Y as explained in Q4,
(2) computing the similarities and differences between
the two descriptions, and (3) then summarizing the re-
sults. We have described the details of the computations
in a previous paper [25].
AURA supports more specific forms of Q5. For
example: “What are the structural differences between X
and Y?”; “What is the difference between the size of X
and size of Y?”; etc.
Q6. What is the relationship between X and Y? (e.g.,What
is the relationship between DNA and a gene?)
Here, we are interested in computing how the individual
instances of X and Y are related to each other. For ex-
ample, how is an individual instance of a DNA-Molecule
related to an individual instance of a Gene. One possible
answer to this question is that a DNA-Molecule has as its
part a DNA-Strand, which in turn, has as its part a Gene.
To answer Q6, AURA first creates an individual instance
of X and recursively computes its relation values (as in
Q1) until it encounters an instance of Y. In general, mul-
tiple such relationships exist in the KB that should be
ranked in the order of interest. AURA uses a variety ofheuristics to limit the search process (for example, first
searching the taxonomic relationships, preferring struc-
tural relationships, etc.).
AURA supports several questions that leverage the
computation supported in Q6. Examples include: “What
are structural relationships between X and Y?”; “X is to Y
as A is to what?”; and “Why is it important that X has
property Y?” To answer the question “X is to Y as A is to
what?”, AURA first computes a path between X and Y,
and then starting from A, traverses the same path to de-
termine the answer [32]. An example formulation of the
question “Why is it important that X has property Y?” is
“How does the selective permeability of membranes facili-
tates its function?” To answer this question, AURA com-
putes a path that begins from the permeability of a
membrane and ends at the function of the Membrane,
and that involves the relation facilitates [32].
We have implemented these reasoning methods in
AURA and have extensively tested them. In the first stage
of testing, we conducted a trial with students studying
from Inquire. This initial test was done for the chapter on
membranes. The results showed that the question tem-
plates were useful to the students, as the students using
the facility achieved higher scores than the students study-
ing from traditional methods, validating the choice of
question templates [6]. Once the question templates were
validated, we instantiated them for the first eleven chap-
ters. The test suite for each chapter was spread across the
content of the chapter and consisted of approximately 150
questions each. We executed the questions against AURA,
and the domain experts rated the answers for correctness.
From the 1,836 questions that we tested, the system cor-
rectly answered 1,540 questions, giving an overall correct-
ness score of approximately 85%. These results showed a
very high degree of system competence for answering
questions. (For example, IBM’s Watson system that won
the television game show Jeopardy! had a passing rate in
mid-seventies [35].)
Textbooks used for comparison
We chose to compare four textbooks spanning a range of
breadth and depth of coverage (i.e., scope) based on the fol-
lowing rationale: choose one textbook comparable to
Campbell, one textbook at a grade level lower, one textbook
at a grade level higher, and one textbook at the advanced
graduate level. Specifically, we used (1) Raven, which rep-
resents a textbook with a similar scope to Campbell [36];
(2) Levine, which offers both less breadth and depth than
Campbell, and is used in a lower-division undergraduate,
non-major course [37]; (3) Alberts, which has a narrower
breadth, but a greater depth than Campbell, and is used in
an upper-division undergraduate class in cell biology, and
is considered a reference text for cellular and molecular
biologists [38]; and (4) Kandel, which targets the specific
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 7 of 19
http://www.jbiomedsem.com/content/5/1/51field of neuroscience, and is therefore, narrower in breadth
but has greater depth than Campbell, and also contains
additional topics such as cognitive scienceh [39]. Kandel is
a textbook written for advanced undergraduates, graduate
students, and medical students studying neuroscience, a
specialized field that is largely biological, but also concerns
itself with psychology and cognitive science. Kandel differs
from the other textbooks in that different authors who are
experts in their respective fields contributed most of the
individual chapters. This approach may lead to a less-
uniform treatment across the book than the other text-
books, which are each written by a small team of authors.
Data comparing the relative lengths of these four text-
books is summarized in Table 1 below.
Goals of research
We divided the high-level goal of investigating to what
extent do our current process and methodology for cap-
turing the semantics of textbook knowledge generalize to
a range of life sciences textbooks into the following three
more-specific questions: (1) To what extent is knowledge
shared between the different textbooks? (2) To what
extent can the same ontology be used to represent the
knowledge found in different textbooks? (Based on our
work with Campbell Biology, we were aware of many of
CLIB’s limitations, especially because, from an AI perspec-
tive, fully capturing natural language text is an extremely
difficult problem. Our goal here was to quantify the extent
to which we could represent knowledge by using the exist-
ing CLIB vs. extending it to address any new require-
ments as we model different textbooks.) (3) To what
extent can the questions of interest for a new textbook be
answered by using the reasoning mechanisms already
available in AURA? Because the foundational set of ques-
tions is expected to be similar in all domains, we expected
good generality, but we wished to quantify it against each
textbook.
Methods
We now consider our methods for answering each of
the three specific questions introduced in the previous
section.Table 1 Data on page length and chapters in selected
textbooks
Textbook Pages Chapters Pages/chapter
Campbell 1263 56 23
Raven 1298 57 23
Levine 1034 45 30
Alberts 1728 45 69
Kandel 1316 67 20Domain analysis
The goal of domain analysis is to answer the question:
To what extent is knowledge shared between the differ-
ent textbooks? More specifically, we were interested in
understanding whether KB creation for each new book
should start from scratch or some knowledge from one
book could be shared from another. Answering this
question for the topics that appear in one textbook but
not in another is straightforward. Therefore, we selected
the topics of action potential and membrane structure,
which appeared in each of the four textbooks. The team
undertook a coarse analysis of the selected material and
selected a few paragraphs for detailed analysis. The team
compiled information such as the length of coverage, the
actual biological content covered, figures, and the type
of language used for describing the material. Such com-
parison gave us insight into the commonality of know-
ledge across different textbooks, and that information
guided us as to what extent we could share the domain-
specific content across the KBs for different textbooks.
Knowledge representation analysis
The goal of the knowledge representation analysis was to
answer the question: To what extent can the same upper
ontology be used to represent the knowledge found in dif-
ferent textbooks? Next, we give an overview of the AURA
knowledge-engineering process that was the basis of the
representation analysis, we provide an approach for deal-
ing with subject matter consensus, and we introduce
categories of representation requirements.
AURA knowledge-engineering process
We used an already established knowledge-engineering
process to represent the content of a textbook [31] as the
basis of this analysis. This process has two distinct phases:
(1) representation design and (2) knowledge encoding. For
the representation-requirements analysis, we performed
only the representation design phase, which includes the
following three steps: (1) determining relevance: analyze
each sentence in the textbook for its relevance for answer-
ing questions; (2) writing universal truths (UTs): for each
relevant sentence, paraphrase it as a universally true state-
ment about a specific entity or an event; and (3) develop-
ing action items for encoding: for each universally true
statement, identify the concepts and relations that will be
used for representing it.
We illustrate the above process by considering an ex-
ample sentence: “Many cells, including most prokaryotes,
also produce a strong supporting layer around the mem-
brane known as a cell wall”. Multiple UTs can be derived
from this sentence. One UT is: “Many cells produce a cell
wall”. The use of word many is also indicative of the fact
that there are some exceptions to this UT. To handle such
exceptions, our knowledge-engineering process dictates
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 8 of 19
http://www.jbiomedsem.com/content/5/1/51that this statement should be further specialized for cells
(for example, plant cells always produce a cell wall). Thus,
the UT will be reformulated as “All plant cells produce a
cell wall”.
Our general strategy to deal with exceptions is finding a
class for which that statement is applicable as a universal
truth. We ignore any exceptions that cannot be dealt with
by using such a strategy. With the CLIB ontology, the UT
under consideration will be represented by asserting that
every Plant-Cell is an agent of a process called Synthesis-
of-Cell-Wall, which has a result of Cell-Wall which is-
part-of the Plant-Cell. Here, agent, result, and is-part-of
are relations from the CLIB ontology. As a second ex-
ample, consider the UT: “Every plant cell has a cell wall
that is a strong supporting layer”. This UT will be repre-
sented by asserting that every Plant-Cell has-part a Cell-
Wall that has-function a Support that has an object the
Plant-Cell itself, and has an intensity value of strong.
Here, has-function, object, and intensity are relations in
the CLIB ontology. As a final example, consider the fol-
lowing sentence: “A protoplast is a plant cell without a cell
wall”. The UT for this sentence will be: “Every protoplast
is a cell without a cell wall”. Clearly, the sentence frag-
ment “every protoplast is a plant cell” cannot be univer-
sally true in our representation, because in that case,
Protoplast will inherit all the properties of a Plant-Cell in-
cluding a Cell-Wall. We will define Protoplast as a sub-
class of Cell in our class hierarchy. The relationship
between a Plant-Cell and a Protoplast will be captured by
other means.
Another central feature of AURA’s knowledge-engineering
process is the division of labor between knowledge engi-
neers and domain experts: the knowledge engineers have
access to the full power of the representation language—
which, as was explained earlier, is comparable to first-
order logic with equality—but the domain experts create
only new classes, declare classes to be disjoint, specify
cardinality constraints, and, most importantly, author
existential rules of the sort visualized in Figures 1 and 2.
Achieving consensus among domain experts
Our approach to achieving consensus among the differ-
ent domain experts working on the project is driven by
the following observations: (1) Even for biological know-
ledge at the level of an introductory college course, no
two textbooks are exactly the same. (2) A textbook such
as Campbell has a large number of reviewers who are
able to approve the content of the textbook. (3) Despite
the differences in the textbooks, the students can be
evaluated using a common test, and their answers can be
rated. The key lessons that we drew from the textbook-
authoring process is to aim for a process in which the
project experts could review a representation and have
an objective test for evaluating the knowledge in thesystem. We developed an extensive set of knowledge-
engineering guidelines that prescribe how the domain
experts should go about capturing textbook knowledge
[31,40]. Just as a textbook undergoes a review process, the
representations undergo a review process that ensures an
adequate application of the guidelines. This review does
not mean that a representation meets an expert’s personal
view on how the knowledge should be modeled, but ra-
ther ensures that the established encoding guidelines are
adequately applied. Question and answer pairs stated in
English provide a natural objective test to check the ad-
equacy of the representation in the same way as students
can be objectively tested on an exam.
Inventory of representation requirements
The representation requirements can be put into two
categories: (1) requirements that are already supported in
CLIB and (2) requirement that are not currently sup-
ported. When we cannot model a universal truth in a
straightforward manner by using the constructs available
in the CLIB, we note this as a new KR requirement. The
new KR requirements are strongly dependent on the state
of CLIB at the time of the analysis. For answering the
question of whether the same upper ontology could be
used across multiple textbooks, however, the primary issue
is the applicability of the representations supported in
CLIB and the commonality of each new requirement
across different textbooks.
KR requirements can arise due to the following rea-
sons: (1) The knowledge can be represented by using the
current features of the representation language and
CLIB, but no established knowledge-engineering guide-
lines exist to handle it. We refer to the challenges arising
due to this reason as process issues. (2) Representing the
knowledge requires intervention from a knowledge engin-
eer to extend the upper ontology. We refer to the issues
arising due to this reason as requiring knowledge-engineer
support. (3) Representing the knowledge is a topic of
current and future research, and the current research has
not yet been incorporated into the project. We refer to
such issues as requiring research and application. We now
give an inventory of the KR requirements that were en-
countered during the process, and we indicate into which
of the above three categories each requirement fell.
Negative information
We say that a UT has a negative information KR issue if
it cannot be modeled by using any of the four existing
methods for handling negative information: (1) disjoint-
ness between classes, (2) cardinality constraints, (3) rela-
tions with negative meaning, and (4) negative values. As
an illustration, consider the following sentence from
Raven: “Because these chains are nonpolar, they do not
form hydrogen bonds with water, and triglycerides are
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 9 of 19
http://www.jbiomedsem.com/content/5/1/51not water-soluble”. Here, we can state that a polar mol-
ecule is disjoint from a nonpolar molecule (to capture
the nonpolarity), and we can assign a value of “insoluble”
to the property solubility-in-water. In principle, one
could introduce a slot with negative meaning (for example,
does-not-form, or use a qualified number constraint on
all Create processes in which Nonpolar-Chains participate
that asserts that the result contains exactly zero Hydrogen-
Bonds). However, no established methodology exists re-
garding which approach to use. Therefore, dealing with the
example of negative information considered here is a
process issue.
Missing relationships
We say that a UT cannot be expressed because of a missing
relationship if the necessary relationship is missing from
the vocabulary. An issue already known based on our work
with Campbell is the lack of certain spatial relationships.
As an illustration of this issue, consider the following sen-
tence from Raven: “Although the distribution of membrane
lipids is symmetrical in the ER where they are synthesized,
this distribution is asymmetrical in the plasma membrane,
Golgi apparatus, and endosomes”. Here, we need a new
relation to capture asymmetrical distribution. Missing rela-
tionships require knowledge-engineer support.
Inability to state graded quantifiers
Recall that whenever the textbook uses words such as
“many”, “most”, “typically”, etc., our KE strategy is to
find a more-specific subclass for which the statement is
universally true. This strategy breaks down when the
textbook does not contain information about such a spe-
cific subclass. For example, consider the following sentence
from Levine: “Most prokaryotes and many eukaryotes have
cell walls”. The main difference between this sentence and
the sentence: “Many cells, including most prokaryotes, also
produce a strong supporting layer around the membrane
known as a cell wall”, which we considered earlier, is the
that Levine does not offer any specific examples of cells
that do contain cell walls, so we cannot apply our KE strat-
egy that worked for the earlier sentence. Whenever we
encounter such a situation, we label it as an inability to
state graded quantifiers, and it is a research and applica-
tion issue.
Modeling biological models and reified statements
The textbooks frequently describe models and theories
about natural phenomena. The statements about models
are not universally true statements, but instead are
contextual statements that hold true only in the context of
that model. As an illustration, consider the following state-
ment from Alberts: “These regions cannot be identified in
hydropathy plots and are only revealed by x-ray crystallog-
raphy, electron diffraction (a technique similar to x-raydiffraction but performed on two-dimensional arrays of
proteins), or NMR studies of the protein's three-dimensional
structure”. Here, the presence of the regions is contextual
to a particular set of techniques. Such knowledge can be
captured in AURA, but the relevant guidelines have not
been developed yet, and therefore, it is a process issue.
Property value comparison
A need frequently exists to compare property values. The
CLIB ontology contains several comparison operators for
properties, but we saw some examples where none of the
existing operators were directly applicable to some sen-
tences in the new textbooks. For example, consider the fol-
lowing sentence from Raven: “However, at the end of each
action potential, the cytoplasm contains a little more so-
dium and a little less K than it did at rest”. Here, we
need qualitative operators to capture relationships such
as “little more” and “little less”. This issue requires
knowledge-engineer support.
Causation
The notion of causality associated in the context of pro-
cesses where causal relationships of events are of primary
interest is already supported in CLIB. The textbook very
often explains things by using the words such as “be-
cause”, “causes”, etc. We use the category label of caus-
ation to capture such issues as the current CLIB does not
provide support to model such information. For example,
consider the following sentence from Alberts: “The shape
and amphiphilic nature of the phospholipid molecules
cause them to form bilayers spontaneously in aqueous
environments”. This KR requirement requires both research
and application.
Disjunction
A need arises to capture two or more alternatives in a
UT that cannot be modeled by another means. For
example, consider the following sentence from Alberts:
“Hydrophilic molecules dissolve readily in water because
they contain charged groups or uncharged polar groups
that can form either favorable electrostatic interactions or
hydrogen bonds with water molecules”. This KR require-
ment requires both research and application.
Conditionality
Capturing a conditional statement in a UT that cannot be
modeled by another means is sometimes necessary. Our
general approach for capturing conditional statements has
been using the class hierarchy. We create a new class, and
the “if” part of the condition becomes a sufficient property
for that class, while the “else” part of the condition
becomes the necessary properties of that class. Such an
approach works for most situations; but in some cases, it
leads to unnatural classes, and thus is undesirable. For
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 10 of 19
http://www.jbiomedsem.com/content/5/1/51example, consider the following sentence from Alberts:
“This change of state is called a phase transition, and the
temperature at which it occurs is lower (that is, the mem-
brane becomes more difficult to freeze) if the hydrocarbon
chains are short or have double bonds”. Here the condi-
tionality is between the temperature and the properties of
hydrocarbon chains. If we model this knowledge by using
sufficient properties, then creating unnatural classes, such
as phase transition for short hydrocarbon chains, would
be necessary. Handling this requirement is a process issue.
Possibility
Many sentences make statements of the form “A can B”,
without necessarily stating that “A always does B”. We
refer to the representation needs of such sentences as
possibility. For example, consider the following sen-
tence from Alberts: “The free hydroxyl group contrib-
utes to the polar properties of the adjacent head
group, as it can form hydrogen bonds with the head
group of a neighboring lipid, with a water molecule,
or with a membrane protein”. Dealing with this KR
requirement is a research and application. Initial steps
in this direction could be undertaken by using research
result on representing dispositions [41].
Data interpretation
In the advanced textbooks, figures are shown that
contain representative data. The text then describes the
form of the data and what conclusions either were or
could be derived from this data. Thus, the figures are
not just meant to illustrate a model but also to teach
students how the actual data led to a set of conclusions.
As an illustration, consider the following sentence from
Alberts: “In a normal unclamped axon, an inrush of
Na + through the opened Na channels produces the
spike of the action potential; inactivation of Na chan-
nels and opening of K channels bring the membrane
rapidly back down to the resting potential”. Dealing
with this requirement is a research and application
issue.
Science as a process
Particularly in Kandel and also in Alberts, many of the
biological concepts are presented in the context of the
process of science, i.e., scientists go through a process of
testing, interpreting data, and developing hypotheses
that are then tested again. For example, consider the
following sentence from Kandel: “A simple interpretation
of these results is that the depolarizing voltage step se-
quentially turns on active conductance channels for two
separate ions: one type of channel for inward current and
another for outward current”. Dealing with this require-
ment is a research and application issue.Qualitative number constraint
Our current representation approach enables quantita-
tive number constraints. We saw several examples in the
textbooks where the constraint values are qualitative, and
no other encoding approach sufficed. For example, con-
sider the following example from Raven: “Mammalian
membranes, for example, contain hundreds of chemically
distinct species of lipids”. Dealing with this requirement
requires knowledge-engineer support.
Mathematical reasoning
CLIB provides two different representations to facilitate
mathematical reasoning: (1) simple qualitative relationships
such as direct proportionality and (2) reasoning with math-
ematical equations. However, Kandel presents more com-
plicated equations beyond CLIB’s current representational
and reasoning capabilities. Kandel also includes derivations
of mathematical formulas that cannot be represented by
using current capabilities. For example, consider the follow-
ing sentences from Kandel: “When tetraethylammonium is
applied to the axon to block the K+ channels, the total
membrane current lm, consists of lc, lv and lNa. This outward
current reaches a plateau that is maintained for the dur-
ation of the pulse (Figure nine-3B)”. Dealing with this re-
quirement requires knowledge-engineer support.
Vagueness/ambiguity
Advanced textbooks cover frontiers of our knowledge,
and hence, this vagueness or ambiguity is not due to
pedagogical presentation. However, it can lead to a uni-
versally true statement that is relevant but too vague to
properly encode. These sentences are found across all
textbooks, and seem to be more common in Alberts.
(For example: “Membrane attachment through a single
lipid anchor is not very strong, however, and a second
lipid group is often added to anchor proteins more firmly
to a membrane”.) Dealing with this requirement requires
research and application.
Other issues
We use the KR category of other issues for representation
problems that do not clearly fit into any of the previous cat-
egory. For example, consider the following sentence from
Raven: “From this simple molecular framework, a large var-
iety of lipids can be constructed by varying the polar organic
group attached to the phosphate and the fatty acid chains
attached to the glycerol”. Here the author is trying to con-
vey the salient variance between different phospholipids.
Certain aspects of this knowledge are easily captured as
sufficient properties, but that approach may not always be
enough, especially to answer a question of the form “How
can you get different instances of a phospholipid?” For the
purposes of answering similarity and difference questions,
Table 2 Data on the length of description of action
potential and membrane potential
Action potential Membrane structure
Textbook Pages Images Sentences Pages Images Sentences
Campbell 7 7 91 6 12 160
Raven 10 9 58 6 5 91
Levine 2 2 37 2 1 17
Alberts 14 14 20 12 18 270
Kandel 20 16 280 4 1 75
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 11 of 19
http://www.jbiomedsem.com/content/5/1/51and relationship questions, a representation based on
sufficient properties is adequate.
Reasoning requirements analysis
The goal of the reasoning requirements analysis was
answering the question: to what extent can the questions
of interest for a new textbook be answered by using the
reasoning mechanisms already available in AURA? We
wanted to confirm that as we move across textbooks, we
would not have to develop new sets of reasoning methods
for answering questions. To perform the analysis, the
domain-expert team developed sample questions about
membrane structure and action potential for each of the
four textbooks. The overall guidance was to focus on the
kinds of questions that a student studying from the book
might have. The biologists had access to the examples of
educationally useful questions that we had previously
developed for Campbell. Some variability in the style and
difficulty of questions potentially exists, because we did
not have a mutual validation of question sets authored by
different biologists. The possibility also exists that we
biased their question-authoring effort by showing them
the questions from the prior effort on Campbell. However,
because the questions from the previous effort received
extensive feedback from multiple teachers and students,
we believe that they were a good guideline for this exercise.
The domain-expert team and the knowledge-engineering
team jointly analyzed the questions.
The questions stated in English needed to be translated
into the question templates supported by the system. Such
translation is done by AURA’s question-understanding
module [42]. In many cases, the English statement of a
question is not very helpful for determining the computa-
tion that must be performed in answering that question.
For example, consider the question: “How does the pos-
ition of the gates in gated proteins cause the blocking of the
movement of ions across the membrane?” We can re-
formulate this question as: “What is the causal relation-
ship between the position of the gates in gated proteins and
the blocking of the movement of ions across the mem-
brane?” Another formulation of the same question is:
“How are the position of the gates in gated proteins and
the blocking of the movement of ions across the membrane
causally related?” In AURA, both of these formulations
will be handled by using Q6, in which we search for
the causal relationships between the two entities in
the question, and we expect the answer to be contained
in the retrieved path. To develop such reformulations, the
knowledge engineers must extensively rely on their know-
ledge of AURA to determine whether a given question in
the corpus could be translated into one of the existing
templates. This approach introduces some imprecision
into the analysis, but this is unavoidable without undertak-
ing the actual implementation.Results and discussion
We now consider the results of our analysis of domain
knowledge, and representation and reasoning require-
ments, for the four textbooks.
Results and discussion on domain knowledge
analysis
We first analyze the two topics that we chose for compari-
son: action potential and membrane structure, and then
offer conclusions based on the analysis.
In Table 2, we summarize data about the length of de-
scription of the different topics across the five textbooks.
To the extent that different textbooks emphasize differ-
ent levels of detail, the corresponding KBs need to match
that level of detail. To make this observation concrete, we
consider below specific example comparisons of content
across the three textbooks.
Campbell covers membrane structure in greater depth
than Levine, Raven, or Kandel, but is limited in its
description of the molecular structure of phospholipids.
Raven and Alberts devote more detail to the molecular
structure of phospholipids. Levine introduces lipids but
has no mention of their more specific forms, such as
glycolipids, which are mentioned in the other textbooks.
In Kandel, membrane structure is not a major topic (it
is more a topic in general biology than in neuroscience).
Campbell describes equilibrium potential by providing a
definition and presenting an equation for the mathemat-
ical model known as the Nernst equation, along with two
examples using this equation. Raven provides a similar
amount of information to Campbell, but omits any exam-
ples using the Nernst equation. Alberts provides a defin-
ition, derives the Nernst equation, and shows several
examples. Kandel provides the greatest breadth and depth
for membrane potential, and devotes an entire chapter
(Chapter 8) to the passive electrical properties of the
neuron that are important for understanding the influence
of neuronal structure and other properties on short and
long-range signaling. Kandel also covers the contribution
of different types of membrane channels to the signaling
properties of different parts of the neuron.
Next, we consider the biological themes that occur
inconsistently across our sample of textbooks: evolution,
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 12 of 19
http://www.jbiomedsem.com/content/5/1/51disorders/disease, scientific uncertainty, and animal models.
Campbell, Raven, and Levine do not mention evolution in
the context of action potential, but Alberts and Kandel
discuss evolution of action potential function and structure
of membrane proteins, respectively. Although Campbell
and Raven omit discussion of disease, Levine, Alberts, and
Kandel provide examples of diseases that affect normal
functioning of action potentials. The presentation of
scientific uncertainty also varies considerably across text-
books. Raven omits any mention of scientific uncertainty
in the context of action potential, while Campbell and
Levine simply report its existence. Alberts suggests that
scientists will resolve uncertainty without exception, but
Kandel presents scientific inquiry with respect to action
potential as an iterative process with some degree of un-
certainty. Animal models for the study of action potential
are not described in Levine, and a single experimental
model is described in both Raven and Campbell. Although
Kandel describes a single experimental model, the giant
squid axon, this text also emphasizes experimental tech-
niques and their specific role in elucidating aspects of the
action potential. Alberts describes multiple experimental
models for the study of action potential.
The examples above suggest a great deal of common-
ality as well as differences in how different topics are de-
scribed across the textbooks. For example, on the topic
of membrane structure, the KB for Levine will contain
far fewer terms than the other KBs (e.g., terms such as
Glycolipid would need to be omitted.) Similarly, the KB
for Alberts and Raven will provide a much more detailed
account of phospholipid structure than the KB forTable 3 Observed knowledge representation issues
Category of KR issue Occurs in textbooks
Levine Raven
Negative information x
Spatial relation x x
Missing slot (other than spatial relation) x
Inability to state graded quantifiers x x
Biological models and reified statements x x
Property-value comparison x
Causation
Disjunction x
Conditionality
Possibility
Data interpretation
Science as a process
Qualitative number constraint x
Mathematical reasoning
Vagueness/ambiguity x
Other xCampbell. Similarly, while the Nernst equation will
exist in all the KBs, the example associated with its use
(as in Alberts), and a description of electrical properties (as
in Kandel), will be specific to the KBs for those textbooks
only. Differences in how to handle evolution, uncertainty,
diseases, and animal models can have major repercussions
in KB design.
Our analysis above suggests that a great deal of com-
monality across textbooks can be leveraged in creating a
KB for each of them. At the minimum, the experience and
representation approaches developed for one textbook can
contribute toward a faster design of representations for a
different textbook. Our analysis does not provide sufficient
information about whether the domain-specific axiom
writing for the textbook for a new KB should begin from
scratch or should reuse the axioms from the previous
ones. Clearly, some reuse should be possible, but the ex-
tent of reuse and its cost effectiveness is an open question.
Further, our analysis provides concrete examples of where
the textbooks have substantial differences requiring repre-
sentation design that is specific to that textbook.
Results and discussion on knowledge representation
requirements
In Table 3 below, we summarize all the KR issues along
with the textbooks for which the issue was encountered.
The column labeled as “New issue” indicates an issue
that we have not encountered or so far addressed in our
work with Campbell Biology.
In Table 4 below, we show the results that indicate the
number of UTs for each of the textbooks that could notOccurs in
Campbell?
New
issue?Alberts Kandel
x x x No
x x x No
x x x No
x x No
x x x No
x No
x x No
x x No
x x No
x x No
x x Yes
x x x No
x No
x x No
x x x No
x No
Table 4 KR requirements by category, for the topic action potential
Category of KR/KE issue Number of UTs affected (%)
Levine Raven Alberts Kandel
Negative information 3 (8%) 6 (6%) 3 (2%)
Spatial relation 3 (2%)
Missing slot (other than spatial relation) 4 (8%) 3 (3%) 3 (2%) 6 (7%)
Inability to state graded quantifiers 1 (1%) 2 (2%)
Modeling biological models and reified statements 1 (1%) 3 (3%)
Property-value comparison 3 (3%)
Causation
Disjunction 1 (3%)
Conditionality
Possibility 11 (8%)
Data interpretation 11 (8%) 4 (4%)
Science as process 8 (9%)
Qualitative number constraint
Mathematical reasoning 3 (3%)
Vagueness/ambiguity 1 (1%) 2 (1%)
Other
Total 8 (21%) 13 (13%) 35 (26%) 26 (30%)
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 13 of 19
http://www.jbiomedsem.com/content/5/1/51be adequately represented for the topic of action poten-
tial. For each UT that could not be represented, we iden-
tify a knowledge representation category to indicate the
nature of requirement. We next explain these results for
each of the textbooks.
For Levine, approximately 20% of UTs for action po-
tential had new KR requirements. In addition, negative
information that could not be adequately encoded oc-
curred for action potential, and we encountered one
instance of disjunction that could not be adequately
encoded. The issues of lack of specificity and models did
not arise for action potential for this text. For the Raven
textbook, 13% of the UTs were problematic. For Alberts,
approximately 25% of UTs presented new KR requirements
for action potential. In addition, the new KR requirement
of data interpretation arose. In Kandel, approximately 30%
of UTs presented new KR requirements. Thus, like Levine
but unlike Raven and Alberts, Kandel presented propor-
tionally more issues for action potential. For example, data
interpretation issues and science as process issues arose
frequently. Further, for action potential, Kandel contained
sentences outside AURA’s current mathematical represen-
tation and reasoning capabilities.
In Table 5 below, we show our results of how well we
could represent the topic of membrane structure for
each of the four textbooks. Detailed explanations follow.
For Levine, we encoded approximately 85% of UTs with-
out any facing any new requirements. The most common
new KR requirements were missing relations (namely,
spatial relations), and the inability to identify a sufficientlyspecific concept, as illustrated in the earlier example.
Raven exhibits a greater percentage and breadth of new
KR requirements than Levine. Nearly 35% of UTs did have
new KR requirements. The most common KR require-
ments were, again, specificity of concepts and missing
slots. A common requirement for Raven was representing
biological models. Raven (and the other textbooks) had
several examples of negative information of a form that
cannot be represented with AURA’s current capabilities.
Alberts has a similar percentage of new KR requirements
to Raven and a greater breadth. Again, more than 30% of
UTs posed some new KR requirement. Further require-
ments come from conditionality, causation, and possibility.
Because Alberts is a research-oriented textbook, it de-
scribes topics at the limit of current biological knowledge.
This leads to the greater number of UTs with the KR issues
of vagueness compared to other textbooks. For Kandel,
more than 80% of UTs were encoded without facing any
new KR requirement, and no new requirements arose that
did not arise for another textbook, except the need to
represent knowledge about science as a process. Hence, in
terms of number of issues, Kandel proved amenable to our
KE process despite its more advanced nature.
Let us now consider how these results address the ques-
tion: to what extent can the same upper ontology be used
to model knowledge across a range of life science text-
books? The results in Table 3 suggest that all the require-
ments that were identified for the new textbooks, with the
exception of data interpretation, were also requirements
for Campbell. This finding is strong evidence in support
Table 5 KR issues by category, for the topic membrane structure
Category of KR/KE issue Number of UTs affected (%)
Levine Raven Alberts Kandel
Negative information 3 (1%) 17 (7%) 2 (2%)
Spatial relation 3 (4.5%) 17 (7%) 15 (6%) 2 (2%)
Missing slot (other than spatial relation) 6 (2.5%) 8 (3%) 4 (4%)
Inability to state graded quantifiers 6 (9%) 28 (12%) 10 (4%)
Modeling biological models and reified statements 1 (1.5%) 19 (8%) 1 6 (7%)
Property-value comparison
Causation 2 (1%)
Disjunction 1
Conditionality 5 (2%)
Possibility 3 (1%)
Data interpretation
Science as process 2 (2%)
Qualitative number constraint 1
Mathematical reasoning
Vagueness/ambiguity 2 15 (6%)
Other 3 (1%)
Total 10 (15%) 79 (33%) 77 (31%) 16 (18%)
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 14 of 19
http://www.jbiomedsem.com/content/5/1/51of the claim that if these requirements were supported in
an upper ontology, such ontology would be applicable
across multiple textbooks. From Table 3, we also see that
spatial relationships and biological models are the require-
ments that occur most uniformly across the textbooks,
followed by negative information, graded quantifiers, and
science as a process. These constitute high-priority areas
for extending the CLIB ontology.
From Tables 4 and 5, we see that the existing upper
ontology enabled us to capture at least 67% of all the UTs
across all topics and across all the textbooks. In some
cases, the coverage was as high as 87%. Based on these re-
sults, we can conclude that CLIB already provides a good
foundation for representing knowledge across the range of
life science textbooks considered here.
Results and discussion on reasoning requirements
Recall that our high-level question regarding reasoning
requirements was: To what extent can the questions of
interest for a new textbook be answered by using the
reasoning mechanisms already available in AURA? We
gave an overview of the current questions supported in
an earlier section.
To answer the above question, we assembled a suite of
new questions for each of the four textbooks and put
them into two different categories: (1) answerable with
existing system capabilities, or minor extensions of them,
supposing that the requisite concepts are encoded; and
(2) require new reasoning capabilities, or major extensionsof existing capabilities, or beyond anticipated feasible
reasoning, or contingent on significant new research.
We will now present the results of our analysis and
will illustrate the questions that fall into each of these
categories.
In Table 6 below, we summarize the overall analysis of
questions about action potential and membrane structure.
Across the four textbooks on average, we observe that
for action potential, approximately 85% of the questions
are category 1 (existing capability), and 15% are category
2 (representational extension or significant reasoning re-
quirements). For membrane structure, nearly 90% of the
questions are category 1 (existing capability), and 10%
are category 2 (representational extensions or significant
reasoning requirements).
From each of the four textbooks, we now give example
question forms that could be answered by using the exist-
ing capability. For each question form, we give an example
question, its model answer if provided, and a reformula-
tion of the question. Because each of these question forms
can be answered by using the existing capability (or a
minor extension of it) through the given reformulations,
new question templates are not required.
? Question template in English: What is the role of X
(in context Y)?
? Example instantiation from the sample question set:
“In the equivalent electrical circuit model, what
cellular element serves as the resistor?” [Kandel]
Table 6 Reasoning requirements analysis for action potential and membrane structure
Action potential Membrane structure
Textbook Questions Existing Research Questions Existing Research
Raven 16 12 4 21 17 4
Levine 53 43 10 32 25 7
Alberts 19 16 3 48 47 1
Kandel 50 43 7 29 26 3
Total 138 114 22 130 115 15
The column labeled as Questions indicates the total number of questions considered in the analysis. The column labeled as Existing indicates the number of
questions that could be handled by using existing capabilities in AURA, and the column labeled as Research indicates the number of questions that cannot be
handled by the current capabilities in AURA and that require further research.
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 15 of 19
http://www.jbiomedsem.com/content/5/1/51? Question template in English: Why is it important
that X has property Y?
? Example instantiation from the sample question set:
“Why is it important that membranes are selectively
permeable?” [Levine]
? Reformulate as: “How does the selective
permeability of membranes facilitate its function?”
? Question template in English: What kinds of X are
common in Y?
? Example instantiation from the sample question set:
“What kinds of lipids are common in cell
membranes?” [Levine]
? Reformulate as: “What are the lipid parts of a cell
membrane?”
? Question template in English:What does X do during Y?
? Example instantiation from the sample question set:
“What is the sodium potassium pump doing during
an action potential?” [Levine]
? Reformulate as: “What does sodium potassium do
during an action potential?”
? Question template in English: What features of X
affects its role in Y?
? Example instantiation from the sample question set:
“What features of the voltage-gated sodium channel
affect its role in an action potential?” [Raven]
? Reformulate as: “What is the relationship
between a voltage-grated sodium channel and
action potential?” (This reformulation is
approximate as it does not specifically ask for
the relationship to role in the action potential.)
We now consider example questions that require new
question templates. For each, we give an example ques-
tion template and its instantiation.
? Question template in English: What is the importance
of X?
? Example instantiation from the sample question set:
“What is the importance of plasma membrane
fluidity?” [Alberts]? Question template in English: What aspects of X can
be seen by Y? (where Y is a inspection technique,
instrument, or process)
? Example instantiation from the sample question set:
“What aspects of the plasma membrane can be seen
by transmission electron microscopy (TEM)?” [Raven]
? Question template in English: What properties of X
contribute to the property Z of Y?
? Example instantiation from the sample question set:
“What characteristic of phospholipids contributes
most to the membrane-forming properties of these
molecules?” [Alberts]
? Question template in English: Given that X does Y,
why does Z also not do Y?
? Example instantiation from the sample question set:
“Given that the sodium-potassium pump results in a
net transport of positive ions from the inside of the cell
to the outside, why don't negative ions also leave the
cell to balance out the charge difference?” [Raven]
? Question template in English: Which strategy does X
use to achieve Y?
? Example instantiation from the sample question set:
“Vertebrate systems generally rely on what adaptive
strategy for increasing the rate of axonal
conduction?” [Kandel]
The quantitative results in Table 6 support the conclu-
sion that a large fraction of the questions in the test
suite assembled by the domain experts (greater than
85%) for a new textbook could be answered by using the
reasoning mechanisms already available in AURA. This
finding is an extremely positive result that attests to the
generality of the already-implemented reasoning mecha-
nisms. However, we would like to emphasize that given
the bias introduced by exposing the domain experts to
the existing capabilities, we should not take these results
to conclude that the existing capabilities could answer
greater than 85% of all possible questions posed against
these textbooks. These results are applicable to only to a
specific style of educationally useful questions that have
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 16 of 19
http://www.jbiomedsem.com/content/5/1/51been found helpful in our work on the intelligent text-
book. These results show that such questions have a high
degree of generality and applicability across the range of
textbooks considered in this analysis.
Comparison to related work and broader impacts
In this section, we relate the work presented here to re-
lated efforts in modeling knowledge by using OWL and
other biomedical ontology development efforts. We also
comment on how our work can be exploited by others.
Most of the representation features used in AURA are
also found in OWL (for example, classes; class-subclass
relationships; disjoint statements between classes; domain;
range; qualified number constraints; etc.). Our work to
capture graph-structured knowledge of the sort illustrated
in axioms A1–A4 is closely related to recent efforts to
extend OWL to capture graph-structured descriptions
[17]. Others have recognized the need to support graph-
structured descriptions to capture chemical structures
[16], and active research is underway to address it [17-20].
KB Bio 101 already contains several hundred examples of
complex concepts that utilize such graph-structured repre-
sentation [13], such as the ones shown in Figures 1 and 2.
One possible technique to achieve decidable reasoning in a
KB with graph-structured descriptions is to avoid certain
kinds of cyclical dependence among concepts [17], but
no empirical evaluation exists of such a technique on a
realistic, large-scale dataset. KB Bio 101 is an excellent
candidate data set for undertaking such evaluation. More
generally, KB Bio 101 can be used as a dataset for testing
techniques for ontology modularization, ontology map-
ping, ontology evaluation, development of ontology design
patterns, etc.
In several prior publications, we related the represen-
tations supported in CLIB with the ones adopted for bio-
medical ontologies (for example, in [32], we describe our
representation for structure and function; in [43], we de-
scribe representation of roles; and in [44], we describe the
representation of genetic entities). Gene Ontology or GO
[45] is a closely related community-wide effort that sup-
ports molecular-level and cellular-level representations for
gene function. Because life science textbooks cover know-
ledge at organismal, species, and population levels, the
scope of knowledge represented in KB Bio 101 is much
broader than the knowledge represented in GO.
A unique feature of our ontology that none of the other
biomedical ontologies supports is a vocabulary of process
classes (e.g., Move, Attach, Release, etc.) and their detailed
definitions using semantic relationships (e.g., agent,
object, source, destination, etc.). Due to lack of such
vocabulary, ontologies such as GO define functions using
only textual strings and functions are not compositionally
defined to capture their complete meaning. The CLIB
approach to modeling processes and their participantscan be readily exploited by biomedical ontologies to
achieve a much greater depth of knowledge capture for
biological functions.
A driving use case for GO, and a major contributor to
its success, has been its use in annotation projects. The
question templates Q1–Q6 introduced in our work can
provide another compelling use case for exploiting GO
and other biomedical ontologies. Although Q1–Q6 were
driven by the needs of education applications, similar
reasoning can be useful for biological discovery applica-
tions such as [46].
Many educational innovations begin at the graduate
level, and slowly find their way to undergraduate and
precollege-level education. Therefore, perhaps, the most
impactful way to exploit this work is using it as an ex-
ample to start incorporating biomedical ontologies into
undergraduate and high-school-level curricula for life sci-
ence education. Future life sciences graduates will need to
routinely use ontology resources, and some of these gradu-
ates will need to help create new ones. However, ontologies
are not yet a standard part of the life sciences curriculum.
Students are not normally exposed to ontologies unless
they enter a graduate program in bioinformatics. We
believe that now is the time to begin making training in
formal languages and their ontological commitments an
integral part of the life sciences curriculum. Wider use of
ontologies in the life sciences will lead to better under-
standing and communication of knowledge by teachers
and students. Such explicit usage of ontologies is different
from the methods used by search tools such as Google,
which are excellent for retrieval but do little to improve
our understanding of the subject matter.
Conclusions
We present our conclusions for each of the three major
analyses presented here: (1) domain knowledge require-
ments, (2) knowledge representation requirements, and
(3) reasoning requirements. We acknowledge at the out-
set that our conclusions are based on the data gathered
for the topics of action potential and membrane struc-
ture. Our generalized conclusions are based on the hy-
pothesis that these data could be generalized to other
biological topics in the textbook.
The results of our domain requirements analysis show
that, as expected, the Levine textbook, which is aimed at a
lower instructional level than Campbell, presents material
from a more general perspective, omitting details that
Campbell and Raven include. Likewise, the textbooks
aimed at a higher instructional level than Campbell present
details that Campbell does not. The breadth and depth of
coverage for action potential and membrane structure
appear most similar between Campbell and Raven. We also
found that the textbooks for instruction levels higher than
Campbell and for a specific field of biological sciences do
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 17 of 19
http://www.jbiomedsem.com/content/5/1/51not cover the broad range of knowledge in Campbell but
instead rely on Campbell’s prerequisite biology knowledge,
and build on a fraction of this foundation. For example,
Kandel provides considerably less breadth on the topic of
membrane structure compared to Campbell. The details of
membrane structure are likely omitted from Kandel be-
cause the authors deem such information as prerequisite or
not germane to the sub-discipline of neuroscience. Our re-
sults suggest that the modeling effort invested in represent-
ing any of these books will reduce the cost of doing
additional books. Because the considered textbooks vary in
detail, and in their choice of the aspects of biology know-
ledge to emphasize, the KB for each of these textbooks also
must be customized and made specific to that particular
textbook.
The results of our knowledge representation require-
ment analysis showed that the knowledge-engineering
process used for Campbell appears to be effective across
the range of considered textbooks. We encountered no
major surprises regarding modeling issues: most of
the issues that we saw in these textbooks also exist in
some form for Campbell. We confirmed that the stud-
ied textbooks that were written for the same grade level
(i.e., Campbell and Raven) were comparable in their
knowledge content and representation requirements. We
found an increase in presentations of theories, models,
and history in the higher-level textbooks, which is ex-
pected as the textbooks for the higher grade levels are
closer to the frontiers of knowledge. For example, Kandel
describes the experiments that are used to test a model or
hypothesis, and the reasoning process that was used to
support or refute that model. Our overall conclusion was
that our existing representation tools are applicable for
modeling knowledge across the range of considered text-
books, and that the new requirements identified here will
have broad applicability to multiple textbooks.
Based on the reasoning requirements analysis, we
can conclude that a majority of the biologist-authored,
educationally useful questions for each of the textbooks
can be adequately addressed by using extensions to
AURA’s current capabilities. This assertion is true because
all the textbooks had the same foundational set of ques-
tions and were all based on the same foundational biology.
The reasoning patterns of relationship questions and com-
parison questions seem to be directly applicable across
multiple textbooks. We also found that the answers for
one textbook may contain vocabulary or detail that is
unexpected at a different grade level. For example, Levine
does not use the term “phospholipid bilayer”. In Kandel
and Alberts, most answers are with respect to models and
cannot be considered as universally true. We definitely
cannot conclude that the existing question templates are
adequate for the space of all questions that the readers of
each of the textbooks might want to ask. Our previouswork with Campbell also showed us that the existing
templates are inadequate for capturing all the reasoning
patterns.
A possible way forward is aligning the presented repre-
sentations and approach with the methods that are
already commonplace in biomedical research, and then
start incorporating those representations in life sciences
textbooks. As an example, consider the representation
for Kandel. Especially for a field as broad as neurosci-
ence, different groups will need to be engaged for differ-
ent parts of a text like Kandel. In fact, in this textbook,
different experts author different chapters to ensure that
the content aligns with current thinking in the field.
Efforts are underway through projects like the International
Neuroinformatics Coordinating Facilityi, the Blue Brain
Projectj, and the Neuroscience Information Frameworkk
to create a semantically unified body of broad neurosci-
ence knowledge. When textbook knowledge is comple-
mented with resources like these, the enhanced version is
not only useful for biomedical research but can also serve
as a valuable education tool.
Undertaking textbook knowledge representation as pro-
posed here will profoundly shift the way we think of life
science education. The semantic representations would
serve as a conceptual mathematics that computers could
rigorously reason over. Exposure to such representations
as part of a life science education will likely instill grad-
uates with an increased level of rigor in learning and
working with biological concepts. The time is now
ripe to introduce these techniques at all levels of biol-
ogy education, so that students are well prepared for the
computational thinking [47] that is both so vital to
practitioners in today’s knowledge economy and indis-
pensable for researchers pursuing advanced biomed-
ical discoveries.
Endnotes
ahttp://www.aaaivideos.org/2012/inquire_intelligent_
textbook/
bAlmost every universally true statement in biology
has an exception. For example, there are eukaryotic cells
that do not have a nucleus.
chttp://www.w3.org/TR/owl2-syntax/
dhttp://www.tptp.org
ehttp://www.ai.sri.com/~halo/public/exported-kb/biokb.
html
fhttps://bioportal.bioontology.org/ontologies/AURA
ghttp://www.ai.sri.com/~halo/public/clib/20130328/clib-
tree.html
hThe 5th edition of Kandel appeared when our study
was underway.
ihttp://incf.org
jhttp://bluebrain.epfl.ch
khttp://neuroinfo.org
Chaudhri et al. Journal of Biomedical Semantics 2014, 5:51 Page 18 of 19
http://www.jbiomedsem.com/content/5/1/51Abbreviations
AURA: Automated User Centered Reasoning and Acquisition System;
CLIB: Component Library; GO: Gene Ontology; KB: Knowledge Base;
KE: Knowledge Engineering; KM: Knowledge Machine; KR: Knowledge
Representation; OWL: Web Ontology Language; UT: Universal Truth.
Competing interests
The authors declare that they have no competing interests.
Authors’ contributions
VKC was the technical leader for the project and was responsible for
formulating, directing, and managing the project, and for writing the final
report. DE served as a knowledge engineer and conducted the knowledge
representation and reasoning requirements analysis. A. Goldenkranz served
as a domain expert and performed the domain analysis tasks for Levin. A.
Gong served as a domain expert and performed the analysis tasks for Raven.
MM served as a domain expert and performed the analysis tasks for Kandel.
WW served as a domain expert and performed the analysis tasks for Alberts.
NYS served as a knowledge engineer, conducted the knowledge
representation and reasoning requirements analysis, and helped in writing
the report. All authors read and approved the final manuscript.
Authors’ information
Dr. Vinay K. Chaudhri is a program director in the Artificial Intelligence (AI)
Center at SRI International. His research focuses on the science and
engineering of large knowledge base systems and spans knowledge
representation and reasoning, question answering, knowledge
acquisition, and innovative applications. His most recent work has been on
creating an intelligent textbook in biology that answers a student’s questions
and leads to significant learning gains. He has co-edited a volume on the
Theory and Application of conceptual modeling, and two special issues of
AI Magazine — one on Question Answering Systems, and another on
Application of AI to Contemporary and Emerging Education Challenges.
He has taught a course on Knowledge Representation and Reasoning at
Stanford University. He holds a Ph.D. in Computer Science from University
of Toronto where he was a Connaught Scholar. He also holds a Masters in
Industrial and Management Engineering from Indian Institute of Technology
Kanpur, and a Bachelor’s degree in Mechanical Engineering from
National Institute of Technology, Kurukshetra. He is a senior member
of AAAI.
Daniel Elenius is a Computer Scientist at the Computer Science Lab at SRI
International. He holds an MS in computer science and engineering from
Linköping University, Sweden. He has developed several reasoning systems,
including a policy reasoner for the DARPA neXt Generation (XG) program, a
probabilistic fault propagation analysis tool for the DARPA META program,
and a system that reasons about hierarchical tasks and resource assignments
for the DoD ONISTT and ANSC projects. His research interests include
automated reasoning, knowledge representation, and the semantic web.
Andrew Goldenkranz is a biology teacher at Monta Vista High School in
Cupertino, California. He has helped position the Inquire application (an iPad
app for AURA) so that it is useful for teaching students studying from
Campbell Biology.
Allison Gong studies marine biology, particularly invertebrate zoology, and
teaches biology at the community college and university levels in California.
She teaches marine biology, zoology, and evolution to science majors and
science-phobes alike. Her interests in marine biology focus on marine
invertebrate life histories, larval biology, and ecology of the rocky intertidal. She
holds a PhD in biology from the University of California.
Maryann E. Martone received her BA from Wellesley College in biological
psychology and her PhD in neuroscience in 1990 from the University of
California, San Diego, where she is currently a professor in the Department
of Neuroscience. She is the principal investigator of the Neuroinformatics
Framework project, a national project to establish a uniform resource
description framework for neuroscience. Her recent work has focused on
building ontologies for neuroscience for data integration. She has completed
her tenure as the US scientific representative to the International
Neuroinformatics Coordinating Facility (INCF), where she still heads the
program on ontologies. MM recently joined FORCE11, an organization
dedicated to advancing scholarly communication and e-scholarship, as
Executive Director.William Webb is an expert in wildlife biology and has taught biology courses
to community college students for five years across multiple campuses.
Dr. Webb has community college teaching experience in diverse topics
within biology, including general education courses such as general
biology and health science, in addition to major’s courses such as human
anatomy and physiology and animal biology. He holds a PhD in wildlife science
from the University of Washington.
Neil Yorke-Smith is an assistant professor at the American University of Beirut,
Lebanon and a visiting scholar at St Edmund’s College, Cambridge. His
research interests include intelligent agents, planning and scheduling,
constraint-based modeling, intelligent user interfaces, and their real-world
application to managerial decision-making. He holds a PhD in optimization
from Imperial College London, UK.
Acknowledgements
Vulcan Inc. and SRI International funded this work. We thank the AURA
development team for providing the context for this effort. We thank Prof.
Craig Heller for his comments on an early version of this manuscript. Finally,
we thank Ilinca Tudose for her work on the representation of Biomembrane
that is used in this paper.
Author details
1SRI International, Menlo Park, CA 94025, USA. 2Monta Vista High School,
Cupertino, CA, USA. 3Cabrillo College, Aptos, CA, USA. 4University of
California, San Diego, CA, USA. 5Foothill Community College, Los Altos Hills,
CA, USA. 6American University of Beirut, Beirut, Lebanon. 7University of
Cambridge, Cambridge, UK.
Received: 23 May 2014 Accepted: 26 November 2014
Published: 18 December 2014
JOURNAL OF
BIOMEDICAL SEMANTICS
Diallo Journal of Biomedical Semantics 2014, 5:44
http://www.jbiomedsem.com/content/5/1/44RESEARCH Open AccessAn effective method of large scale ontology
matching
Gayo DialloAbstract
Background: We are currently facing a proliferation of heterogeneous biomedical data sources accessible through
various knowledge-based applications. These data are annotated by increasingly extensive and widely disseminated
knowledge organisation systems ranging from simple terminologies and structured vocabularies to formal ontologies.
In order to solve the interoperability issue, which arises due to the heterogeneity of these ontologies, an alignment task
is usually performed. However, while significant effort has been made to provide tools that automatically align small
ontologies containing hundreds or thousands of entities, little attention has been paid to the matching of large sized
ontologies in the life sciences domain.
Results: We have designed and implemented ServOMap, an effective method for large scale ontology matching. It
is a fast and efficient high precision system able to perform matching of input ontologies containing hundreds of
thousands of entities. The system, which was included in the 2012 and 2013 editions of the Ontology Alignment
Evaluation Initiative campaign, performed very well. It was ranked among the top systems for the large ontologies
matching.
Conclusions: We proposed an approach for large scale ontology matching relying on Information Retrieval (IR)
techniques and the combination of lexical and machine learning contextual similarity computing for the generation of
candidate mappings. It is particularly adapted to the life sciences domain as many of the ontologies in this domain
benefit from synonym terms taken from the Unified Medical Language System and that can be used by our IR strategy.
The ServOMap system we implemented is able to deal with hundreds of thousands entities with an efficient
computation time.
Keywords: Ontology matching, Life sciences ontologies, Entity similarity, Information retrieval, Machine learning,
Semantic interoperabilityIntroduction
With the wide adoption of Semantic Web technologies,
the increasing availability of knowledge-based applications
in the life sciences domain raises the issue of finding
possible mappings between the underlying knowledge
organisation systems (KOS). Indeed, various terminolo-
gies, structured vocabularies and ontologies are used to
annotate data and the Linked Open Data Initiative is
increasing this activity. The life sciences domain is very
prolific in developing KOS ([1-4] are examples of such
resources) and intensively using them for different pur-
poses including documents classification [5] and coding
systems to Electronic Health Records [6].Correspondence: gayo.diallo@u-bordeaux.fr
University Bordeaux, ISPED, Centre INSERM U897, F-33000 Bordeaux, France
© 2014 Diallo; licensee BioMed Central Ltd. Th
Commons Attribution License (http://creativec
reproduction in any medium, provided the orOne of the key roles played by these KOS is providing
support for data exchanges based on a common syntax
and shared semantics. This particular issue makes them
a central component within the Semantic Web, the
emerging e-science and e-health infrastructure.
These KOS, which are independently developed at the
discretion of various project members, are heteroge-
neous in nature, arising from the terminology used, the
knowledge representation language, the level of semantics
or the granularity of the encoded knowledge. Moreover,
they are becoming more complex, large and multilingual.
For instance, the Systematized Nomenclature of Medicine–
Clinical Terms (SNOMED-CT) [7], a multiaxial, hier-
archical classification system that is used by physicians
and other healthcare providers to encode clinical health
information, contains more than 300,000 regularly evolvingis is an Open Access article distributed under the terms of the Creative
ommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
iginal work is properly credited.
Diallo Journal of Biomedical Semantics 2014, 5:44 Page 2 of 19
http://www.jbiomedsem.com/content/5/1/44concepts. Each concept is designated by synonymous
terms, sometimes by several. Another example is the
International Classification of Diseases (ICD), the World
Health Organization’s standard diagnostic tool for epi-
demiology, health management and clinical purposes used
to monitor the incidence and prevalence of diseases and
other health issues. The current ICD-10 version con-
tains more than 12,000 concepts designated with terms
in 43 different languages including English, Spanish and
French.
There is a clear need to establish mappings between
these different KOS in order to make inter-operable
systems that use them. For instance, the EU-ADR pro-
ject [8] developed a computerised system that exploits
data from eight European healthcare databases and
electronic health records for the early detection of
adverse drug reactions (ADR). As these databases use
different medical terminologies (ICD-9, ICD-10, Read
Codes, International Classification of Primary Care) to
encode their data, mappings are needed to translate a
query posed to the global system into queries under-
standable for the different data sources. Performing
manual mappings between all the mentioned resources
is not feasible within a reasonable time. Generally
speaking, the data integration domain [9], the semantic
browsing of information domains [10] and web ser-
vices composition [11] are areas where the matching of
knowledge resources is usually performed.
There is therefore a crucial need for tools which are
able to perform fast and automated mapping computa-
tion between entities of different KOS and which can
scale to large ontologies and mapping sets. Significant
effort has been expended in the ontology alignment/
matching domain. A matching system is defined by the
Ontology Alignment Evaluation Initiative (OAEI) [12]
as a software program capable of finding mappings
between the vocabularies of a given set of input ontol-
ogies [13]. Formally, given two ontologies, a mapping is
a 4-tuple [14]:
< id; e1; e2; r >
such that:
  id is an identifier for the given mapping;
  e1 and e2 are entities, i.e. classes and properties of
the first and second ontology, respectively;
  r is a relation, e.g. equivalence (=), subsumption (?),
disjointness (?) between e1 and e2.
Some metadata, including a confidence value, w (usually
? [0, 1]), are often associated with the mapping.
In the following section we will briefly give an over-
view of different approaches and systems in line with theapproach we propose in this paper. In particular, we will
review approaches which use a space reduction strategy
for large scale ontology matching and machine learning-
(ML) based matching and briefly present systems evalu-
ated recently for the largest task in the context of the
international OAEI campaign. We will further discuss,
in Discussion, systems for matching ontologies in the
biomedical domain.
Related work
Ontology matching is an active research area. Existing
ontology matching systems use terminological, structural
and semantic features for the computation of candidate
mappings (please see [14-16] for a complete survey).
Despite the advances achieved in matching relatively
small size ontologies, the large scale matching problem
still presents real challenges to tackle, due to the com-
plexity of such a task. These challenges include effi-
ciency issues in term of space and time consumption,
the use of background knowledge, user involvement
and the automated evaluation of the matching system
[14,17]. Therefore, approaches for ontology matching
have been proposed in the literature including cluster-
ing and blocking strategies (reduction of search space),
ML- based matching (in particular for reusing existing
alignments or combing results for parallel matches),
interactive alignment (taking into account the user) and
the use of specialised background knowledge (in particular
for the life sciences domain).
A structure-based clustering approach for the matching
of large ontologies is introduced in [18]. The idea is to
partition each input schema graph into a set of dis-
jointed clusters before identifying similar clusters in the
two schema graphs to be matched. The COMA++ system
[19] is finally used to solve individual matching tasks and
combine their results. Hamdi et al. provide TaxoMap [20],
a tool which is based on the implementation of the
partition-based matching algorithm proposed in [21] to
find oriented alignment from two input ontologies.
TaxoMap provides one-to-many mappings between single
concepts and establishes three types of relationships:
equivalence, subclass and semantically related relation-
ships. The semantically related relationships denote an
untyped link indicating the closeness of two concepts.
Hu et al. [21] address the issue of aligning large ontol-
ogies by proposing a partition-based block approach for
the matching of large class hierarchies. Their matching
process is based on predefined anchors and uses struc-
tural affinities and linguistic similarities to partition
small block input class hierarchies. In contrast to these
divide-and-conquer methods, Wang et al. [22] use two
kinds of reduction anchors to match large ontologies
and reduce time complexity. In order to predict ignor-
able similarity calculations, positive reduction anchors
Diallo Journal of Biomedical Semantics 2014, 5:44 Page 3 of 19
http://www.jbiomedsem.com/content/5/1/44use the concept hierarchy while negative reduction
anchors use locality of matching. A partial reference
alignment strategy is used in [23] in order to partition
ontologies to be aligned, computing similarities be-
tween terms and filter mapping suggestions. To test
the approach, alignments provided by OAEI and from
previous evaluation of the SAMBO system [24] are
used.
On the other hand, Nezhadi et al. use an ML approach
to combine similarity measures of different categories in
order to align two given ontologies [25]. Their evaluation
of different learning classifiers – K Nearest Neighbor,
Support Vector Machine (SVM), Decision Tree (DT)
and AdaBoost – on real life (small) ontologies for bib-
PROCEEDINGS Open Access
SEE: structured representation of scientific
evidence in the biomedical domain using
Semantic Web techniques
Christian Bölling1*, Michael Weidlich2, Hermann-Georg Holzhütter1
From Bio-Ontologies Special Interest Group 2013
Berlin, Germany. 20 July 2013
* Correspondence: christian.a.
boelling@gmail.com
1Institute of Biochemistry, Charité
Universitätsmedizin Berlin, Berlin,
Germany
Abstract
Background: Accounts of evidence are vital to evaluate and reproduce scientific
findings and integrate data on an informed basis. Currently, such accounts are often
inadequate, unstandardized and inaccessible for computational knowledge
engineering even though computational technologies, among them those of the
semantic web, are ever more employed to represent, disseminate and integrate
biomedical data and knowledge.
Results: We present SEE (Semantic EvidencE), an RDF/OWL based approach for
detailed representation of evidence in terms of the argumentative structure of the
supporting background for claims even in complex settings. We derive design
principles and identify minimal components for the representation of evidence. We
specify the Reasoning and Discourse Ontology (RDO), an OWL representation of the
model of scientific claims, their subjects, their provenance and their argumentative
relations underlying the SEE approach. We demonstrate the application of SEE and
illustrate its design patterns in a case study by providing an expressive account of
the evidence for certain claims regarding the isolation of the enzyme glutamine
synthetase.
Conclusions: SEE is suited to provide coherent and computationally accessible
representations of evidence-related information such as the materials, methods,
assumptions, reasoning and information sources used to establish a scientific finding
by adopting a consistently claim-based perspective on scientific results and their
evidence. SEE allows for extensible evidence representations, in which the level of
detail can be adjusted and which can be extended as needed. It supports
representation of arbitrary many consecutive layers of interpretation and attribution
and different evaluations of the same data. SEE and its underlying model could be a
valuable component in a variety of use cases that require careful representation or
examination of evidence for data presented on the semantic web or in other
formats.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2014 Bölling et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in
any medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver (http://
creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Background
Scientific evidence, as a concept, can be defined as information that is relevant to assess
the likelihood that a particular scientific idea is correct. Representation of the corre-
sponding evidence is therefore key to evaluating hypotheses and assessing claims con-
tained in scientific articles, databases or any other repository of scientific information.
Biomedical knowledge is often highly context-dependent and based on evidence obtained
from the skilful combination and evaluation of individual results, involving, among other
aspects, a range of model organisms, diverse experimental and computational techniques,
different forms of interpretation, and various inference schemes. Consequently, all those
aspects - the materials, methods and information sources used, the observations made,
the reasoning employed and the context-specific assumptions made - are important for
comprehensive evidence accounts. Likewise, when data, often from disparate sources, is
integrated to study complex biological systems an account of the evidence that was used
to infer a model’s properties and those of and among its components is critical for cor-
rect and transparent understanding of that model.
Scientific findings are now routinely published as resources on the World Wide
Web. Besides electronic versions of natural language texts more and more information
from both new and legacy sources becomes available through databases [1] and web
services [2] which provide through structured formats and interfaces consolidated
views of and programmatic access to biomedical data. Semantic web technologies and
standards in particular offer by virtue of their well-defined semantics and broad applic-
ability potent means for the computational integration and analysis of biomedical data
from heterogeneous and distributed sources on a large scale [3-5]. Accordingly, the
Resource Description Framework (RDF, [6]) is increasingly employed to represent and
disseminate new and legacy biomedical data [7,8] and biomedical ontologies specified
in the Web Ontology Language (OWL, [9]) are being developed to encode domain-
specific knowledge and annotate data from biomedical investigations [10-12]. As with
any other means for communicating scientific results, findings encoded in semantic
web formats need to be accompanied by an account of how they have been established
to evaluate their relevance. Towards this end different models, tools and methods have
been proposed: for representing and evaluating research hypotheses [13,14], contextua-
lization [15], models of discourse [16], of argument [17], extended means for annota-
tion [18,19], or specific container formats [20]. There is, however, currently no
dedicated model supporting a coherent, extensible and semantic-web compatible repre-
sentation of all those aspects routinely considered by a researcher inspecting the
evidence for a given scientific finding, i.e. a representation of (i) the experimental and
computational methods and settings that were used to establish the observational
results and process the data, (ii) the reasoning including additional findings and
assumptions used to infer the result in question, and (iii) information sources and
agents through which the corresponding views were communicated and propagated.
Here we introduce SEE (Semantic EvidencE), an RDF/OWL based approach for pro-
viding detailed, extensible and computationally accessible accounts of evidence even in
complex settings. SEE is designed to enable the fabric of observations, methods,
assumptions, and inferences examined by researchers to evaluate the evidence for a
claim to be formally represented along with their sources using semantic web techni-
ques. Evidence is captured in terms of the argumentative structure of the supporting
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 2 of 22
background for a claim i.e., by a coherent representation of claims, of the entities the
claims are about, of the argumentative relations between the claims and of claim pro-
venance. SEE accommodates nested layers of interpretation and attribution and different
evaluations based on the same data. We demonstrate its application in a case study that is
typical for the task of collecting, representing and evaluating evidence for systems biology
approaches such as genome-scale metabolic network reconstruction by providing an
expressive account of evidence for the location of the enzyme glutamine synthetase.
Results
Overview of the SEE approach
The SEE approach for representing evidence consists of providing (i) a formal representa-
tion of scientific claims, their provenance and the argumentative structure used to justify
them by other claims, (ii) a formal representation of claim content and (iii) a coherent
integration of the two. SEE relies on an abstract model for the representation of claims,
provenance and argumentative structure specified in the Reasoning and Discourse
Ontology (RDO), a lightweight OWL vocabulary developed for this purpose. Claim con-
tent e.g., what is claimed regarding the properties of biological entities or the results and
methods of an investigation is represented in RDF graphs by using appropriately defined
semantic web resources and design patterns which as a best practice should, if possible, be
re-used from existing domain ontologies. The connection between claims as representa-
tional primitives and their content relies on named RDF graphs [21] which enable pointing
to collections of RDF-triples or OWL-axioms serialized as such.
After outlining general requirements and design principles for representation of evi-
dence we describe the RDO. We then demonstrate the application and design patterns of
the SEE approach in a case study generating an expressive representation of evidence
reported in the literature for the location of the enzyme glutamine synthetase.
Deriving design principles and requirements for representation of evidence
We posit two design principles for the representation of evidence and explain their
rationale in the following:
DP1: Representation of evidence amounts to representation of claims and argumentative
structure.
DP2: Evidence relations in the sense of “A is evidence for B” obtain between the
things being claimed.
Accounts of evidence are directed towards the justification of scientific claims. The SEE
approach is based on the notion that scientific claims put forward possible, more or less
likely scenarios and outcomes - states of affairs [22] - as being accurate descriptions of a
subject of scientific inquiry. Something is evidence for a certain state of affairs, if and only
if it gives reason to believe that this state of affairs in fact obtains [23]. A pairing of evi-
dence and what it is claimed to be evidence for therefore corresponds to the set of pre-
mises and the conclusion of an argument in which the truth of the premises alleges to
give reason to believe the conclusion is true. Therefore the evidence used by authors or
agents to justify a claim, possibly using further unstated background assumptions, can be
mirrored by an argumentative structure having the claim as its conclusion. Typically, what
is used to justify the authors conclusions within this argumentative structure are claims in
themselves accepted as true on the basis of observations or inferences of the same or of
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 3 of 22
other investigators. SEE, therefore, models evidence relations in the sense of “A is evidence
for B” specifically as relations between claims.
We derive two additional requirements:
DP3: A researcher’s assessment of the evidence for a finding usually includes evaluation
of which materials and methods were used, what kind of data was obtained and which
properties were observed, inferred or assumed to establish the finding. Consequently, a
representation of the materials, methods, data items and other elements forming the
subject of a claim should be part of a computationally accessible evidence representation.
In RDF and OWL the subject of a claim, a state of affairs, must be expressed, using appro-
priately defined resources, as (one or more) triples and axioms, respectively. It follows
then, in accordance with DP2 that in an RDF/OWL-based representation of evidence that
includes claim subjects the representation of evidential relationships should operate
between claim subject representations, i.e. between sets of RDF-triples and/or
OWL-axioms.
DP4: Representation of claims and hence representation of evidence must take into
account claim provenance, in particular through which source and by which agents the
claims were made. Knowing which agent made the claim is crucial for evaluating inde-
pendence and reproducibility. Tracking the original source of a claim provides a natural
reference point for all subsequent representations of the claim and its supporting back-
ground and for re-evaluation of the claim within the original context in which it was
communicated.
We therefore identify as minimal components for modelling evidence elements repre-
senting (i) scientific claims and the argumentative structure used to justify them by other
claims, (ii) the subjects of the claims i.e. that what is claimed with regard to a subject of
inquiry, (iii) the agents making the claims and arguments, (iv) the sources in which claims
were originally made e.g., the original scientific articles or database records.
Reasoning and Discourse Ontology (RDO)
Based on the foregoing we developed an abstract model for representation of evidence in
terms of claims, their argumentative structure and their provenance. It is specified here as
the Reasoning and Discourse Ontology (RDO) using the Web Ontology Language (OWL).
This section outlines the core classes and properties of RDO. Full, formal specification of
all RDO constructs is provided in the ontology file provided as additional file 1.
The typical scenario that underlies the constructs defined in RDO is the following:
Agents (e.g., individual scientists) make claims on particular occasions (e.g., as authors
of a published scientific article) about a subject of inquiry. The subject of the claim - i.
e. what is claimed - is communicated in some linguistic form, often as part of a more
comprehensive report (e.g., a scientific article) authored by the agents. Claims are
usually justified by other claims the subject of which has been accepted as true, usually
on the basis of yet other claims. RDO (Figure 1) rests on the distinction of a claim, its
subject and the linguistic form in which this subject is communicated and is centered
around the concept of an assertion [24]: instances of the class assertion (courier
typeface denotes OWL classes, courier in italics denotes OWL properties)
represent particular claims made by particular agents on a particular occasion that a
particular proposition, the subject of the claim, is true. Propositions, in our model, are
represented by the class proposition and taken to represent the semantic content
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 4 of 22
of contextualized lexical entities formulated in some natural or artificial language [25].
The lexical entities by which the subject of a claim and propositions and reports in
general are formulated are represented using the class text. Further core classes are
report representing accounts intended to accurately describe an event or situation.
Thus, scientific journal articles or database records as typical sources of assertions are
examples of a rdo:report. Agent is used to represent individual persons, corporate
bodies or information processing devices as roleplayers in the creation of reports or
assertions. RDO specifies various properties to represent the relations between
instances of these classes (Figure 1). In particular, argumentative structure is captured
by the property is inferred from which relates an instance of assertion to
another if and only if the former is, directly or indirectly, inferred from the latter (and
possibly other premises).
Application: representation and evaluation of evidence for a source of glutamine
synthetase
Introducing the case study
We applied SEE to generate a computationally accessible, expressive and extensible
account of evidence gathered in the literature regarding a claimed source of the enzyme
glutamine synthetase (GS). We have chosen this particular test case because obtaining reli-
able information on location of enzyme activities is a subject area of particular importance
for systems biology approaches such as the reconstruction of cell-type specific [26] or
organism-level [27] metabolic networks. Furthermore, it embodies the typical task of
acquiring knowledge on a subject of inquiry by extracting and combining evidence from
different sources.
Figure 1 Core classes and properties in RDO. Boxes denote labelled classes, arrows denote labelled
properties, direction of arrows denotes property domains and ranges. Asterisk: property has domain and
range-specific sub-properties. The color code used here is also used in subsequent figures.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 5 of 22
Starting point is our evaluation of a scientific journal article [28] (referred to as ‘Meister
1985’ in the following) authored by Alton Meister which asserts in the second paragraph
of the text, among other things, that the enzyme glutamine synthetase (GS) was isolated
from rat liver. This assertion is based, by way of citation, on the contents of another article
by Tate, Leu and Meister [29] (referred to as ‘Tate 1972’ in the following). In Tate 1972
the isolation of GS from rat liver is reported. The finding is reported to be based on an
investigation which involved, among other things, extraction of rat livers, protein purifica-
tion and g-glutamyl hydroxamate synthesis (g-GHS) assays. In the following we show how
this context is formalized using the SEE approach to yield a detailed formal account of the
evidence presented through these articles for rat liver as source of GS. In doing so, we
illustrate various design patterns used in SEE for representing the relevant items. For
clarity assertion instances will be indexed as A1, A2, and so forth.
Representing the evidence
Figure 2 shows how the assertion from Meister 1985 that GS was isolated from rat
liver is represented using RDO, exemplifying the design pattern used to represent the
relations between a particular assertion and its subject and provenance: The article
Figure 2 Representation of assertions with subject and provenance. Assertion instances are
related to proposition instances representing the subject of the assertion by asserts, to the agents
making the assertion by is_assertion_made_by and to the reports in which they are made by
is_assertion_made_in. See text for additional relations among assertions, agents, reports and textual
representations. Assertion and proposition labels reflect the graph representation of assertion subjects (see
text). Color code of assertion and proposition labels indicates the structured representation of assertion
subjects (yellow: class, blue italics: property, red: Manchester syntax restriction keyword). Circle shows the
index by which the assertion is referred to in the text.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 6 of 22
itself, Meister 1985, is classified as instance of report annotated with a uniform
resource locator (URL) providing its digital representation. The second paragraph of
Meister 1985 constitutes a report_part. It is expressed as the English language text
as which it is written and which is represented as an instance of text. The original
text is linked to it via the data property has_lexical_structure. Meister’s claim
that glutamine synthetase was isolated from rat liver contained in this paragraph is
represented by an instance of assertion (A1) labelled as ‘! some GS-enzyme isolated
from some rat liver ! AM’ to indicate the assertion subject in a concise, human read-
able manner (formalization of assertion subjects is described below). A1 is related to a
corresponding instance of proposition identifying the subject of the claim, to an
instance of agent representing Alton Meister, and to said report part by the prop-
erties asserts, is_assertion_made_by and is_assertion_made_in,
respectively.
Claims which reiterate previous findings are represented as assertions on the same sub-
ject made by the respective agents. Formally, the reiterating claim is represented as an
assertion instance which is linked to the source assertions by is_directly_in-
ferred_from and linked to the same proposition instance as the source assertions
by asserts. Each assertion can be linked to its corresponding agents and reports. Appli-
cation of this design pattern to our case study is shown in Figure 3: The fact that Meisters
assertion (A1) reiterates what Tate & co-workers have asserted on the isolation of GS
from rat liver (A2), is represented by a relation of the former to the latter via is_direc-
tly_inferred_from and by sharing the same proposition instance via asserts.
The argumentative structure within and across the publications is represented as a
series of assertion instances and is_directly_inferred_from relations with
additional links to represent assertion subjects and provenance (Figure 4). The
assertion instances linked to A2 reflect the results and the reasoning of the authors
at various steps of their investigation based on a careful analysis of the internal argu-
mentative structure of Tate 1972. Specifically, Tate et al.’s main conclusion that GS-
enzyme was isolated from rat liver (A2) is essentially based on asserting that (A3)
there is a biological sample (labelled ‘sample-1’) which has GS-activity, that (A4) any
GS-activity is borne by some GS-enzyme and that (A5) sample-1 was isolated from
some rat liver (precise definitions for GS-enzyme, GS-activity in the context of the
case study are detailed in additional file 2). The joint use of A3, A4 and A5 to infer A2
Figure 3 Representation of claims which reiterate previous findings. The fact that Meisters assertion
(A1) reiterates what Tate & co-workers have asserted on the isolation of GS from rat liver (A2), is
represented by a relation of the former to the latter via is_directly_inferred_from and by
sharing the same proposition instance via asserts. Each assertion instance is linked to its
corresponding agents and reports. Color code as in figure 1.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 7 of 22
is made explicit by using the has_conjunctive_part property to link them to the
same composite assertion instance which in turn is related to A2 using the
is_directly_inferred_from property. This pattern is used whenever an asser-
tion is inferred from more than one premise. A3, the assertion that sample-1 has GS-
activity is justified in turn by asserting that (A6) it was input to a particular assay
(labelled assay-1), that (A7) this assay produced a particular result, data item 1, and
that (A8) this data item is a measurement of some GS-activity. A8, in turn, is justified
by asserting that (A9) the data item is output of assay-1, that (A10) this assay was a g-
GHS assay, and that (A11 & A12) this type of assay is suited to measure GS-activity.
Some assertions are not further justified, either because they reflect factual descriptions
in Tate 1972 (A9, A10), represent general assumptions of the authors (A11) or are
expressions of terminological domain knowledge (A12, A4). A5 exhibits a similar justi-
fication trail, as shown in Figure 4. Full, formal representation of the argumentative
structure for the test case is provided in additional file 2.
The prevalent pattern in SEE for recording individual and logically relevant steps of an
investigation is for any such step to link its outcomes (data or material), the techniques
used to produce these outcomes, and their objectives as exemplified in the composite
assertions comprising assertions A9-A12 and A15-A20 (Figure 4). In A9-A12, for example,
the experimental process type (g-GHS assay) is linked to the objective of its application
(GS-activity measurement) and in turn to the quality that is intended to be determined
(GS-activity). Generally, the relations between these ontologically different entities are not
Figure 4 Representation of argumentative structure within and across publications. The
argumentative structure used to justify Meister’s claim on the isolation of GS from rat liver (A1) is
represented as a series of assertion instances linked by is_directly inferred_from relations.
Dashed line separates assertions made in Meister 1985 and Tate 1972. Dashed-dot boxes indicate composite
assertions with their component assertions placed inside signifying the has_conjunctive_part
relations. Author initials tags of the assertions made by Tate et al. are omitted, as are links to propositions,
reports, authors and texts. Color code as in figure 1.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 8 of 22
trivial and not one-to-one (one objective can consist of the determination of several quali-
ties recognized in a scientific domain, a certain quality can be the subject of inquiry in sev-
eral objectives). However, in this particular case the objective and quality are narrowly
defined and directly correlated.
Representation of assertion subjects
The representation of argumentative structure and claim provenance as an interrelated set
of assertion instances described so far is complemented by a structured representation
of what is asserted in each assertion, the assertion subject. To this end each assertion
instance is linked to a corresponding proposition instance the IRI (Internationalized
Resource Identifier) of which identifies a named RDF graph. This graph provides a struc-
tured representation of the assertion subject using appropriately defined resources (Figure
5). This setup enables querying the elements forming the assertion subject. In assertion
A10, shown in Figure 5 as an example, it is asserted by Tate and co-workers that the parti-
cular assay they performed was a g-GHS assay. The representation of this statement as a
graph identified by the IRI of the proposition instance linked to the assertion
instance representing A10 enables to access the entities A10 is about: the particular assay,
its asserted type, and the typing relation itself. Full specification of all propositions as
named graphs in the context of the case study is provided in additional file 3.
Figure 5 Representation of assertion subjects as named graphs. In SEE structured, queryable
representations of assertion subjects are provided as named graphs. A) The structured representation of the
subject of an assertion can be accessed as the RDF graph identified by the IRI of the proposition instance
related by the asserts property to the assertion instance representing the assertion. B) Structured
representation of the subject of assertion A10 asserting that the particular assay performed by Tate et al.
(:assay-1) was a g-GHS assay (:gamma_ghs_assay). C) TriG representation of the graph shown in B.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 9 of 22
To generate the graph representations of the assertion subjects, the natural language
expressions of the assertions identified in the Meister 1985 and Tate 1972 reports were for-
malized in RDF using appropriately defined resources (see additional files 2, 3 and 4). Most
assertion subjects could be formalized in a straightforward manner applying OWL 2 RDF-
based semantics [30]. The principal claim that “glutamine synthetase was isolated from rat
liver” which is the common subject of assertions A1 and A2 was formalized in RDF by
instantiating the class gs_enzyme and is_isolated_from some rat_liver (shown
as :proposition-1 in additional file 3). This exemplifies instantiation of the OWL-class
(A and related_to some B) as a design pattern for formalization of statements which
can, in natural language, be represented in the form “some A related to some B” (A and B
denoting OWL-classes used to represent the types A and B, respectively and related_to
denoting an OWL-property used to represent the relation among some of their instances).
Labels of assertion and corresponding proposition instances are directly
derived from the graph representation of the assertion subject (see methods section).
In particular, the label “some A related_to some B“ is used for proposition
instances that represent statements of the form “some A related to some B” by applying
the design pattern described above.
Representing consecutive layers of interpretation and own conclusions
We use the test case to specify additional design patterns to represent activity of a
curator or generally of a third party evaluating a scientific report. Our representation
of the evidence in the Meister 1985 and Tate 1972 reports is the result of the interpre-
tation by another agent (Christian Bölling - CB). This can be explicitly represented in
SEE using its familiar design pattern for propositions and assertions. For example, the
claim that Tate et al. indeed assert that the assay they performed was a g-GHS assay in
their 1972 publication can be represented as an assertion instance in its own right,
made by another agent, CB (Figure 6). This pattern allows for representing arbitrary
many consecutive layers of interpretation or attribution.
So far the presented account consists of assertions attributed to the authors of the
Meister 1985 and Tate 1972 reports, i.e. a representation of what these authors assert.
SEE also provides the resources to append own conclusions. For example, an agent, CB,
could upon evaluation of the claims made by Tate et al. conclude for himself that GS
was indeed isolated from rat liver. This is represented as an assertion instance in its
own right (A30, labelled ‘! some GS-enzyme isolated from some rat liver ! CB’). It is
linked to the corresponding proposition via asserts and the assertions made by Tate
et al. via is_directly_inferred_from. We describe two semantically different pat-
terns to make this connection. In pattern 1 assertion A30 is linked to assertion A2
(Figure 7). In pattern 2 (Figure 8) A30 is linked to a new composite assertion that
involves two more curator assertions (A31, A32) and A4 as a representation of termino-
logical domain knowledge. A31 and A32 are linked by is_directly_inferred_-
from to composite assertions reflecting factual descriptions of data and procedures
given in Tate 1972. There is a subtle, yet important difference in meaning between these
two representations. In pattern 1 CB’s conclusion is based on Tate et al.’s assertion on
the same subject, i.e., it is based on the author statement itself and does not necessarily
imply an affirmation of how Tate et al. reached their conclusion. In pattern 2 the curator
inference is based on factual descriptions in Tate 1972, i.e., it affirms the conclusions of
Tate et al. as own conclusions on the basis of the reported experimental results.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 10 of 22
Evaluation of a given set of data might also lead to conclusions different from those
of the authors. Such alternative interpretations can be represented using SEE. For
example, one might dispute that g-GHS assays are suited to measure GS-activity (EC
6.3.1.2). The g-GHS assay works by measuring the formation of L-g-glutamyl hydro-
xamate rather than glutamine [31]. Tate et al. assert as the objective of its application
GS-activity measurement, accepting the formation of the hydroxamate under the
conditions of the assay as a proxy for the formation of glutamine and the actual reac-
tion mechanism. Assertion A11 using the property achieves_objective reflects
this acceptance by Tate et al.. Alternatively, a third party could assert that g-GHS
assays merely achieve the less specific objective of measuring g-glutamyl transferase
(GGT) activity (EC 2.3.2.2) (Figure 9, assertion A45). In this case the data reported
by Tate et al. can still be used to infer that rat liver is a source of GGT-enzyme
(Figure 9, assertion A40).
Figure 6 Representation of consecutive layers of interpretation. Consecutive layers of interpretation
can be represented as assertions the subject of which is about other assertions. A) CB’s assertion that ‘Tate
and co-workers assert that assay-1 was a gamma-GHS assay in their 1972 report’ is represented as an
assertion instance linked to a proposition instance whose named graph representation relates the
assertion instance A10 to the Tate 1972 report via the is_assertion_made_in property. B) TriG
representation of the graph shown in A. In combination with the RDF dataset shown in Figure 5C this is
an example of a named graph referencing a named graph via the corresponding assertion and
proposition instances. Color code as in figure 1.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 11 of 22
Evaluating the test case evidence representation
The test case evidence representation that was created using the RDO constructs and
design patterns was evaluated in terms of its potential to answer, within the confines of
the case study, a list of competency questions reflecting different aspects of the evidence
a researcher investigating glutamine synthetase knowledge would be interested in:
Q1: Which locations of GS have been asserted?
Rat liver.
Q2: Where has rat liver GS been reported?
The Meister 1985 and Tate 1972 reports.
Q3: Do the assertions made in these reports pertain to independent observations?
No. Meister’s assertion is based on Tate et al.’s assertion. Moreover, some of the
authors of the two reports are identical.
Q4: Is there experimental evidence and where is it described?
Yes. In the Tate 1972 report.
Figure 7 Representation of curator activity: inference from author statement . The pattern to
represent inference from author statement is illustrated here by linking curator assertion A30 (shown in
bold) to assertion A2 of the Tate et al. argumentation signifying inference of A30 on the basis of an author
statement of Tate et al. on the same subject. Color code as in Figure 1.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 12 of 22
Q5: Which observations and techniques were used for establishing rat liver as GS source?
1. extraction of a protein sample from rat liver (technique: TLM purification)
2. that sample has GS-activity (technique: g-GHS assay)
Q6: Did Tate et al. really make these observations and conclusions? Who created this
account of their findings?
Christian Bölling.
Based on the SEE design patterns, these questions could be formulated as SPARQL
[32] queries and successfully answered (see additional file 5). In each of Q1-Q6 the
structured representation of assertion subjects as named graphs, besides the other SEE
design patterns, is used to identify assertions which are relevant to answer the query.
For answering Q1 assertions are identified whose subject’s graph representation
includes a graph pattern indicative for the isolation of GS from some location (Figure
10A). For answering Q3, pairs of assertions are identified whose subjects share the
same graph representation and where one is inferred from the other (Figure 10B).
Figure 8 Representation of curator activity: inference from experimental evidence. The pattern to
represent inference from evaluation of the reported experimental evidence, in contrast to inference based
on author statement (Figure 7), is illustrated here by linking curator assertion A30 to a new composite
assertion involving curator assertions A31 and A32. These are, in turn, linked to the composite assertions
A9-A12 and A15-A20, respectively. These composite assertions reflect data and procedures reported in Tate
1972. Taken together this graph therefore represents a curator conclusion (A30) based on the affirmative
outcome (A31, A32) of the evaluation of the data and procedures reported in Tate 1972. Note that A2, the
principal conclusion of Tate et al. is unrelated to the new composite assertion. Curator assertions and their
links to the Tate et al. argumentation are shown in bold. Color code as in Figure 1.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 13 of 22
The following evidence-related information can be queried exploiting property chains
and other axioms defined for the RDO constructs:
- all assertions which are directly or indirectly used to infer a given assertion
- all assertions made in a given report
- all assertions made by a given agent
- all assertions on the same subject
- all agents making assertions on a given subject
For the corresponding queries see additional file 5. As an example, in Figure 11 the
object property assertions inferred for assertion A1, Meister’s assertion that GS was
isolated from rat liver, are shown. These inferences, simply derived in Protégé 4 with
HermiT 1.3.8 as a reasoner include all assertions which A1 is directly or indirectly
inferred from and all reports and texts A1 is based on.
Discussion
SEE design
SEE offers a tangible interpretation of the concept of evidence in terms of the argu-
mentative structure of the supporting background for a claim. It rests on the
Figure 9 Representation of curator activity: alternative interpretations of reported data. Alternative
interpretations of reported data can be represented as assertions that are made by a third party and linked
to assertions reflecting factual descriptions of the reported data. Based on CB’s assertion that g-GHS assays
measure GGT activity (A45) - rather than GS-activity - it is inferred that GGT-enzyme has been isolated from
rat liver (A40). The corresponding inference chain relies on a number of new curator assertions (A41-A46)
and their combination into composite assertions but re-uses assertions on the quantitative data obtained
and the procedures conducted by Tate et al. Curator assertions and new inference links are shown in bold.
Color code as in Figure 1.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 14 of 22
Figure 10 Competency questions SPARQL queries. A) SPARQL query to identify all asserted locations of
GS (Q1). This query identifies patterns in which an assertion (_:q11) has a subject (_:q12) which includes a
graph pattern indicative for the isolation of GS from some location. B) SPARQL query to identify
dependency of assertions on the same subject. The query identifies assertions (_:q31, _:q33) which share
the same subject (?proposition) and are inferred from one another.
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 15 of 22
disctinction between claims as such (assertion), their subjects (proposition) and the
linguistic form in which these subjects are communicated (text). As a consequence of
this design evidential relations (as in “A is evidence for B”) can be represented consis-
tently as relations between assertions. This means that statements of the form “this
dataset / experiment / publication / method is evidence for B” are regarded as figura-
tive expressions. Instead, the relation between a dataset, an experiment or a publication
and the state of affairs it is claimed to be evidence for is represented indirectly through
relations between assertions the subjects of which relate to the entities in question.
The advantage of this design is that it enables a coherent representation not only of
extensive argumentative networks but also of arbitrary many layers of consecutive
Figure 11 Inferred object property assertions. Inferred object property assertions for Meister’s assertion
that GS was isolated from rat liver (A1).
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 16 of 22
interpretations and alternative evaluations of the same observations or information
sources. RDO offers clear, formally defined types and relations for representing claims,
their subjects, their linguistic representations, related information sources and agents
on the basis of well established concepts from epistemology and the philosophy of lan-
guage [22,24,25,33,34]. The case study examples suggest that the SEE design principles
and their implementation in RDO are capable of correctly representing, in a computa-
tionally accessible and coherent form, the entire ‘evidence trail’ for a claim needed to
evaluate its relevance including observational data, research techniques, assumptions
and information sources.
SEE represents argumentative structure at its foundational level of premises being
used to infer a conclusion using the is_directly_inferred_from property and
its transitive superproperty is_inferred_from. This allows for a coherent repre-
sentation of different argument forms and larger rhetorical structures which can be
mapped onto their underlying assertions.
SEE aims to capture arguments as they are presented in their sources rather than to
evaluate their quality or to categorize them. How conclusive an argument is will typi-
cally depend on agent background knowledge or application-dependent requirements.
The SEE design enables users to evaluate evidence according to their own, possibly
domain- and application-specific criteria.
SEE-based accounts could also be used alongside specified rules, or argument forms
considered as acceptable by individual researchers or within specific domains of
inquiry which could then be leveraged to automatically infer new assertions on the
basis of the already asserted information.
With regard to the extraction of assertion subjects and a specific argumentative
structure from a natural language text SEE relies in its current form on a heuristic
approach leveraging expert domain knowledge to identify assertions and formalize
them in OWL. As OWL is a subset of first order logic there may be statements from
natural or artificial languages which cannot directly be translated into OWL, constrain-
ing the formalization of assertion subjects in SEE. It is, however, not clear which actual
limitations arise from this theoretical constraint for the representation of evidence in
specific use cases. The test case presented here suggests that within a specified domain
of discourse, using appropriate constructs and design patterns, the relevant contents of
the statements made originally in a context-rich narrative format such as a scientific
journal article can be adequately formalized.
Formalization of natural language statements is an important prerequisite for compu-
tational approaches to data evaluation. For applications that can forego this need the
statements can be represented in their original form as texts or referenced by links to
the original information sources. Both are by default designed to be provided in SEE as
reference points for evaluation.
The presented design patterns make SEE-based accounts of evidence extensible. This
design is in line with the open world assumption on which RDF and OWL as knowledge
representation languages operate. The particular argumentative structure and level of
detail presented in the case study are based on heuristics reflecting domain-specific
requirements to understand how an enzyme was characterised. This representation can be
extended or shortened as required. For example, details on the protein purification process
performed by Tate et al. or indeed any other detail that becomes relevant for the
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 17 of 22
evaluation of the presented evidence could be appended to the existing assertions. Like-
wise, as we have demonstrated, alternative views and conclusions can be accommodated.
On the other hand, for applications which only require information on claim provenance,
only the source publications of the main claims could be represented.
Evidence types
Evidence type schemes provide a useful shorthand categorization of research techni-
ques used to establish a claim. SEE could be aligned with any categorization of
research techniques and hence evidence type scheme to characterise the evidence for
an assertion. Essentially, SEE provides a platform to define custom, extensible evidence
types and apply them as needed. For example, the evidence for rat liver as a source of
GS in the test case could be characterised as “experimental evidence” as “based on a
direct assay” as “based on a g-GHS-assay” or as “based on a g-GHS assay, protein puri-
fication involving Sephadex chromatography, and samples from Sprague-Dawley rats”
depending on the level of accuracy desired.
The flexibility and extensibility of the SEE approach may also be useful to character-
ise evidence where several techniques have been combined to establish a scientific
result or evidence is characterised in combination with claim provenance. We illustrate
this with a comparison to the Gene Ontology (GO) evidence codes which are meant to
reflect the type of work or analysis described in the cited reference which supports the
GO term to gene product association [35]. GO evidence codes consist of a collection
of terms arranged in a hierarchical format. In this taxonomy the terms representing
justifications based on author statements (TAS, NAS) are unrelated to those representing
experimental techniques (EXP and child terms). Consequently, GO associations marked
as being made on the basis of an author statement are usually not qualified with respect
to how this author statement came about. In contrast, as demonstrated in the case
study, using SEE any author statement can be extensively qualified in terms of the
experimental evidence or other author statements it is directly or indirectly based on.
Use cases
Representations which use SEE or its underlying model could be productive in a variety
of use cases requiring careful examination or recording of evidence, e.g.,
- providing supporting background information for biomedical knowledge bases,
- creating digital abstracts of research publications,
- adding a claim-level perspective on research publications which could be used by
publishers, in bibliographic databases and in personal bibliography managers,
- providing open linked data which can be integrated on an informed basis using
varying, application specific evidence criteria.
Related and future work
The SWAN biomedical discourse ontology [16] developed in the context of the
Semantic Web Applications in Neuromedicine (SWAN) project offers a formal model
of scientific discourse based on two different classes of statements; swan:hypothesis
and swan:claim. Claim subjects are to be represented in natural language and the reso-
lution of their supporting background is confined to the document level. The
Bölling et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S1
http://www.jbiomedsem.com/content/5/S1/S1
Page 18 of 22
Annotation Ontology (AO) [18] has been implied as a means to provide formalized
accounts of claims and their supporting background conceptualized as annotations and
document parts, respectively. While it is possible in this way to relate individual ontol-
ogy terms to parts of documents, the AO semantics and use cases suggest that its
main application area is representation and support of annotations of documents
rather than representation and evaluation of extensive, possibly nested, networks of
claims. Nanopublications have been proposed as a container format to encode and
publish individual assertions using Semantic Web and Linked Data principles [36].
PROCEEDINGS Open Access
Semantic Web repositories for genomics data
using the eXframe platform
Emily Merrill1*, Stéphane Corlosquet1*, Paolo Ciccarese1,2, Tim Clark1,2,3, Sudeshna Das1,2*
From Bio-Ontologies Special Interest Group 2013
Berlin, Germany. 20 July 2013
* Correspondence:
mmerrill@partners.org;
scorlosquet@gmail.com;
sdas5@partners.org
1Massachusetts General Hospital,
Partners Research Building, 65
Landsdowne St, Cambridge, MA,
02139, USA
Abstract
Background: With the advent of inexpensive assay technologies, there has been an
unprecedented growth in genomics data as well as the number of databases in
which it is stored. In these databases, sample annotation using ontologies and
controlled vocabularies is becoming more common. However, the annotation is
rarely available as Linked Data, in a machine-readable format, or for standardized
queries using SPARQL. This makes large-scale reuse, or integration with other
knowledge bases very difficult.
Methods: To address this challenge, we have developed the second generation of our
eXframe platform, a reusable framework for creating online repositories of genomics
experiments. This second generation model now publishes Semantic Web data. To
accomplish this, we created an experiment model that covers provenance, citations,
external links, assays, biomaterials used in the experiment, and the data collected during
the process. The elements of our model are mapped to classes and properties from
various established biomedical ontologies. Resource Description Framework (RDF) data
is automatically produced using these mappings and indexed in an RDF store with a
built-in Sparql Protocol and RDF Query Language (SPARQL) endpoint.
Conclusions: Using the open-source eXframe software, institutions and laboratories
can create Semantic Web repositories of their experiments, integrate it with
heterogeneous resources and make it interoperable with the vast Semantic Web of
biomedical knowledge.
Background
There has been a rapid cost reduction per megabase of genomic information obtained,
beating Moore’s law [1] many-fold [2,3], resulting in an exponential growth of geno-
mics data, especially next generation sequencing data [4]. Standards to unambiguously
describe the experimental details are required to facilitate the understanding, quality
checking, reusing, reproducing and integrating the data. The bioinformatics community
has responded to the challenge and several standards have been developed over the
years. The first standard to be published provided requirements for the Minimum
Information About a Microarray Experiment (MIAME) [5]. Several other standards
were published as new technologies evolved and then the Minimum Information for
Biological and Biomedical Investigations guideline was proposed for reporting all types
Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3
http://www.jbiomedsem.com/content/5/S1/S3 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2014 Merrill et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in
any medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver (http://
creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
of biomedical experiments [6]. The major public repositories of genomics experiments,
Gene Expression Omnibus (GEO) [7] and ArrayExpress [8], are compliant with these
standards.
While standards addressed the need for uniform experiment representation, controlled
vocabularies, terminologies and ontologies were developed to describe the samples, assays
and other experimental details in an unambiguous manner. For example, the Ontology for
Biomedical Investigations (OBI) [9] provides a model for biomedical experiments with
classes that describe elements of the experimental investigation process. The Experimental
Factor Ontology (EFO) [10] was developed as an application ontology to describe the
genomics data in ArrayExpress [8]. In addition several ontologies and vocabularies have
also been developed to describe biological specimens such as the organism, tissue, cell
type, disease state. These include the Cell Ontology (CL) [11], the Foundation Model of
Anatomy (FMA) [12], Disease Ontology (DO) [13] among numerous others.
Several repositories of genomics data have adopted the MIAME or MIBBI standards and
are leveraging these biomedical ontologies to provide consistent annotation of experi-
ments. A few examples from diverse domains include the Gemma repository - a resource
for sharing, reuse and meta-analysis of microarray data [14], Chemical Effects in Biological
Systems (CEBS) database that contains data of interest to environmental health scientists
[15] and Oncomine an integrated database and mining platform for oncology data mine
[16]. Although these resources make use of ontologies to represent experimental data in a
standardized manner, the annotations are not machine-readable by other software and
thus integration with other knowledge resources remain a challenge.
Meanwhile, Semantic Web [17] technologies such as Linked Data, Resource Description
Framework (RDF) and SPARQL are increasingly being used in the bioinformatics commu-
nity to respond to the knowledge integration needs [18]. Semantic Web allows one to
query across disparate resources using a single flexible interface. For example, the Bio2RDF
project successfully applies Semantic Web technologies to create a mashup of key publicly
available databases using a common ontology and normalized Uniform Resource Identifiers
(URI) [19,20]. Cheung et al. demonstrate the use of Semantic Web technologies for a
federated query in the neuroscience domain [21]. There are several other examples across
various biomedical domains that demonstrate the power of Semantic Web technologies.
However, surprisingly there has been no wide spread adoption of Semantic Web
technologies for experiment repositories, where queries using domain ontologies can
help bridge different disciplines, for important applications such as translational
medicine. Recently the European Bioinformatics Institute (EBI), recognizing this urgent
need, has released an RDF platform that includes a SPARQL endpoint for the Gene
Expression Atlas [22], a database that summarizes gene expression from ArrayExpress
experiments[23]. However, it doesn’t provide reusable software that can be used by
other institutions to house and query their genomics data.
To address this gap, we developed eXframe as a reusable software platform to build
genomics repositories that automatically produce Linked Data and a SPARQL
endpoint. Our platform is based on an open source content management system and
uses existing biomedical ontologies to produce Semantic Web data enabling intero-
perability with the other resources. The code is freely available and application is
demonstrated with a repository of stem cell data.
Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3
http://www.jbiomedsem.com/content/5/S1/S3
Page 2 of 12
Implementation
In this section we describe the implementation of eXframe and how it automatically
generates Linked Data.
Framework
The eXframe software framework [24] enables creation of web-based genomics experi-
ment repositories. It is based on an open source content management system, Drupal
[25], with modifications to support genomic experiment data. In this paper, we report a
re-factored second generation of eXframe, which produces Linked Data and a SPARQL
endpoint for querying it. The revised version also includes an updated experiment
model that has been generalized to support various types of biomedical experiments as
well as an upgrade to Drupal 7.
We have defined content types (e.g. experiments, assays, biomaterials and bibliographic
citations) as well as their relationships as first class objects in Drupal. These predefined
content types are packaged as Drupal features and available for use within eXframe. All
content types and their fields are mapped to appropriate ontologies and vocabularies as
described in the following section. Using these mappings, the Drupal RDF modules [26]
are used to produce RDF as well as a SPARQL endpoint. Data can also be exported in
other standard formats such as ISA-Tab [27]. A simple schematic of the architecture is
shown in Figure 1. The software also includes a basic theme (colors, fonts and style) for
the website. Any group or institution that uses eXframe can customize the content types,
theme or ontology mappings.
Data model
The main content type within eXframe is an experiment. It describes the experiment
and its meta-data including title, description, contributors, design, citations, and links
Figure 1 eXframe architecture . Overall schematic of eXframe architecture displaying the major
components.
Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3
http://www.jbiomedsem.com/content/5/S1/S3
Page 3 of 12
to external resources such as GEO [28] and ArrayExpress [8]. The experiment content type
is mapped to the OBI investigation class obo:investigation. The experiment’s “publication”
meta-data is represented using the Dublin Core ontology [29]. However, we are currently
evaluating the PAV ontology [30] as it provides more detailed and precise provenance
information. For example, the Dublin Core ontology specifies the relation dc:date; but does
not provide precise information as to whether the date is the “submitted date”, “published
date” or “last updated date”. The researchers that conducted the experiment are repre-
sented as Drupal users with a profile and mapped to foaf:Person in the FOAF ontology
[31]. While we do not specify the principal investigator (for the sake of simplicity), one
could use VIVO [32] to do so. Bibliographic citations are represented using the Drupal
biblio module and mapped to the bibliographic ontology, BIBO [33]. These classes and
mappings are illustrated in Figure 2.
The experiment class also describes the overall protocol; measurement type and
includes the experimental-factors, which can be exploited by bioinformaticians for data
analysis. Experiments are composed of assays represented by the bioassay content type.
Figure 2 Data Model. Data model outlining the relationship between the experiment, its assays and
biomaterials. The Drupal content types are indicated as green circles with the mapping listed underneath.
Arrows indicate the relationships.
Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3
http://www.jbiomedsem.com/content/5/S1/S3
Page 4 of 12
The bioassay content type is mapped to obo:bioassay and specifies the technology plat-
form used and other assay details. Bioassays are typically performed on several repli-
cates specified by the replicate content type and mapped to efo:replicate (OBI only
models replicate design and analysis). Each replicate is associated with the biological
material on which the assay is conducted and is specified by the biomaterial content
type. Thus technical replicates reference the same biomaterial, whereas biological repli-
cates reference the unique materials used for the assay. The assays have raw data as
their output. Data transformations and analyses conducted on the raw data are cur-
rently not represented, but are included in future plans for the system.
Biomaterial is deeply annotated using Drupal Taxonomies and mapped to various
controlled vocabularies and ontologies. In the eXframe default package, the organism,
tissue type, cell type, disease state and chemical treatment taxonomies are mapped to
NCBI Taxonomy (NCBITaxon) [34], FMA [12], CL [11], Disease Ontology (DO) [13]
and Chemical Entities of Biological Interest Ontology (ChEBI) [35] terms, respectively.
EFO [10], NCI Thesaurus [36] or Breda Tissue Ontology (BTO) [37] is also used to
increase coverage when required. Biomaterial properties and their mappings are config-
urable and can be easily customized to a particular domain as required. The mappings
of the main content types (experiments, bioassay, citation, biomaterials etc.) to ontolo-
gies are configured in PHP code, in a single file (an excerpt of which is shown in
Figure 3). Attributes of the experiment, bioassays, and biomaterials that can be defined
via structured vocabularies are stored as Drupal taxonomies. For example, “Cell Type”,
an attribute of the biomaterial, is represented as taxonomy. Each term in the taxonomy
is mapped to a class or classes in external ontologies. Thus, “Fibroblast” a term in the
“Cell Type” taxonomy, is easily added, edited and mapped to ontologies through the
web interface.
Linked data & SPARQL endpoint
We use the Drupal RDF modules to produce RDF using the mappings discussed above.
RDF generated using the Drupal modules [26] is indexed into an RDF store powered
by the ARC2 PHP library [38]. A SPARQL endpoint is also published by this RDF
store. The RDF indexer in Drupal is designed to be backend-agnostic and allow for
any RDF store to be plugged in. We’re using ARC2, which is sufficient for our needs,
but other stores can be used depending on the size of the dataset, or particular
SPARQL features that might be needed.
Some of the data in the repository is kept private until the researchers publish their
work. To maintain privacy, we utilize two stores: one of which solely contains the
Figure 3 Ontology mapping code. Excerpt from exframe.entity_rdf.inc showing how Drupal classes are
mapped to external ontologies.
Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3
http://www.jbiomedsem.com/content/5/S1/S3
Page 5 of 12
public data, and whose SPARQL endpoint is publicly available; the other which con-
tains the entire data and is kept secure using an API key. The secure, administrative
endpoint is used by R scripts (described in the next section) to access data for query
and analysis by members who have access authorization. The other benefit of having
decoupled stores is that we have the flexibility of optimizing the performance and scal-
ability of each store independently from the other.
R Integration
We wanted to provide programmatic access to the repository data to retrieve experi-
mental information in a manner that is independent of the Drupal database schema.
The R statistical programming language [39] and platform is a popular tool for analyz-
ing genomics data. Thus, we decided to provide support for accessing RDF data and
the SPARQL endpoint using R. The publicly available R packages to access RDF data
are not yet fully featured; for example the SPARQL package doesn’t support
DESCRIBE queries. Hence the RDF package that does support DESCRIBE statements
was used to provide information about the resources. Using the package, first the
experiment RDF is used to obtain information about the assays, and then the assays
provide information about the biomaterial (See relationships in Figure 2). The RDF
package also had problems; it is hindered by UTF8 encoding issues. The resulting R
scripts included in the eXframe package produce data structures compatible for analy-
sis with R packages such as BioConductor [40,41].
Results
Case study: Stem Cell Commons
Stem Cell Commons (SCC) is a project of the Harvard Stem Cell Institute (HSCI) to
freely share biomedical data, tools and resources within the research community [42].
Our platform, eXframe, was first implemented independently for the Blood genomics
program at HSCI, and then later extended to support all researchers at the Institute, as
the repository of Stem Cell Commons. Data from both the previously developed Blood
Genomics store and the Stem Cell Discovery Engine (SCDE) [43] was merged into the
eXframe-based SCC database.
Genomics datasets are actively curated into the database; currently the repository con-
tains over 200 datasets from 20 laboratories representing 4 organisms and 119 different cell
types and 39 tissue types. Results based on approximately half of the datasets (86) have
been published in scientific journals, and these datasets are therefore available to the public.
All bioassays and samples have been deeply annotated with ontologies. First we used
the OBI ontology [9] for the main entities (experiment, biomaterial and assays) as
described in the data model section. Dublin Core [29] and FOAF [31] were used for the
metadata and researcher respectively. The ontologies used to annotate the biomaterials
are listed in Table 1. All the Stem Cell Commons public data is available as Linked Data
as well as a SPARQL endpoint as described in the next sections.
RDF generation
RDF for the experiment, bioassay and biomaterials are automatically generated using
the Drupal RDF modules as described previously. A screenshot of actual RDF output
for an experiment curated in the Stem Cell Commons is depicted in Figure 4. It is a
Merrill et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S3
http://www.jbiomedsem.com/content/5/S1/S3
Page 6 of 12
next-generation sequencing experiment performed by a HSCI researcher and measures
DNA methylation (using bisulphite sequencing) in the leukemia cell line K562, repro-
grammed leukemia cell lines (LiPS) and the human embryonic stem cell line H1. From
Figure 4, we see how the Dublin Core ontology provides the provenance information
JOURNAL OF
BIOMEDICAL SEMANTICS
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30
http://www.jbiomedsem.com/content/5/1/30RESEARCH Open AccessA framework for assessing the consistency of
drug classes across sources
Rainer Winnenburg and Olivier Bodenreider*Abstract
Background: The objective of this study is to develop a framework for assessing the consistency of drug classes
across sources, such as MeSH and ATC. Our framework integrates and contrasts lexical and instance-based ontology
alignment techniques. Moreover, we propose metrics for assessing not only equivalence relations, but also inclusion
relations among drug classes.
Results: We identified 226 equivalence relations between MeSH and ATC classes through the lexical alignment, and
223 through the instance-based alignment, with limited overlap between the two (36). We also identified 6,257
inclusion relations. Discrepancies between lexical and instance-based alignments are illustrated and discussed.
Conclusions: Our work is the first attempt to align drug classes with sophisticated instance-based techniques, while
also distinguishing between equivalence and inclusion relations. Additionally, it is the first application of aligning
drug classes in ATC and MeSH. By providing a detailed account of similarities and differences between drug classes
across sources, our framework has the prospect of effectively supporting the creation of a mapping of drug classes
between ATC and MeSH by domain experts.
Keywords: Drug classes, MeSH, ATC, Instance-based mapping, Lexical mappingBackground
Motivation and objectives
Drug classes provide a convenient mechanism for organizing
drugs in terms of chemical structure (e.g., Sulfonamides–a
group of compounds that contain the structure SO2NH2),
function (e.g., Anti-Bacterial Agents–often referred to as an-
tibiotics), mechanism of action (e.g., Hydroxymethylglutaryl-
CoA Reductase Inhibitors–a group of drugs, also called
statins, which block an enzyme involved in the production
of cholesterol in the liver), metabolism (e.g., inhibitors of
CYP2C9–drugs that block an enzyme from the Cytochrome
P450 protein family, which is involved in the metabolism
of drugs, such as ibuprofen and fluoxetine, and whose ac-
tivity is influenced by other drugs, such as rifampicin and
fluconazole), and adverse events (e.g., drugs that induce
QT prolongation–the antimalarial drug halofantrine slows
down ventricular repolarization, which predisposes to
certain types of arrhythmias). The interested reader is
referred to [1] for more details about drug classes.* Correspondence: obodenreider@mail.nih.gov
Lister Hill National Center for Biomedical Communications, National Library
of Medicine, Bethesda, MD, USA
© 2014 Winnenburg and Bodenreider; license
of the Creative Commons Attribution License
distribution, and reproduction in any medium
Domain Dedication waiver (http://creativecom
article, unless otherwise stated.Several drug classifications have been developed for
different purposes. For example, the Anatomical Therapeutic
Chemical (ATC) classification of drugs supports pharmacoe-
pidemiology, while the Medical Subject Headings (MeSH)
is oriented towards the indexing and retrieval of the
biomedical literature [2,3]. Moreover, sources tend to
provide different lists of drug classes, and such lists
tend to be organized in different ways according to the
purpose of a given source. For example, the ATC uses
a complex classificatory principle, in which the first
subdivision is primarily anatomical (i.e., distinction
based on the target organs or anatomical systems–e.g.,
cardiovascular system drugs vs. dermatologicals), followed
by a therapeutic subdivision (i.e., therapeutic intent of the
drugs in each anatomical group–e.g., antibacterial drugs
vs. antiviral drugs), followed by a chemical subdivision
(i.e., distinction between the structural and functional
characteristics of drugs within a therapeutic subgroup–e.
g., macrolides, such as erythromycin, vs. fluoroquinolones,
such as ciprofloxacin, among the antibacterial drugs). On
the other hand, MeSH maintains two parallel classifica-
tions, one based on chemical structure (e.g., ciprofloxacin
is represented under fluoroquinolones), and one based one BioMed Central Ltd. This is an Open Access article distributed under the terms
(http://creativecommons.org/licenses/by/2.0), which permits unrestricted use,
, provided the original work is properly credited. The Creative Commons Public
mons.org/publicdomain/zero/1.0/) applies to the data made available in this
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 2 of 14
http://www.jbiomedsem.com/content/5/1/30functional characteristics, including mechanism of action,
physiologic effect and therapeutic use. (e.g., ciprofloxacin
is linked to the mechanism of action Topoisomerase II In-
hibitors and to the therapeutic use Anti-Bacterial Agents).
In contrast to ATC, MeSH does not make distinctions
based on the target anatomical location of the drug
(e.g., there are two Fluoroquinolones classes for oph-
thalmological use vs. for systemic use in ATC, but only
one Fluoroquinolones class in MeSH).
Ideally, drug classes with similar names should have
similar members and drug classes with similar members
should have similar names. In practice, however, the same
name can be used to refer to different classes. For ex-
ample, in ATC, Fluoroquinolones refers to both a set of
ophthalmological drugs (8 members) and a set of systemic
drugs (20 members), while, in MeSH, it refers to over 50
chemical compounds with similar structural properties. In
the absence of an authoritative reference for drug classes,
the task of determining when two classes are equivalent
across sources remains extremely challenging. At the same
time, the use of multiple classifications is often required in
applications. This is increasingly the case as the use of
ATC for pharmacovigilance is on the rise (e.g., [4]).
The objective of this study is to develop a framework
for assessing the consistency of drug classes across sources,
leveraging multiple ontology alignment techniques. This
framework is meant to assist experts in the curation of a
mapping between drug classes across sources. We present
two applications of this framework, one to the alignment of
drug classes between MeSH and ATC, and the other to the
integration of MeSH and ATC drug class hierarchies. To
our knowledge, this work represents the first effort to align
drug classes between MeSH and ATC using a sophisticated
instance-based alignment technique. Moreover, we propose
metrics for assessing not only equivalence relations be-
tween classes, but also inclusion relations.
Application of ontology alignment techniques to drug classes
The broad context of this study is that of ontology alignment
(or ontology matching). Various techniques have been pro-
posed for aligning concepts across ontologies, including lex-
ical techniques (based on the similarity of concept names),
structural techniques (based on the similarity of hierarchical
relations), semantic techniques (based on semantic similarity
between concepts), and instance-based techniques (based on
the similarity of the set of instances of two concepts). An
overview of ontology alignment is provided in [5]. The main
contribution of this paper is not to propose a novel tech-
nique, but rather to apply existing techniques to a novel
objective, namely aligning drug classes between MeSH
and ATC. To this end, we use lexical and instance-based
techniques, because the names of drug classes and the list
of drugs that are members of these classes are the main
two features available in these resources.Lexical techniques
Lexical techniques compare concept names across on-
tologies and are a component of most ontology align-
ment systems [5]. When synonyms are available, they
can be used to identify additional matches. Matching
techniques beyond exact match utilize edit distance or
normalization to account for minor differences between
concept names.
As part of the Unified Medical Language System (UMLS),
linguistically-motivated normalization techniques have been
developed specifically for biomedical terms [6]. UMLS
normalization abstracts away from inessential differences,
such as inflection, case and hyphen variation, as well as
word order variation. The UMLS normalization techniques
form the basis for integrating terms into the UMLS
Metathesaurus, but can be applied to terms that are not
in the UMLS. For example, the ATC class Thiouracils
(H03BA) and the MeSH class Thiouracil (D013889) match
after normalization (ignoring singular/plural differences).
Lexical techniques typically compare the names of con-
cepts across two ontologies as provided by these ontologies.
However, additional synonyms can be used, for example,
synonyms from the UMLS Metathesaurus. In other words,
we leverage cosynonymy similarity for matching drug clas-
ses. In this case, although the ATC class Anticholinesterases
(N06DA) and the MeSH class Cholinesterase Inhibitors
(D002800) do not match lexically, both names are cosyno-
nyms, because they are found among the synonyms of the
UMLS Metathesaurus concept C0008425.
While there have been attempts to map individual drugs
from ATC to concepts in the UMLS and MeSH through
lexical techniques, [7] note that these techniques are not
appropriate for the mapping of drug classes.
Instance-based techniques
Also called extensional techniques, instance-based tech-
niques compare classes based on the sets of individuals
(i.e., instances) of each class. While instance-based tech-
niques are also available in many ontology alignment sys-
tems, the applicability of this technique is limited, because
most biomedical ontologies consist of class hierarchies, but
do not contain information about instances. Here, however,
individual drugs (e.g., atorvastatin) are the members–not
subclasses–of drug classes (e.g., statins). In other words,
drug classes have individual drugs as instances, not sub-
classes and are therefore amenable to instance-based
alignment techniques.
Several methods have been proposed to implement
instance-based matching. [8] decompose these methods into
three basic elements: (1) A measure is used for evaluating
the association between two classes based on the proportion
of shared instances. Typical measures include information-
based measures (e.g., Jaccard similarity coefficient) and stat-
istical measures (e.g., log likelihood ratio). (2) A threshold is
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 3 of 14
http://www.jbiomedsem.com/content/5/1/30applied to the measures and pairs of classes for which
the measure is above the threshold are deemed closely
associated and mapping candidates. (3) Hierarchical re-
lations in the two ontologies to be aligned can also be
leveraged by deriving instance-class relations between
instances of a given class and the ancestors of this class.
In other words, in addition to asserted classes (i.e., the
classes of which individual drugs are direct members),
we also consider inferred classes (i.e., the classes of
which asserted classes are subclasses). For example,
the class asserted in MeSH for the drug atorvastatin is
Hydroxymethylglutaryl-CoA Reductase Inhibitors (i.e.,
statins), whose parent concepts include Anticholesteremic
Agents. Therefore, the class Anticholesteremic Agents is
an inferred drug class for atorvastatin.
To our knowledge, our work is the first attempt to align
drug classes with instance-based techniques (i.e., beyond
name matching), and the first application of aligning drug
classes in ATC and MeSH. Moreover, while most ontology
alignment systems mainly consider matches between
equivalent classes, we are also interested in identifying
those cases where one class is included in another class.
Related work on drug classes, MeSH and ATC
In previous work, we compared drug classes between
the National Drug File-Reference terminology (NDF-RT)
and SNOMED CT from the perspective of semantic
mining [9]. We also used an instance-based alignment
technique, but only considered overlap between classes,
not inclusion. Lexical alignment of the classes was not
performed. Overall, we found that the overlap between
NDF-RT and SNOMED CT classes was very limited. In
[10], we mapped selected drug classes between NDF-RT
and ATC through lexical techniques, observed the limi-
tations of lexical techniques for the alignment of drug
classes (also noted by [7]), and argued that the alignment
could be improved by identifying mappings between the
drugs in these classes.
As part of the EU-ADR project, [11] extracted adverse
drug reactions from the biomedical literature and mapped
MeSH drugs to ATC through the UMLS. However, their
mapping was limited to individual drugs and did not
include drug classes. The alignment of drug classes is
one element of the broader integration of drug information
sources in systems, such as the one developed by [12].
However, the preliminary version of their system integrates
ATC, NDF-RT, RxNorm and the Structured Product labels,
but not MeSH or the biomedical literature.
Resources
Our framework leverages several knowledge sources.
In addition to ATC and MeSH, the two sources of drug
classes we propose to align and from which we extract
information about drug-class membership, we also takeadvantage of RxNorm for aligning and normalizing indi-
vidual drugs, and of the UMLS Metathesaurus as a source
of synonymy for the lexical mapping of drug class names.
Anatomical Therapeutic Chemical Drug Classification
System (ATC)
The ATC is a clinical drug classification system developed
and maintained by the World Health Organization (WHO)
as a resource for drug utilization research to improve qual-
ity of drug use [2]. The system is organized as a hierarchy
that classifies clinical drug entities at five different levels: 1st
level anatomical (e.g., C: Cardiovascular system), 2nd level
therapeutic (e.g., C10: Lipid modifying agents), 3rd level
pharmacological (e.g., C10A: Lipid modifying agents, plain),
4th level chemical (e.g., C10AA: HMG CoA reductase
inhibitors), and 5th level chemical substance or ingre-
dient (e.g., C10AA05: atorvastatin). The 2013 version
of ATC integrates 4,516 5th-level drugs and 1,255 drug
groups (levels 1-4). We refer to these drug groups as
“ATC classes”.
Medical Subject Headings (MeSH)
The Medical Subject Headings (MeSH) is a controlled
vocabulary produced and maintained by the NLM [3]. It
is used for indexing, cataloging, and searching the bio-
medical literature in the MEDLINE/PubMed database,
and other documents. The MeSH thesaurus includes
26,853 descriptors (or “main headings”) organized in 16
hierarchies (e.g., Chemical and Drugs). Additionally,
MeSH provides about 210,000 supplementary concept
records (SCRs), of which many represent chemicals and
drugs (e.g., atorvastatin). Each SCR is linked to at least
one descriptor through a “heading mapped to” relation
(e.g., atorvastatin is associated with Heptanoic Acids and
Pyrroles). These descriptors “mapped to” generally denote
the chemical structure of the drug. While most chemical
descriptors provide a structural perspective on drugs,
some descriptors play a special role as they can be
used to annotate the functional characteristics of drug
descriptors and SCRs through a pharmacologic action
relation (e.g., atorvastatin is linked to the mechanism of
action Hydroxymethylglutaryl-CoA Reductase Inhibitors
and to the therapeutic use Anticholesteremic Agents).
MeSH 2013 is used in this study.
RxNorm
RxNorm is a standardized nomenclature for medications
produced and maintained by the U.S. National Library
of Medicine (NLM) [13]. RxNorm concepts are linked
by NLM to multiple drug identifiers for commercially
available drug databases and standard terminologies,
including MeSH and ATC. (While RxNorm integrates
drugs and classes from ATC and drugs from MeSH, it
does not integrate classes from MeSH.) RxNorm serves
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 4 of 14
http://www.jbiomedsem.com/content/5/1/30as a reference terminology for drugs in the U.S. The
August 2013 version of RxNorm used in this study inte-
grates 10,108 substances, including ingredients (IN) and
precise ingredients (PIN). Ingredients generally represent
base forms (e.g., atorvastatin), while precise ingredients tend
to represent esters and salts (e.g., atorvastatin calcium).
RxNorm also represents clinical drugs, i.e., the drugs
relevant to clinical medicine (e.g., atorvastatin 10 MG
Oral Tablet). The relations among the various drug entities
are represented explicitly in RxNorm (e.g., between
ingredients and precise ingredients, and between in-
gredients and clinical drugs). NLM also provides an
application programming interface (API) for accessing
RxNorm data programmatically [14].
Unified Medical Language System (UMLS)
The UMLS is a terminology integration system created and
maintained by the National Library of Medicine (NLM)
[15]. The UMLS Metathesaurus integrates over 150 ter-
minologies, including MeSH, but not ATC. Synonymous
terms across terminologies are grouped into concepts
and assigned the same concept unique identifier. The
Metathesaurus provides a comprehensive set of synonyms
for biomedical concepts, including drug classes, and is
often used for integrating terminologies beyond its own
(e.g., [16]). Therefore, the UMLS is a useful resource for
mapping class names from ATC to drug class concepts
present in the source vocabularies of the Metathesaurus.
NLM provides an application programming interface
(API) for accessing UMLS data programmatically. Version
2013AA of the UMLS is used in this studya.
Methods
Our framework for assessing the consistency of drug classes
across sources (here MeSH and ATC) uses techniques for
aligning drug classes based on their names and drug
instances as depicted in Figure 1. It can be summarized
as follows. Having established a reference list of drugs
and drug classes, we compare the drug classes between
MeSH and ATC based on their names (lexical align-
ment, Figure 1, right) and on the individual drugs these
classes contain as members (instance-based alignment,
Figure 1, left). Toward this end, we leverage similarityRxNorm
INs/PINs
MeSH
drugs
ATC
drugs ATC (classes)
MeSH (classes)
instance-based 
alignment
lexical
alignment
Figure 1 Alignment of ATC and MeSH classes.measures to compare the set of drugs in a class to the
set of drugs in another class from the dual perspective
of equivalence and inclusion. Finally, we compare the
alignments obtained by the two approaches.
Establishing a common reference for drugs, drug classes
and drug-class members
Drugs
As of August 2013, both ATC and MeSH are integrated
in RxNorm. We consider all MeSH drugs present in
RxNorm, regardless as to whether they correspond to
descriptors (also called “main headings”) or Supplementary
Concept Records (SCR) in MeSH. Our starting set of ATC
drugs consists of 5th-level ATC entities, from which we
exclude combination drugs, often underspecified and
unlikely to be represented in MeSH.
As a result of the integration of MeSH and ATC into
RxNorm, the same RxNorm identifier is assigned to an
ATC drug and to the equivalent drug in MeSH. Individ-
ual drugs in MeSH and ATC correspond to ingredients
(IN) and precise Ingredients (PIN) in RxNorm. In order
to facilitate the comparison of individual drugs between
MeSH and ATC, we normalize the drugs by mapping
each precise ingredient to its corresponding ingredient.
We restrict our set of drugs to drugs of clinical relevance
by filtering out those ingredients that are not associated
with any clinical drugs in RxNorm. The set of individual
drugs described here constitutes the set of eligible drugs
for this study.
Drug classes
In order to minimize the number of pairwise compari-
sons between MeSH and ATC drug classes, we exclude
broad, top-level classes from MeSH and ATC, for
which the alignment would not be meaningful anyway.
In practice, we exclude the 14 ATC classes of level 1
(anatomical classification). Similarly, we exclude the
top-level descriptors of the Chemicals and Drugs hier-
archy (i.e., D01-D27) in MeSH, as well as the top-level of
the pharmacological action descriptors (Pharmacologic
Actions, Molecular Mechanisms of Pharmacological Action,
Physiological Effects of Drugs, and Therapeutic Uses).
Additionally, we exclude 167 of the 1,241 ATC classes
(2nd–4th level) corresponding to drug combinations,
because combination drugs are often underspecified in
ATC. We define drug combination classes in ATC as
classes that contain “combination” (case-insensitive) in their
labels or have ancestor classes with “combination” in their
labels (e.g., G03EA: Androgens and estrogens are excluded
along with their ancestor class G03E: ANDROGENS
AND FEMALE SEX HORMONES IN COMBINATION).
Finally, we further exclude from MeSH and ATC any
classes that are not connected to any eligible individual
drug (as defined above), directly or through a subclass
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 5 of 14
http://www.jbiomedsem.com/content/5/1/30(e.g., A03AC: Synthetic antispasmodics, amides with
tertiary amines contains three drugs (dimethylamino-
propionylphenothiazine, nicofetamide, tiropramide), of
which none are eligibleb). The set of drug classes de-
scribed here constitutes the set of eligible drug classes
for this study.
Drug-class membership
As mentioned earlier, the relation between a class and
its drug members can be either direct (i.e., asserted) or
indirect (i.e., inferred). In ATC, we consider as direct re-
lations the relations asserted between 5th-level drugs
and their 4th-level chemical classes. We infer drug-class
relations between 5th-level drugs and the corresponding
ATC classes at the 3rd and 2nd level. For example, as il-
lustrated in Figure 2, the drug temafloxacin (J01MA05)
is a member of the chemical class Fluoroquinolones
(J01MA - asserted), the pharmacological class QUINOLONE
ANTIBACTERIALS (J01M–inferred, 3rd level), and the
therapeutic class ANTIBACTERIALS FOR SYSTEMIC USE
(J01–inferred, 2nd level). Level-1 classes are ignored.Figure 2 Individual drugs and drug classes in RxNorm, MeSH and ATCExtracting drug-class membership relations from MeSH
is a more complex process, because drugs can be repre-
sented at different levels (descriptor or supplementary
concept record), structural classes and functional classes
are represented by different types of descriptors, and drugs
are related to classes through various kinds of relationships.
Relations between drugs (descriptors or SCRs) and func-
tional classes (i.e., descriptor from the pharmacological
actions hierarchy) are asserted through a “pharmacologic
action” relationship. Relations between an SCR drug and
its heading mapped toc constitute the asserted relations
to structural classes, as do relations between a descriptor
drug and its direct parent. We infer drug-class relations
between any drug and all the ancestors (direct or indirect)
of the descriptors corresponding to their structural and
functional (asserted) classes.
For example, as illustrated in Figure 2, the SCR
temafloxacin has Anti-Bacterial Agents as pharmacological
action and Fluoroquinolones as heading mapped to. From
these asserted classes, we infer membership to Anti-Infective
Agents (from Anti-Bacterial Agents) and to Quinolones,.
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 6 of 14
http://www.jbiomedsem.com/content/5/1/30Quinolines, and Heterocyclic Compounds, 2-Ring (from
Fluoroquinolones). Top-level classes are ignored.
Aligning drug classes
Lexical alignment
We leverage the UMLS (synonyms and lexical match-
ing features) for aligning drug classes by their names.
In practice, we consider equivalent classes those MeSH
and ATC classes, whose names map to the same UMLS
concept. If both MeSH and ATC were integrated in the
UMLS, we would only have to extract all UMLS con-
cepts to which both a MeSH class and an ATC class
are mapped. Since MeSH is integrated in the version of
the UMLS used in this study, but ATC is not, we map
ATC classes to the UMLS in order to link them to the
equivalent classes in MeSH. More precisely, we use the
ExactString and NormalizedString search function of
the UTS API 2.0 to establish mappings between the
names of the ATC classes and UMLS concepts. We use
normalization only when the exact technique does not
result in a match. We then associate the ATC class to a
MeSH class through the UMLS concept to which they
both map (e.g., H03BA: Thiouracils to D013889: Thiouracil
through UMLS concept C0039957).
Instance-based alignment
We assess the similarity between two classes based on
the individual drug members (instances) they share. In
practice, we perform a pairwise comparison between all
ATC classes and all MeSH classes, asserted and inferred.
We define two scores for identifying equivalence and in-
clusion relations between ATC and MeSH classes.
Equivalence Score (ES) The Jaccard coefficient (JC) is a
measure of the similarity between two sets, for example
between the set of drugs in a given ATC class (A) and in
a given MeSH class (M). However, many drug classes
only contain a small number of drugs, and, in this case,
a small number of shared drugs between classes can
yield relatively high Jaccard values. In order to reduce
the similarity of pairs of classes with small numbers of
shared drugs, we use a modified version of the Jaccard
coefficient, JCmod, as suggested in [8],
JC A;Mð Þ ¼ am
aþmþ am
ES A;Mð Þ ¼ JCmod A;Mð Þ ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
am  am?0:8ð Þp
aþmþ am
where am represents the number of drugs common to
A and M, and a +m + am the total number of unique
drugs in both classes.
Inclusion Score (IS) The Jaccard coefficient measures
the similarity between the two classes, but does not reflectwhether one class is included in the other. Because of the
difference in organization and granularity between classes
in ATC and MeSH, a given ATC class may not have an
equivalent class in MeSH, but can be included in another
MeSH class (e.g., C07AA: Beta blocking agents, non-selective
included in D000319: Adrenergic beta-Antagonists). Such
inclusion relations are crucial for a comprehensive align-
ment of the drug classes. We introduce a metric for finding
fine-grained (“child”) classes that are included in coarse
(“parent”) classes. This metric combines two elements. The
first one measures the intensity of the “one-sidedness”, i.e.,
the extent to which the instances outside the intersection
are not distributed between both sides, but rather belong to
only one of the two classes. The second element measures
the coverage of the finer-grained (“child”) class by the
coarser (“parent”) class.
IS is calculated as follows:
IS A;Mð Þ ¼ 0; for Cp A;Mð Þ ¼ 0 and Cc A;Mð Þ ¼ 0
IS A;Mð Þ ¼ a?m
aþm 
am
min amþ a; amþmð Þ ; otherwise
where am represents the number of drugs common to A
and M, and a and m the number of drugs specific to A
and M, respectively.
For example, if A contains 10 drugs and M contains
20 drugs and if the two classes share 9 drugs, IS(A,M)= 0.75,
providing a strong indication that A is included in M.
More generally, a value of IS close to 0 indicates that
the drugs that are not shared by the two classes are
evenly distributed between the ATC and MeSH class, i.
e., there is no inclusion relation between the classes. In
contrast, a value of IS close to 1 (in absolute value) indi-
cates that the parent class contains most of the drugs
that are not shared by the two classes and that the child
class has a small proportion of specific drugs. The IS(A,M)
score varies between ?1 and 1, and a score of 1 corresponds
to the inclusion of A in M, while a score of ?1 corresponds
to the inclusion of M in A.
Selecting classes with the best equivalence and inclusion
relations A given class in ATC or MeSH may have both
equivalence and inclusion relations to classes from the
other terminology. Moreover, it may have more than
one equivalence relation and often has multiple inclu-
sion relations. We propose an approach for selecting
the best equivalence and inclusion relations for a given
class. We heuristically determined 0.5 to be a reasonable
threshold for both ES and IS. Therefore, none of the
pairs of classes with ES or IS values lower than 0.5 will
be considered for equivalence or inclusion, respectively.
For a given class Cc, the class Ce selected as the best
equivalent class is the one with the highest ES. In con-
trast, the class Cp selected as the best inclusion class is
not necessarily the one with the highest IS, because the
Table 1 Selection of the ATC and MeSH classes suitable
for the instance-based alignment
ATC MeSH
Candidate drugs in terminology 2,730 4,153
Corresponding drug entities in RxNorm (IN, PIN) 2,239 5,274
Drug entities after normalization of PINs to INs 2,215 4,112
Restriction to clinically-significant ingredients 1,706 2,339
Restriction to clinically-significant ingredients
present in both terminologies
1,685 1,685
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 7 of 14
http://www.jbiomedsem.com/content/5/1/30class with the highest IS is most likely a very broad class.
IS favors large parent classes, while the best parent class
is the smallest parent class that covers a large propor-
tion of the child class. Therefore, we select as the best
inclusion relation the first pair among the best candi-
date equivalence pairs for which IS is above the thresh-
old of 0.5. Although it might seem counterintuitive to
select inclusion pairs among the candidate equivalence
pairs, the high ES is consistent with the requirement for
coverage of the child class by the parent class.
Usually the best equivalence and inclusion pairs are
different, but not always. For instance, the mapping be-
tween two very similar classes, where one class contains
a few specific drugs, might have both IS and ES above
the threshold. Different use cases may call for different
strategies for determining the best equivalent and inclu-
sion pairs. For instance, while our strategy considers
both scores, ES and IS, when they are above the thresh-
old, an alternative strategy could be to choose one score
over the other based on max(ES, IS).
Assessing the consistency between lexical and
instance-based alignments
We hypothesize that classes with similar drugs should
have similar names and classes with similar names should
contain similar drugs. We compare the results of the
lexical and instance-based alignment methods and assess
their consistency. We expect the lexical alignment to iden-
tify equivalence classes, not class inclusion. Therefore, pairs
of classes identified through the lexical alignment (LEX+)
and identified as equivalent through the instance-based
alignment (EQ+) are considered consistent, as are the pairs
of classes neither identified through the lexical alignment
(LEX-) nor identified as equivalent through the instance-
based alignment (EQ-). Conversely, pairs of classes identified
through the lexical alignment (LEX+) but not identified as
equivalent through the instance-based alignment (EQ-) are
considered inconsistent, as are the pairs of classes not identi-
fied through the lexical alignment (LEX-) but identified as
equivalent through the instance-based alignment (EQ+).
Results
Establishing a common reference for drugs, drug classes
and drug-class members
Drugs
As shown in Table 1, we retrieved from RxNorm 2,239
Ingredients (IN) and Precise Ingredients (PIN) that are
mapped to 2,730 unique drugs in ATC, and 5,274 that are
mapped to 4,153 drugs in MeSH. After normalization to
INs, we selected 2,215 INs for ATC and 4,112 for MeSH.
Finally, after restricting the RxNorm INs to those that are
clinically relevant, we selected 1,706 INs for ATC and
2,339 for MeSH. Of these, 1,685 drugs are present in both
ATC and MeSH.Drug classes
From the 1,255 ATC classes (1st–4th level) we excluded
14 ATC classes at the 1st level (anatomical classification)
and 167 classes corresponding to drug combinations,
leaving 1,074 classes eligible for the lexical alignment.
We further excluded 81 empty classes without any
drug (ATC contains empty classes by design), and 159
classes containing only drugs that cannot be mapped to
RxNorm. The final set of ATC classes eligible for the
instance-based alignment, A*, contains 834 drug classes,
of which 558 are considered asserted (4th level) and 276
inferred (2nd–3rd level).
In MeSH, we identified 1,516 descriptors as drug classes
for the eligible drugs, including 1,223 asserted classes and
293 inferred classes. These classes constitute the set of
MeSH classes eligible for both the lexical and the instance-
based alignment, M*. We classify 403 of the drug classes in
M* as functional classes, i.e., their descriptors are located in
the Chemical Actions and Uses [D27] sub-tree in MeSH,
and 1,113 as structural classes.
Drug-class membership
For the 1,685 eligible drugs in MeSH, we established
15,122 drug-class pairs, of which 4,759 are asserted
and 10,363 inferred. For the eligible drugs in ATC, we
established 6,368 drug-class pairs, of which 2,140 are
asserted and 4,228 inferred.
Aligning drug classes
Lexical alignment
For the 1,074 eligible ATC classes, we were able to retrieve
226 mappings to descriptors from the Chemicals and Drugs
([D]) tree in MeSH. We found 18 mappings for therapeutic
classes (2nd level), 43 for pharmacological classes (3rd level),
and 165 for chemical classes (4th level). Of the 226 map-
pings, 99 are to pharmacological actions (functional classes)
in MeSH, whereas 127 are to other descriptors at various
levels of the MeSH hierarchy (structural classes).
Instance-based alignment
Equivalence and inclusion scores Of the 834 ATC
classes eligible for instance-based alignment (|A*| = 834),
828 (99%) could be associated with at least one MeSH class.
Table 3 Characterization of the associations between
ATC and MeSH classes based on scores for equivalence
and inclusion
ATC to MeSH Best equivalence
>.5 <.5 Total
Best inclusion >.5 148 (17%) 580 (70%) 728 (87%)
<.5 1 (1%) 99 (12%) 100 (13%)
Total 149 (18%) 679 (82%) 828 (100%)
MeSH to ATC Best equivalence
>.5 <.5 Total
Best inclusion >.5 120 (9%) 390 (30%) 510 (39%)
<.5 45 (3%) 762 (58%) 807 (61%)
Total 165 (12%) 1,152 (88%) 1,317 (100%)
Table 4 Consistency between lexical and instance-based
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 8 of 14
http://www.jbiomedsem.com/content/5/1/30Of the 1,516 eligible drug classes in MeSH (|M*| = 1,516),
1,317 (87%) could be associated with at least one ATC class.
We conducted a pairwise comparison of all ATC classes
with all MeSH classes (|A*| x |M*| = 1,264,344). For the
26,842 pairs that had at least one drug in common, we cal-
culated the equivalence (ES) and inclusion (IS) scores. As
shown in Table 2, 223 pairs (<1%) had an ES ? .5 and were
considered equivalent (EQ+), and 6,257 pairs (23%) had an
IS ? .5 and were considered in inclusion relation (IN+). Of
note, there were 108 pairs with both strong equivalence and
inclusion relations (EQ+ and IN+). The remaining 20,470
pairs were considered unrelated, absent any strong equiva-
lence or inclusion relations (EQ- and IN-).
Classes with strong equivalence and inclusion relations
A given class in ATC or MeSH may have more than one
strong relation to a drug class from the other terminology.
We determined the best equivalence and inclusion map-
pings (not mutually exclusive) for each of the 828 ATC and
1,317 MeSH classes with shared drugs, respectively.
As shown in Table 3 (top), 828 ATC classes had some re-
lation (equivalence or inclusion, but not necessarily strong)
to a MeSH class. Of these, we identified 149 ATC classes
(18%) with at a strong equivalence relation to MeSH, all
but one of which also showed a strong inclusion to some
MeSH class (albeit not necessarily the same as the equiva-
lent class). A strong inclusion relation to MeSH was found
for 728 (87%) of these ATC classes. On the other hand,
1,317 MeSH classes had some relation to an ATC class.
Of these, we identified 165 MeSH classes (12%) with a
strong equivalence relation to ATC, most of which also
showed a strong inclusion relation to some ATC class. A
strong inclusion relation to ATC was found for 510 (39%)
of these MeSH classes (Table 3, bottom). The 1,317 MeSH
classes linked to ATC include 374 functional classes (28%)
and 943 structural classes (72%). Overall, a strong relation
(equivalence or inclusion) was found between 729 ATC
classes in ATC and the 555 MeSH classes.
Assessing the consistency between lexical and
instance-based alignments
The results of the comparison between the lexical and
instance-based alignments are shown in Table 4. We
performed the comparison on the cross-product of the 834Table 2 Analysis of the instance-based alignment between
ATC and MeSH classes–equivalence vs. inclusion relations
Inclusion relation
Yes (IN+) No (IN-) Total
Equivalence relation Yes (EQ+) 108 115 223
No (EQ-) 6,149 20,470 26,619
Total 6,257 20,585 26,842eligible ATC and 1,516 MeSH classes (1,264,344 pairs). Of
the 226 pairs of equivalent classes between ATC and MeSH
identified through the lexical alignment, 36 (16%) were con-
firmed through the instance-based approach (LEX+/EQ+),
of which 14 were also categorized as inclusion relations.
Not surprisingly, no equivalence relation was identified by
either approach for the bulk of the pairs from the cross-
product between ATC and MeSH classes. A total of 313 in-
consistencies between the two alignment approaches were
identified, including 126 pairs identified exclusively by the
lexical alignment (LEX+/EQ-), and 187 pairs specific to the
instance-based alignment (LEX-/EQ+). This finding dis-
proves our initial hypothesis that classes with similar names
have similar drugs and vice versa. Of note, 64 pairs of
equivalent classes identified through the lexical alignment
were not amenable for processing by the instance-based
alignment, because at least one class of the pair did not
contain any eligible drug.
Discussion
Analysis of similarities and discrepancies between lexical
and instance-based alignments
As illustrated through a few examples throughout this
section, our framework facilitates the comparison of drug
classes across sources and reveals inconsistencies in the
classes, as well as deficiencies in the alignment techniques.alignments of drug classes (italics values denote
inconsistencies)
Lexical alignment
Yes (LEX+) No (LEX-) Total
Instance-based
alignment
Yes (EQ+) 36 187 223
No (EQ-) 126 1,263,995 1,264,121
Total 162 1,264,182 1,264,344
No data 64
Total LEX+ 226
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 9 of 14
http://www.jbiomedsem.com/content/5/1/30Valid mappings
We identified an equivalence relation between the 4th-level
ATC class Tetracyclines (J01AA) and the MeSH descriptor
Tetracyclines (D013754). The two classes share nine drugs.
The MeSH class has one extra drug (meclocycline), which is
in a different class in ATC (Antiinfectives for treatment of
acne), because, although structurally similar, it is not used
systemically but topically. Jaccard similarity is high (0.86).
This (equivalence) mapping is also identified by the
lexical technique (exact match). Of note, the inclusion
score (1 in absolute value) is also high, because there is only
one drug that is not in common, which is - automatically -
located on only one side of the intersection.Erroneous lexical mappings
We identified an inclusion mapping between the 4th-
level ATC class Fluoroquinolones (S01AE) and the MeSH
descriptor Fluoroquinolones (D024841). Although the two
class names are identical, which would suggest an equiva-
lence relation, our mapping is identified as an inclusion,
with seven drugs in common, one drug specific to the
ATC class and eleven drugs specific to the MeSH class. In
fact, the ATC class is the specific class of fluoroquinolones
for ophthalmic use (S01AE), in contrast to the class of
fluoroquinolones for systemic use (J01MA)d. The fluor-
oquinolones used for eye disorders are (almost) a subset
of all fluoroquinolones and the ATC class S01AE is ap-
propriately characterized as being included in the MeSH
class for fluoroquinolones. This example also constitutes
an erroneous lexical mapping, since lexical mappings are
expected to reflect equivalence relations.Missing instance-based mappings
Many ATC and MeSH classes share only one or very
few drugs, making it difficult to assess equivalence or inclu-
sion with confidence. For example, the 4th-level ATC class
Silver compounds (D08AL) and the MeSH descriptor Silver
Compounds (D018030) share only one drug (silver nitrate),
where Silver Compounds (D018030) contains another drug
(silver acetate), which is in RxNorm but not in ATC. The
modified version of the Jaccard coefficient has a score of
0.22 in this case, which is below our threshold of 0.5 for
equivalence. However, we classified the ATC class D08AL
as being included in the MeSH class Silver Compounds.
During this failure analysis, we discovered that some
MeSH drugs did not have a pharmacological action
assigned to them as we expected. For example, while pyr-
antel is listed as Antinematodal Agents, oxantel is note.
The MeSH editorial rules require that a certain number of
articles assert a given pharmacologic action for it to be
recorded in MeSH. Because of these missing pharmaco-
logic actions, the 3rd-level ATC class ANTINEMATODAL
AGENTS (P02C) fails to be mapped to the MeSHpharmacological action Antinematodal Agents (D000969),
the Jaccard similarity being below the threshold (0.37).
As mentioned earlier, some ATC classes only contain
drugs that cannot be mapped to MeSH through RxNorm,
which we used to bridge between the two. Such classes
may be amenable to lexical alignment, but cannot be
aligned through their instances. Similarly, some drug
entities and biologicals (e.g., vaccines) are less well stan-
dardized than most common drugs. For this reason, the
instance-based alignment may not be able to align these
classes, when simple lexical techniques can. For example,
the instance-based method fails to align the two classes
Epoxides (L01AG) and Epoxy Compounds (D004852) be-
cause the ATC class does not contain any eligible drug
(the only instance, etoglucid (L01AG01), is not listed as
a clinical drug in RxNorm).
Missing lexical mappings
Despite the use of UMLS synonymy and normalization,
the lexical alignment fails to identify a mapping between
the 3rd-level ATC class POTASSIUM-SPARING AGENTS
(C03D) and the MeSH pharmacological action Diuretics,
Potassium Sparing (D062865). In contrast, the instance-
based alignment identifies an equivalence mapping with
high Jaccard similarity (0.72). This finding is consistent with
the conclusions of [7].
Further characterization of equivalence and
inclusion relations
Even when considering only strong relations and the
best inclusion relations between ATC and MeSH classes,
it is difficult to give a detailed account of the direction-
ality of the relations, and the distribution between struc-
tural and functional classes. Some salient findings are
summarized in Table 5. For example, we found 223
(strong) equivalence relations between 149 unique ATC
classes and 165 unique MeSH classes, distributed al-
most evenly between structural and functional classes in
MeSH. When restricting the analysis to the best inclu-
sion relations, more ATC classes (728) are found to be
included in some MeSH class, than MeSH classes (510)
are in some ATC classes. And fewer functional classes
(146) than structural classes (364) in MeSH are included
in some ATC class.
For almost all drug classes in ATC that have an
equivalence mapping to a drug class in MeSH, there is
also at least one inclusion mapping to a broader class
in MeSH. There is only one exception. The class Drugs
used in diabetics (A10) is equivalent to Hypoglycemic
Agents (D007004), which is already at the highest level we
consider in MeSH (we ignore its parent class Physiological
Effects of Drugs because it is too general). In contrast,
there are 45 classes in MeSH that are equivalent to ATC
classes but are not included in another class in ATC. For
Table 5 Detailed analysis of the mapping between ATC
and MeSH classes–Structural vs. functional classes
Type of
relation
Direction # strong
relations
# unique
ATC classes
# unique
ATC classes
Equivalence
(all)
ATC-MeSH (all) 223 149 165
ATC-MeSH (St) 115 77 84
ATC-MeSH (Fn) 108 86 81
Inclusion
(all)
ATC to
MeSH (all)
4914 728 650
MeSH (all)
to ATC
1343 358 510
Inclusion
(best)
ATC to
MeSH (all)
1267 728 483
ATC to
MeSH (St)
597 559 275
ATC to
MeSH (Fn)
670 657 208
MeSH (all)
to ATC
568 264 510
MeSH (St)
to ATC
406 211 364
MeSH (Fn)
to ATC
162 102 146
Details of the instance-based alignment between functional (Fn) and structural
(St) classes in ATC and MeSH.
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 10 of 14
http://www.jbiomedsem.com/content/5/1/30example, Antiparkinson Agents (D00978) maps to the 2nd
level class Anti-Parkinson Drugs (N04) in ATC. Because
we exclude 1st level classes in ATC, there is no parent
class in ATC which would include the drug of the MeSH
class Antiparkinson Agents. Conversely, the ATC class
Anti-Parkinson Drugs (N04) is included in the higher level
class Central Nervous System Agents (D002491) in MeSH,
which is a parent of Antiparkinson Agents.
The alignment between ATC classes and MeSH classes
can be further characterized, especially in order to ac-
count for concomitant occurrences of a strong inclusion
relation to a structural class and to a functional class. As
shown in Table 6, of the 505 strong equivalence and bestTable 6 Detailed analysis of the mapping between ATC
and MeSH classes–equivalence vs. inclusion relations
ATC to MeSH To a
structural
class only
To a
functional
class only
To both a
structural and a
functional class
Total
Equivalence
relation only
0 1 0 1
Both equivalence
and best
inclusion relations
1 8 58 67
Best inclusion
relations only
50 75 312 437
Total 51 84 370 505
Analysis of concomitant equivalence and best inclusion relations between ATC
and MeSH classes, when structural and functional classes in MeSH are
considered separately.inclusion relations to structural and functional clas-
ses in MeSH, the most frequent situation is the con-
comitant occurrence of inclusion to both a structural
and a functional class. Of note, there is only one case
where an equivalence relation occurs without a con-
comitant inclusion relation.
Application of the framework to the alignment of
important drug classes
One typical use case for the alignment of drug classes
is to find equivalent classes in reference sources for a
given class (e.g., to find which class best represents
macrolides in MeSH and ATC). In order to illustrate
how our approach supports the alignment of drug
classes between MeSH and ATC, we applied our
framework to a set of clinically relevant drug clas-
ses. We used the set of high-severity, clinically sig-
nificant drug–drug interactions created by [17], in
which most drugs are categorized in reference to
drug classes.
We extracted all 13 drug classes from the list of veri-
fied critical drug–drug interactions discussed in their
paper (Table 7). We first performed a lexical mapping
to identify these 13 classes in MeSH and ATC (using
normalized string matches against the UMLS). Only
in six cases did the lexical mapping approach retrieve
classes in both classifications. In another six cases, we
were able to retrieve the class in either ATC or MeSH.
The class QT prolonging agents was not found in either
source.
For each drug class that we retrieved through lexical
mapping, we used our instance-based approach to deter-
mine the best corresponding class in the other termin-
ology. Table 8 shows the strength of the mappings in
terms of equivalence and inclusion. There is only one case
(HMG CoA reductase inhibitors) where the two lexical
matches also correspond to the best equivalent classes
based on the drug instances. For five other classes we
found equivalent class pairs starting from one lexical
match. For four classes we could not find equivalent
mappings across the two classifications, but inclusion map-
pings instead. Finally, three classes were left unmapped.
(Two of these classes were underspecified as evidenced by
the mention “[and] derivatives” in their name. The last one,
QT prolonging agents, was not represented in either source,
which is often the case for drug classes defined in reference
to adverse effects [18]).
This application illustrates the effectiveness of our
framework to support a clinical expert in the curation of
an alignment of drug classes between MeSH and ATC.
It helps identify lexically similar classes in these two
sources, but, more importantly, it helps identify which
class of the other source is most closely related to a
given class. This feature enables experts to verify if the
Table 7 Lexical mapping to ATC and MeSH for 13 clinically relevant drug classes
DDI class ATC class lexical match MeSH class lexical match Best corresponding
class in ATC
Best corresponding
class in MeSH
Triptans - Tryptamines (D014363) Selective serotonin (5HT1)
agonists (N02CC)
-
Proton pump inhibitors Proton pump inhibitors
(A02BC)
Proton pump inhibitors
(D054328)
- 2-Pyridinylmethylsulfinyl-
benzimidazoles (D053799)
HMG CoA reductase inhibitors HMG CoA reductase
inhibitors (C10AA)
Hydroxymethylglutaryl-CoA
Reductase Inhibitors (D019161)
- -
Tricyclic antidepressants - Antidepressive agents,
Tricyclic (D000929)
Non-selective monoamine
reuptake inhibitors (N06AA)
-
Protease inhibitors Protease inhibitors (J05AE) Protease inhibitors (D011480) - HIV Protease inhibitors
(D017320)
Narcotic analgesics - Narcotics (D009294) OPIOIDS (N02A) -
Selective serotonin
reuptake inhibitors (SSRIs)
Selective serotonin
reuptake inhibitors
(N06AB)
Serotonin uptake inhibitors
(D017367)
Selective serotonin reuptake
inhibitors (N06AB)
Serotonin uptake inhibitors
(D017367)
MAO inhibitors MAO inhibitors (C02KC) Monoamine oxidase
inhibitors (D008996)
Monoamine oxidase inhibitors,
non-selective (N06AF)
Benzylamines (D001596)
Macrolides Macrolides (J01FA) Macrolides (D018942) Macrolides (J01FA) Macrolides (D018942)
Azoles - Azoles (D001393) Imidazole and triazole
derivatives (D01AC)
-
Amphetamine derivatives - Amphetamines (D000662) - -
Ergot alkaloids and derivatives Ergot alkaloids
(C04AE, G02AB, N02CA)
Ergot Alkaloids (D004876) - Ergotamines (D004879)
QT prolonging agents - - - -
Lexical mapping to ATC and MeSH (columns 2-3) for 13 clinically relevant drug classes, along with their corresponding class in the other source obtained through
instance-based mapping (columns 4-5). Italicized classes denote best corresponding pairs of classes.
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 11 of 14
http://www.jbiomedsem.com/content/5/1/30equivalence suggested through lexical mapping is also
supported by a large proportion of shared drugs be-
tween these two classes. For example, the original class
Proton pump inhibitors is mapped lexically to Proton
pump inhibitors in ATC and to Proton Pump Inhibitors
in MeSH. The best corresponding class in MeSH for the
ATC class Proton pump inhibitors, however, is not Proton
Pump Inhibitors, but rather 2-Pyridinylmethylsulfinyl-
benzimidazolesf. Moreover, in many cases, the original
class can only be mapped lexically to either MeSH or
ATC. In these cases, the instance-based mapping offers
a solution for finding which class of the other source
has the best correspondence. For example, the original
class Tricyclic antidepressants can only be mapped
lexically to the class Antidepressive Agents, Tricyclic in
MeSH. However, the instance-based mapping identifies
the ATC class Non-selective monoamine reuptake inhibitors
as a potential equivalence.
While exploring mappings for these 13 clinically signifi-
cant drug classes, we actually found no cases where the best
corresponding classes in MeSH and ATC had exactly the
same members. Here are some reasons why.
  As mentioned earlier, the classificatory principles
used by ATC and MeSH are different. For example,Azoles represents a broad structural class in MeSH,
whereas ATC splits azole drugs into several classes
based on their therapeutic use (e.g., antibacterials
and antimycotics).
  Some drugs appear to be missing from ATC,
because of differences in the scopes of MeSH and
ATC. Such drugs include dietary supplements
(e.g., red yeast rice), veterinary drugs (e.g., many
macrolides exclusively marketed for veterinary use),
drugs of abuse (e.g., heroin) and drugs that only
exist in combinations (e.g., lopinavir and ritonavir,
but not lopinavir alone).
  Even though they are present in MeSH, some drugs
appear to be missing from MeSH classes, because of
missing relations to a drug class. For example, the
class assigned to tipranavir is Anti-HIV Agents,
while most of the drugs from the same ATC class
are (more appropriately) in the MeSH class HIV
Protease Inhibitors.
  In many cases, the name of an ATC class is
underspecified, i.e., derives part of its meaning from
its position in the hierarchy. As a consequence, the
lexical mapping of such class names is likely to point
to a broader class in MeSH. For example, the ATC
class Protease inhibitors is under the class Antivirals
Table 8 Best corresponding classes in ATC and MeSH for 13 clinically relevant drug classes
DDI class ATC class MeSH class Drugs
common
Drugs only
in ATC
Drugs only
in MeSH
ES. IS Rel.
Triptans Selective serotonin
(5HT1) agonists
Tryptamines 7 0 1 0.82 ?1 Eq
Proton pump inhibitors Proton pump inhibitors 2-Pyridinylmethylsulfinyl-
benzimidazoles
5 1 0 0.76 1 Eq
HMG CoA reductase inhibitors HMG CoA reductase
inhibitors
Hydroxymethylglutaryl-
CoA Reductase Inhibitors
8 0 2 0.76 ?1 Eq
Tricyclic antidepressants Non-selective monoamine
reuptake inhibitors
Antidepressive agents,
Tricyclic
10 2 2 0.69 0 Eq
Protease inhibitors Protease inhibitors HIV Protease inhibitors 8 3 1 0.63 0.44 Eq
Narcotic analgesics OPIOIDS Narcotics 15 3 11 0.50 ?0.48 Eq
Selective serotonin
reuptake inhibitors (SSRIs)
Selective serotonin
reuptake inhibitors
Serotonin uptake
inhibitors
6 0 8 0.40 ?1 In
MAO inhibitors Monoamine oxidase
inhibitors, non-selective
Monoamine oxidase
inhibitors
3 0 5 0.32 ?1 In
Macrolides Macrolides Macrolides 8 0 21 0.26 ?1 In
Azoles Imidazole and
triazole derivatives
Azoles 11 1 147 0.07 ?0.90 In
Amphetamine derivatives - - -
Ergot alkaloids and derivatives - - -
QT prolonging agents - - -
Best corresponding classes in ATC and MeSH for 13 clinically relevant drug classes, with the equivalence (ES) and inclusion (IS) scores from our framework’s
metrics, and the relation, equivalence or inclusion, between the two classes (Rel).
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 12 of 14
http://www.jbiomedsem.com/content/5/1/30for systemic use, which means that it represents not
all protease inhibitors, but only those that are used
to treat viral infections (which, in practice, means
HIV infections.)g In contrast, the MeSH class
Protease Inhibitors truly represent all drugs, whose
mechanism of action is to block some protease
enzyme. Therefore, despite the similarity of their
names, the ATC class Protease inhibitors is actually
included in the MeSH class with the same name,
and the best equivalence in MeSH for the ATC class
Protease inhibitors is actually the class HIV Protease
Inhibitors.
  Differences in granularity between MeSH and ATC
classes are also responsible for some of the
discrepancies observed in the mapping between the
two sources. For example, the MeSH class
Monoamine Oxidase Inhibitors is not found in ATC,
which provides three more specific classes instead
(Monoamine oxidase inhibitors, non-selective,
Monoamine oxidase A inhibitors, Monoamine
oxidase B inhibitors).
Application of the framework to the integration of the
MeSH and ATC classifications
The equivalence and inclusion relations obtained through
our framework can be combined in order to integrate the
hierarchical structures of two drug classifications, such asMeSH and ATC. These additional relations create bridges
across the original classifications, yielding an emerging
hierarchy that combines both of them. As an illustration,
we integrated the classes related to alkylating agents in
MeSH and ATC. As depicted in Figure 3, all 4th-level
classes under Alkylating Agents (L01A) in ATC have in-
clusion mappings to Antineoplastic Agents, Alkylating
and Alkylating Agents in MeSH. The 3rd-level ATC class
Alkylating Agents (L01A) itself is found to be equivalent to
these two classes in MeSH and is included in their parent
classes, Antineoplastic Agents and Toxic Actions, respect-
ively. The 2nd-level ATC class Antineoplastic Agents (L01)
can be regarded as equivalent to one of these parents,
namely Antineoplastic Agents, although the equivalence
score ES is slightly under the threshold of 0.5. Such a
representation helps users make sense of the similarities
and differences in the organizational structure of the
classifications.
Limitations and future work
The purpose of this framework is to provide a set of
methods for assessing the consistency of drug classes
across sources. While we believe our framework will
facilitate the curation of an alignment of drug classes
between two sources, it is beyond the scope of this
work to provide such a reference alignment. Moreover,
different reference alignments will most likely be
Alkyla ng Agents
An neoplas c Agents, Alkyla ng
Noxae
Other alkyla ng agents L01AX
Nitrogen mustard analogues L01AA
Alkyl sulfonates L01AB
Nitrosoureas L01AD
Ethylene imines L01AC
ALKYLATING AGENTS L01A
ANTINEOPLASTIC AGENTS L01 An neoplas c Agents Toxic Ac ons
Nitrogen Mustard Compounds D009588
Butylene Glycols D002072 / Mesylates D008698
Aziridines D001388 / Triethylenephosphoramide D013721
Nitrosourea Compounds D009607
Func onal
(PA)
Structural
(MH)
has_PA
*
Inclusion
Equivalence
Hierarchical  rela on
asserted by terminology
* Below the threshold
Figure 3 Integration of MeSH and ATC through the equivalence and inclusion relations obtained through our framework.
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 13 of 14
http://www.jbiomedsem.com/content/5/1/30required for different use cases, as different applica-
tions require different degrees of confidence.
As part of this framework, we have developed equiva-
lence and inclusion scores, for which we have deter-
mined thresholds heuristically. We have not, however,
fully investigated the impact of increasing or lowering
these thresholds on the quality of the alignment. We plan
to do so in future work.
Another limitation is that we have only applied our
framework to one pair of drug classifications, MeSH and
ATC. However, our framework is amenable to aligning
any pairs of classifications for which instance-level
data are available. We plan to revisit our earlier work
on NDF-RT and SNOMED CT classes to demonstrate
the generalizability of our approach.
As mentioned earlier, the instance-based alignment can
be applied only to those classes for which both MeSH and
ATC have drug members. This has been shown to be a
limitation. On the other hand, the lexical alignment
can still be used on these classes.
The UMLS Methesaurus relies for a large part on lexical
similarity for determining synonymy among terms. With
the recent inclusion of ATC in the UMLS Metathesaurus
(in version 2013AB of the UMLS), it would no longer be
necessary for us to perform the lexical alignment of ATCclasses to MeSH classes, since we could simply derive it
from the UMLS, where synonymous terms from various
sources are given the same UMLS concept unique identi-
fier. However, as discussed earlier, the lexical similarity of
class names does not always reflect equivalence and our
instance-based mapping remains an important alternative
method for comparing classes.Conclusions
To our knowledge, our work is the first attempt to align
drug classes with sophisticated instance-based techniques,
while also distinguishing between equivalence and inclusion
relations. Additionally, it is the first application of aligning
drug classes in ATC and MeSH. Moreover, this is the first
systematic investigation of the consistency between lexical
and instance-based alignment techniques for these two
drug resources. We believe that the proposed framework
will effectively support the curation of a mapping between
ATC and MeSH drug classes by providing a detailed ac-
count of the interrelations between the two resources.Endnotes
aATC was integrated for the first time in version 2013AB
of the UMLS released after this study was completed.
Winnenburg and Bodenreider Journal of Biomedical Semantics 2014, 5:30 Page 14 of 14
http://www.jbiomedsem.com/content/5/1/30bNone of these drugs are currently available on the
U.S. market.
cIf the SCR is mapped to a drug, rather than a structural
class descriptor, we associate it with the structural class of
this drug descriptor instead.
dWhen ATC was integrated into the UMLS Metathe-
saurus, new terms were created for ambiguous classes
such as Fluoroquinolones, which appears at several loca-
tions in the ATC hierarchy with slightly different mean-
ings (e.g., Fluoroquinolone antiinfectives, ophthalmologic
for S01AE and Fluoroquinolone antibacterials, systemic
for J01MA).
eThe pharmacological action Antinematodal Agents for
oxantel was not present in MeSH 2013, but was added
to MeSH in the 2014 edition.
fUpon investigation, it appears that some proton pump
inhibitor drugs, such as esomeprazole, were missing a
link to the class Proton Pump Inhibitors in the 2013 ver-
sion of MeSH. This was corrected in the 2014 version.
gWhen ATC was integrated into the UMLS Metathe-
saurus, the new term Protease inhibitors, direct acting
antivirals was created for the underspecified class Protease
inhibitors (J05AE).
Competing interests
The authors declare that they have no competing interests.
Authors’ contributions
RW and OB conceived the project and contributed equally to performing
the acquisition, analysis, and interpretation of data and to the writing of the
manuscript. Both authors read and approved the final manuscript.
Acknowledgements
This work was supported by the Intramural Research Program of the NIH,
National Library of Medicine (NLM). This work was also supported by the
Office of Translational Sciences, Center for Drug Evaluation and Research at
the Food and Drug Administration (FDA) through an interagency agreement
with NLM (XLM12011 001). The authors want to thank Fred Sorbello, Ana
Szarfman, Rave Harpaz and Anna Ripple for useful discussions.
Received: 3 December 2013 Accepted: 4 February 2014
Published: 9 July 2014
RESEARCH Open Access
The pathway ontology – updates and
applications
Victoria Petri1*, Pushkala Jayaraman1, Marek Tutaj1, G Thomas Hayman1, Jennifer R Smith1, Jeff De Pons1,
Stanley JF Laulederkind1, Timothy F Lowry1, Rajni Nigam1, Shur-Jen Wang1, Mary Shimoyama1,4,
Melinda R Dwinell1,2, Diane H Munzenmaier1,2, Elizabeth A Worthey1,3 and Howard J Jacob1,2,3
Abstract
Background: The Pathway Ontology (PW) developed at the Rat Genome Database (RGD), covers all types of
biological pathways, including altered and disease pathways and captures the relationships between them within
the hierarchical structure of a directed acyclic graph. The ontology allows for the standardized annotation of rat,
and of human and mouse genes to pathway terms. It also constitutes a vehicle for easy navigation between gene
and ontology report pages, between reports and interactive pathway diagrams, between pathways directly
connected within a diagram and between those that are globally related in pathway suites and suite networks.
Surveys of the literature and the development of the Pathway and Disease Portals are important sources for the
ongoing development of the ontology. User requests and mapping of pathways in other databases to terms in the
ontology further contribute to increasing its content. Recently built automated pipelines use the mapped terms to
make available the annotations generated by other groups.
Results: The two released pipelines – the Pathway Interaction Database (PID) Annotation Import Pipeline and the
Kyoto Encyclopedia of Genes and Genomes (KEGG) Annotation Import Pipeline, make available over 7,400 and
31,000 pathway gene annotations, respectively. Building the PID pipeline lead to the addition of new terms within
the signaling node, also augmented by the release of the RGD “Immune and Inflammatory Disease Portal” at that
time. Building the KEGG pipeline lead to a substantial increase in the number of disease pathway terms, such as
those within the ‘infectious disease pathway’ parent term category. The ‘drug pathway’ node has also seen
increases in the number of terms as well as a restructuring of the node. Literature surveys, disease portal
deployments and user requests have contributed and continue to contribute additional new terms across the
ontology. Since first presented, the content of PW has increased by over 75%.
Conclusions: Ongoing development of the Pathway Ontology and the implementation of pipelines promote an
enriched provision of pathway data. The ontology is freely available for download and use from the RGD ftp site at
ftp://rgd.mcw.edu/pub/ontology/pathway/ or from the National Center for Biomedical Ontology (NCBO) BioPortal
website at http://bioportal.bioontology.org/ontologies/PW.
Keywords: Biological pathway, Ontology, Pipeline, Pathway annotations, Pathway diagrams
* Correspondence: vpetri@mcw.edu
1Human and Molecular Genetics Center, Medical College of Wisconsin,
Milwaukee, WI, USA
Full list of author information is available at the end of the article
JOURNAL OF
BIOMEDICAL SEMANTICS
© 2014 Petri et al.; licensee BioMed Central Ltd. This is an open access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited.
Petri et al. Journal of Biomedical Semantics 2014, 5:7
http://www.jbiomedsem.com/content/5/1/7
Background
Introduction
The Pathway Ontology (PW) originated and is being de-
veloped at the Rat Genome Database (RGD) [1]. Its goal
is to cover any type of biological pathway, including al-
tered and disease pathways, and to capture the relation-
ships between them within the hierarchical structure of
a controlled vocabulary or ontology. The building of bio-
logical ontologies as directed acyclic graphs (DAG) and
the use of structured or controlled vocabularies was first
advanced and implemented by the Gene Ontology (GO)
project [2,3]. Many bio-ontologies have been developed
since [4], as witnessed by the ever-growing number sub-
mitted to and made available at the National Center for
Biomedical Ontology (NCBO) BioPortal [5,6]. Several
ontologies, including the Pathway Ontology, are being
developed at RGD ([7], in the “Biomedical Ontologies”
thematic series of the Journal of Biomedical Semantics).
Within the structure of a DAG, terms have defined rela-
tionships to one another and a particular term can have
more than one parent. This means that there can be
more than one path in the ontology tree from a broader,
more general parent term to a more specialized child
term. Within the tree structure, terms are nodes whose
names designate the class(es) they represent and which
are connected by edges that represent the relationship(s)
between them. In PW, a node is the network/pathway
class it stands for, and its features and aspects are cap-
tured in the definition. A pathway is a set of inter-
connected reactions and interactions whose delineation
and scope are used as a model for exploring and study-
ing, describing and understanding the working of and
relationships between biomolecules within a context.
The categories or types of pathways are conceptualized
and referenced in the scientific literature and repre-
sented in pathway databases such as the Kyoto
Encyclopedia of Genes and Genomes (KEGG), the
Pharmacogenomics Knowledge Base (PharmGKB), the
Small Molecule Pathway Database (SMPDB) and Wiki-
Pathways, among others [8-11].
The pathway ontology structure
The first of the main five nodes of the ontology, the meta-
bolic node, contains networks/pathways that stand for/rep-
resent the set of reactions underlying the transformation of
compounds. The set of reactions/interactions underlying
the coordinated responses that maintain the cellular/tissue
and/or organ/organismal status quo and homeostasis are
placed under the regulatory node. The set of reactions/in-
teractions initiated or triggered by a binding/molecular
interaction/conformational change event are found under
the signaling node. The set or sets of interactions where
one or more are deviant and represent the system’s perturb-
ation(s) fall under the disease node. Finally, the set or sets
of reactions/interactions representing the system’s response
to and handling of treatment(s) geared towards dealing with
those perturbation(s) are housed in the drug node. Thus,
the main nodes of the Pathway Ontology are: metabolic,
regulatory, signaling, disease and drug pathway (Figure 1A).
Two types of relationships are being used in the ontology:
“is_a” and “part_of”. For instance, insulin and glucagon are
peptide hormones whose signaling - ‘insulin signaling path-
way’ and ‘glucagon signaling pathway’, are children terms in
an ‘is-a’ relationship to the parent term ‘peptide and protein
hormone signaling pathway’. The two signaling pathways
which are initiated in response to high levels of circulating
glucose – ‘insulin signaling pathway’, or low – ‘glucagon sig-
naling pathway’, and whose engagement of intracellular cas-
cades aims at restoring the normal physiological levels of
glucose, are also in a “part-of” relationship to the ‘glucose
homeostasis pathway’ term, along with other pertinent
terms. Insulin also plays important roles in energy homeo-
stasis. In the brain, insulin (and leptin) act to increase the
expression of appetite-decreasing Pomc while decreasing
the expression of appetite-stimulating Agrp genes. The
‘peptide and protein hormone signaling pathway’ term is in
turn a child of the more general term ‘hormone signaling
pathway’, as other classes of compounds with very different
physico-chemical properties can also act as hormones. For
instance, the steroid hormones and the eicosanoids which,
as the names suggest, are hormones, are lipid molecules.
The signaling pathways they initiate are children of the
‘lipid hormone signaling pathway’ term which in turn, is a
sibling of ‘peptide and protein hormone signaling pathway’
and child of ‘hormone signaling pathway’ terms (Figure 1B).
The nodes are not disjoint and a given pathway class can
be the child of terms residing in different nodes, as the ex-
amples of insulin and glucagon signaling above show. The
‘peptide and protein hormone signaling pathway’ and the
‘glucose homeostasis pathway’ are both parents of the sig-
naling pathways of insulin and glucagon, albeit with differ-
ent relationships to their children; the two parent terms are
within the signaling and regulatory nodes, respectively. The
‘energy homeostasis pathway’ term is also a parent of insu-
lin signaling and like glucose homeostasis, it is within the
regulatory node (Figure 1C).
The “pathway” and the “process” concepts, although at
times interchangeably used, are distinct. A pathway conveys
the idea of a set of interacting molecules, of the reactions
and interactions underlying its functioning. A process on
the other hand, conveys the idea of the end result, the con-
clusion of a plan of action, whether the consequence of the
combined work that the set of reactions and interactions
produces, in the case of a simpler one, or in the case of a
more complex one, the combined work of pathways that
contribute to or in some fashion modulate the end result.
At the same time, a given pathway can participate in and/
or regulate several processes [12]. In the Biological Process
Petri et al. Journal of Biomedical Semantics 2014, 5:7 Page 2 of 12
http://www.jbiomedsem.com/content/5/1/7
(BP) ontology of GO there are metabolic and other process
terms that map to KEGG pathways and to terms in PW.
For instance, the formation of a fatty acid molecule is the
‘fatty acid biosynthetic process’ term in GO; it is the ‘fatty
acid biosynthetic pathway’ term and the ‘fatty acid biosyn-
thesis’ entry in PW and at KEGG, respectively. While the
phrasing is similar in GO, PW and KEGG, the term repre-
sents a process in GO, a pathway in PW and the KEGG
database. KEGG is a primary source for metabolic pathways
and projects such as databases and ontologies that in some
fashion represent metabolism are going to exhibit a sharing,
or an overlapping of terms/entries naming, but not an over-
lapping of concepts and/or contexts. Likewise, there are sig-
naling pathway terms in BP that relate to similar terms in
the signaling pathway node of PW and map to entries in
pathway databases such as KEGG and others. However, the
positions of and relationships between such terms are dif-
ferent, as are the perspectives of the two ontologies.
Disease and altered pathways
The provision of terms for the altered versions of path-
ways and the representation of disease pathways and dia-
grams as collections of altered pathways are unique to
PW and its use at RGD. An altered pathway is one
where defects in one or several components of the path-
way affect its normal functioning with potential implica-
tions for a diseased phenotype. The severity of an
altered pathway or the convergence of several altered
pathways can overcome the ability of the system to ad-
just and is manifested in the diseased state. Viewing
A
C
B
Figure 1 The pathway ontology main nodes and positions of selected terms. A. The five nodes of the Pathway Ontology. B. The term ‘lipid
hormone signaling pathway’ in the ontology showing the parent, siblings and children terms. C. The term ‘insulin signaling pathway’ in the
ontology showing the position of the term within the tree. ‘Insulin signaling pathway’ is in a part_of relationship to the ‘glucose’ and ‘energy
homeostasis pathway’ terms within the regulatory node and in an is_a relationship to ‘peptide and protein hormone signaling pathway’ term
within the signaling node.
Petri et al. Journal of Biomedical Semantics 2014, 5:7 Page 3 of 12
http://www.jbiomedsem.com/content/5/1/7
diseases from a network- rather than a gene-centric per-
spective, from the systems level of pathway cross-talk
and alterations within, is an approach increasingly being
considered [13-15].
As an example, a large-scale study carried out on a
number of pancreatic tumors identified several sets of
genes that were altered in the majority of tumors. Of
these, many were associated with core signaling path-
ways and altered in 67% to 100% of tumors [16]. Perhaps
not surprisingly, these are pathways important for
growth and proliferation and in some cases, also known
to be oncogenic (Figure 2). What may be intriguing is
the relatively large number of altered pathways and one
is tempted to wonder/speculate whether it is this num-
ber and the combinations that result from it, that over-
come the ability of the system to adjust and/or recover
and render the condition intractable. The pancreatic
cancer pathway diagram presents the main pathways al-
tered in the condition with the culprit genes shown
color coded. Additional links to a list of miRNAs
(microRNAs) aberrantly expressed in pancreatic tumors
and to the Cancer Portal at RGD are provided (see
Figure 2).
Pathway annotations, interactive pathway diagrams,
pathway suites and suite networks
The use of the ontology allows for the standardized an-
notation of rat, human and mouse genes to pathway
terms. Generally, annotations are made for the term ra-
ther than on a gene-by-gene basis; thus, what is being
targeted for annotation is the pathway itself – like the
ontology the overall pathway curation process is
network-centered [12,17]. Importantly, the ontology pro-
vides the navigational means to access pathway annota-
tions, interactive pathway diagrams, pathway suites and
suite networks as well as a variety of tools, from many
entry points. A pathway suite is a collection of pathways
that revolves around a common concept or is globally
Figure 2 Pancreatic cancer pathway diagram. The interactive pathway diagram page for the ‘pancreatic cancer pathway’. The altered
pathways associated with the condition are shown as gray rectangles that link to the ontology report(s) for the those terms. Culprit genes within
the pathways are shown color-coded (default is red). The icon for the microRNAs (miRNA) with potential roles in pancreatic cancer links to a page
where several down- and up-regulated miRNAs are shown with some targets listed and with links to their report pages in RGD and the microRNA
database (MiRBase). The icon for the condition links to the Cancer Disease Portal in RGD.
Petri et al. Journal of Biomedical Semantics 2014, 5:7 Page 4 of 12
http://www.jbiomedsem.com/content/5/1/7
related. If two (or more) pathway suites relate in some
fashion, they constitute a suite network. For instance,
the ‘Glucose Homeostasis Pathway Suite Network’
brings together the suite dedicated to the various meta-
bolic pathways involving glucose and the one dedicated
to the contributing signaling and regulatory pathways.
Together, the pathway ontology, the pathway annota-
tions and the graphical representations of pathways,
constitute the elements of the Pathway Portal [12,17,18],
an important project at the Rat Genome Database
[19,20]. Pathway, along with disease, phenotype and bio-
logical process, are the major concepts around which
the Disease Portals are built and are entry points to
access the data they contain. The Disease and Pathway
Portals can be accessed from the main homepage of
RGD (Figure 3A). The “Pathways” entry point leads to
the Molecular Pathways link which houses the collection
of interactive pathway diagrams and suites that RGD
publishes. This entry point also provides access to path-
way related publications by members of RGD as well as
other information and data links (Figure 3B).
An ontology search, accessed through the “Function”
entry point (see Figure 3A), brings up all the ontologies
that have terms which contain the keyword(s) used. Se-
lection of an ontology will show the terms containing
the keyword(s) with the option to search the tree or view
A
B
Figure 3 Pathway portal data access. A. Rat Genome Database homepage with the main entry points to its content; the “Pathways” and
“Function” entry points described in the text, are circled. B. Accessing the “Pathways” entry point and entries within.
Petri et al. Journal of Biomedical Semantics 2014, 5:7 Page 5 of 12
http://www.jbiomedsem.com/content/5/1/7
the annotations. Selecting the branch icon to the left of
a term brings up a browser result showing the parent,
siblings and children of the term. The browser has been
developed at RGD and recently updated to indicate
whether interactive pathway diagrams are available or
not for terms and/or their children in the form of a
boxed “D” of darker or paler green color, respectively
(see Figure 1A-B). Any dark green “D” box links to that
interactive diagram page. In addition, if the searched
term has a diagram, a small icon will be shown in the
term entry, to the right of the term description; it will
also link to the diagram page. [The boxed “A” in
Figure 1A-B denotes the presence of annotations].
Selecting a term brings up an ontology report page with
the GViewer tool – a genome-wide view of rat chromo-
somes with genes annotated to the term, a tabular list of
genes annotated to the term by species with links to re-
spective gene report pages and a diagram showing the
paths to the root term in the ontology tree. If there is an
interactive pathway diagram for the chosen term, an
icon is present at the top of the page to the right of the
diagram and it links to the pathway diagram page.
Every diagram page consists of several sections. The
first provides an in-depth, expandable description of the
pathway and the diagram itself whose objects link to
their report pages in RGD (genes, chemicals, pathways)
or other websites. Beneath that is a tabular list of anno-
tated genes by species with each entry linking to its re-
port page and other links. As applicable, the altered
version of the pathway and additional elements in the
diagram can also be found in this section. The next sec-
tion contains tabular lists of genes in the pathway that
have been annotated to disease, other pathway and
phenotype terms with links to corresponding report
pages. The user has the option of toggling between
terms and genes and can follow links to ontology report
pages for terms and to gene report pages for genes.
PROCEEDINGS Open Access
Evolving BioAssay Ontology (BAO):
modularization, integration and applications
Saminda Abeyruwan1†, Uma D Vempati2†, Hande Küçük-McGinty1, Ubbo Visser1, Amar Koleti2, Ahsan Mir2,
Kunie Sakurai3, Caty Chung2, Joshua A Bittker5, Paul A Clemons5, Steve Brudz5, Anosha Siripala6, Arturo J Morales6,
Martin Romacker6, David Twomey6, Svetlana Bureeva7, Vance Lemmon2,3, Stephan C Schürer2,4*
From Bio-Ontologies Special Interest Group 2013
Berlin, Germany. 20 July 2013
* Correspondence: sschurer@med.
miami.edu
2Center for Computational Science,
University of Miami, 1320 S. Dixie
Highway, Gables One Tower, 33146
Coral Gables, FL, USA
Abstract
The lack of established standards to describe and annotate biological assays and
screening outcomes in the domain of drug and chemical probe discovery is a severe
limitation to utilize public and proprietary drug screening data to their maximum
potential. We have created the BioAssay Ontology (BAO) project (http://
bioassayontology.org) to develop common reference metadata terms and definitions
required for describing relevant information of low-and high-throughput drug and
probe screening assays and results. The main objectives of BAO are to enable
effective integration, aggregation, retrieval, and analyses of drug screening data.
Since we first released BAO on the BioPortal in 2010 we have considerably expanded
and enhanced BAO and we have applied the ontology in several internal and
external collaborative projects, for example the BioAssay Research Database (BARD).
We describe the evolution of BAO with a design that enables modeling complex
assays including profile and panel assays such as those in the Library of Integrated
Network-based Cellular Signatures (LINCS). One of the critical questions in evolving
BAO is the following: how can we provide a way to efficiently reuse and share
among various research projects specific parts of our ontologies without violating
the integrity of the ontology and without creating redundancies. This paper provides
a comprehensive answer to this question with a description of a methodology for
ontology modularization using a layered architecture. Our modularization approach
defines several distinct BAO components and separates internal from external
modules and domain-level from structural components. This approach facilitates the
generation/extraction of derived ontologies (or perspectives) that can suit particular
use cases or software applications. We describe the evolution of BAO related to its
formal structures, engineering approaches, and content to enable modeling of
complex assays and integration with other ontologies and datasets.
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5 JOURNAL OF
BIOMEDICAL SEMANTICS
© 2014 Abeyruwan et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative
Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and
reproduction in any medium, provided the original work is properly cited. The Creative Commons Public Domain Dedication waiver
(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.
Background
Introduction to BAO and the domain
The development of novel small molecule therapeutics (drugs) typically begins with the
identification of suitable compounds with desirable biological activity in simple model
systems such as a purified protein that is a validated disease target or a cell related to
a disease or disease state. Target-based and cell-based phenotypic high-throughput
screening (HTS) are among the most important approaches to identify new hits and
leads from large compound libraries [1,2]. Innovations in assay design and technologi-
cal advances in detection and throughput have dramatically increased the size and
diversity of HTS datasets generated in pharmaceutical companies and in public
research projects. Examples of NIH-funded large-scale screening programs in which
we have been participating include the Molecular Libraries Program (MLP) [3] and the
Library of Integrated Network-based Cellular Signatures (LINCS) program [4]. In the
MLP, a large library (up to 430, 000 compounds) has been screened in over 600 probe
projects to develop novel tool and drug compounds. This data is deposited in Pub-
Chem [5] and is also being curated and made available for structured analysis in the
BioAssay Research Database (BARD) [6]. The LINCS project, in contrast to traditional
screening, generates extensive signatures of cellular responses consisting of thousands
of results for any perturbation (such as small molecule drugs) to enable the develop-
ment of better system-level disease models. Examples of LINCS screening results and
assays include Landmark gene expression signatures (L1000), Kinome-wide binding
affinities (KINOMEscan), phenotypic profiling across 1,000 cell lines, and many others,
covering “omics” and HTS data. LINCS results are currently available via participating
centers and can be queried and explored via the LINCS Information FramEwork
(LIFE) developed by our group [7]. Several other publicly accessible resources of
screening data exist, for example ChEMBL, a database that contains structure-activity
relationship (SAR) data curated from the medicinal chemistry literature [8], the Psy-
choactive Drug Screening Program (PDSP), which generates data from screening novel
psychoactive compounds for pharmacological activity [9], or Collaborative Drug Dis-
covery (CDD), a private company enabling drug discovery research collaborations [10].
Despite being publicly available, current data repositories suffer from structural, syntac-
tic, and semantic inconsistencies, complicating data integration, interpretation and analy-
sis. As one of the largest and first repositories of public drug screening data, PubChem,
has been essential to illustrate the need for clear metadata standards to describe drug and
chemical probe discovery assays and screening results [11]. To address these prevailing
issues; we have previously developed the first version of the BioAssay Ontology (BAO)
[12]. This first version was developed iteratively based on domain expertise and available
assay data, primarily from the MLP, which we annotated using evolving versions of BAO.
Since the first release of BAO, we have engaged with several more groups in public
research projects and in pharmaceutical companies and the biomedical ontology commu-
nity. We aligned the organization of BAO with existing efforts as much as possible, most
importantly at the Novartis Institutes of BioMedical Research, and we have significantly
extended the terminology and axioms in BAO to cover a broader range of assays and
related concepts. One of our objectives in redesigning BAO was to introduce an upper-
level ontology to facilitate alignment and integration with other biomedical domain ontol-
ogies and to provide a more formal ontology development framework. However, a critical
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 2 of 22
requirement was to maintain a “native” organization of BAO that is meaningful to end
users and which enables straight-forward incorporation into software systems, such as our
previously developed BAOSearch application [13]. This led us to a formal, structural, and
functional modularization of BAO, which we describe here. We also provide a general
solution to defining profile and panel-type assays in which many results are generated in
parallel, such as those in the LINCS project. Meanwhile, BAO has been applied in several
new projects, most importantly BARD, which also contributed to extending and improv-
ing BAO further.
Semantic Web technologies have become increasingly popular to integrate biome-
dical research information; a prominent example is the Bio2RDF project [14]. In
addition to “open-world” integration of diverse omics and high-throughput drug
screening data, Semantic Web technologies provide capabilities for inference reason-
ing with many potential benefits over traditional systems [15]. Only very recently
however, have large public drug screening datasets been made available as Resource
Description Framework (RDF) format. One such resource is ChEMBL, whose RDF
model leverages BAO to describe the results [16]. A large initiative to develop an
integrative solution to diverse drug discovery data is the Open Pharmacological Con-
cepts Triple Store (Open PHACTS) consortium [15]. Because of increasing adapta-
tion of Semantic Web technologies in drug discovery data management, it was
critical to develop BAO as a formal Description Logic (DL) ontology implemented in
Web Ontology Language (OWL). We show modeling examples illustrating BAO
semantic inference capabilities to identify mechanistically related assays in absence of
such explicit annotation.
Description logic
Description logic (DL) contains a set of decidable constructs from the first-order predi-
cate logic, and it is the corner stone for the development of OWL DL ontologies in
knowledge representation [17]. The computational complexity of a given DL depends
on the constructs that are being used, and they are traditionally represented with dif-
ferent complexity classes. Attribute Language with Complement (ALC) provides the
preliminary DL constructs with classes, roles, and individuals. The formal syntax of
ALC is defined as follows (as a convention, we indicate conceptualization by capital
letters (e.g., C, D) or sans serif letters (e.g., Thing, bioassay), sets by bold face letters
(e.g., C, I), and functions by lower case letters (e.g., fC,fI)). Let A be a named atomic
class, and, without loss of generality, let R be an abstract role. The class expressions
(concepts or concept expressions) C, D are recursively constructed by: C, D ¬ A | ? |
? | ?C | C ? D | C ? D | ?R.C | ?R.C, where, ? is the top concept, ? is the bottom
concept, the symbols for conjunction, disjunction, and negation are given by ?, ?, and
? respectively, and ? and ? represent the universal and existential quantifier. ALC
DL knowledge bases consist of two groups: (1) TBox provides statements about the
terminological knowledge; and (2) ABox provides the statements about the assertional
knowledge about individuals. These statements are also known as axioms in descrip-
tion logic. For class expressions C and D, the TBox statements are of the form C ? D
or C ? D, where ? denotes the equivalences among classes and ? constructs the sub-
sumption or general class inclusion (GCI) axioms. On the other hand ABox consists of
axioms of the form C(a) and R(a, b), where R is a role, and, a, b are individuals.
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 3 of 22
ALC DL has been extended toSROIQ(D) DL with the following syntactic con-
structs: {a} | ?R.Self | ? nR.C | ? nS.C, where, {a} represents nominals, ?R.Self relates
an individual to itself, and n ? ?+ with ? nR.C and ? nS.C provide the qualified cardin-
ality restrictions. SROIQ(D) DL introduces an RBox with general role inclusion
axioms of the form R1 . . . R2 ? R, which provides the meaning that concatenation of
R1, . . . , R2 is a subrole of R. In addition, there exists constructs to represent transitive,
symmetric, asymmetric, reflexive, irreflexive, functional, inverse functional, and disjoint
roles and concepts. It is to be noted that roles can either be abstract or concrete.
The interpretation of DL is given by the direct model-theoretic semantics. The
classes, roles, and individuals are given symbols from mutually disjoint sets of C, R,
and I respectively. There exists another set called the domain of interpretation, ?,
which contains entities for resources, individuals, or single objects. Using the domain
of interpretation, the individuals, classes, and roles are interpreted by functions fI : I
??, fC : C ? 2
?, and fR : ? 2
?×? respectively. The complex classes and role expressions
are interpreted by an extended interpretation function, .I , such that the interpretation
faithfully capturers the structure of the knowledge base. If a model exists, then the
knowledge base is satisfiable, and the implicit knowledge (logical consequence) is
entailed though an inference procedure. DL logic uses efficient tableau algorithms to
infer subsumption, class equivalence, class disjointness, global consistency, class consis-
tency, instance checking, and instance retrieval.
DL provides an appropriate trade-off between expressivity and scalability in practice.
The complexity of DL is dominated by the data complexity, which is NP-hard for
SROIQ(D) DL ABox and N2ExpTime-complete for the combined TBox, RBox, and
ABox. Modern SROIQ(D) DL reasoners such as the (1) tableau-based FaCT++ [18]
and Pellet [19] reasoners; and the (2) hyper-tableau HermiT [20] reasoner, use intelli-
gent heuristics and optimization methods to perform inferencing as efficiently as possi-
ble. The reader is referred to [21,22] for a comprehensive discussion on SROIQ(D)
DL syntax, semantics, deduction procedures, and model construction.
Results and discussion
BAO 2.0 native organization and main components
The new BAO 2.0 formally describes perturbation bioassays in the domain of drug and
probe discovery, such as small molecule HTS assays and screening results for the purpose
of categorizing the assays and outcomes by concepts that relate to the screening model
system (format), assay method, the biology interrogated in the assay (such as a protein tar-
get or biological process), the detection method (how does the assay work), and types of
results (endpoints). BAO 2.0 is organized into several major sections, which include multi-
ple levels of subcategories of subsumption class hierarchies. A number of specific object
property relationships were created to connect the classes and develop a knowledge
representation.
The main categories in BAO 2.0, titled components, include bioassay, assay biology,
assay method, assay format, assay endpoint, assay screened entity (Figure 1). Each of
these component classes includes the subsumption trees of terms corresponding to the
category and additional trees of related terms to describe each of the main components
properly and formally. In BAO 2.0, we incorporated a slightly different pattern from
BAO 1.6, since we were interested in making BAO 2.0 compatible with the existing
Abeyruwan et al. Journal of Biomedical Semantics 2014, 5(Suppl 1):S5
http://www.jbiomedsem.com/content/5/S1/S5
Page 4 of 22
upper-level and other domain-level ontologies. The BAO 2.0 categories also lend BAO
to its native structures that is most useful to users, for example to annotate assays or
to implement a user interface in a software application. We describe briefly the main
class hierarchies of BAO 2.0 corresponding to the above components (Figure 1):
• BAO assay bioassay component includes the bioassay subsumption tree, and sev-
eral other classes to describe assays, including assay kit, bioassay type, and bioassay
spec
